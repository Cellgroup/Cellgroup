{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHnWXy2Vgqn6"
   },
   "source": [
    "#Cellgroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJ4Rqxa6i56W"
   },
   "source": [
    "this is a library to cluster, analyse, and potentially forecast cell location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-omqw5Cfje43"
   },
   "source": [
    "## Download Libraries and Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LI2iNiXkYLPj",
    "outputId": "f1da9949-ac13-42a0-986c-18680062650e"
   },
   "outputs": [],
   "source": [
    "pip install reportlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "797c99pmji3W",
    "outputId": "6411cee8-4eaa-4f9d-b6a6-fce27961b23a"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/dvida/cyoptics-clustering.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wwms4JVFkMNH",
    "outputId": "5c8194ce-174a-48c3-ce0c-62edc2cc7687"
   },
   "outputs": [],
   "source": [
    "%cd cyoptics-clustering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkfga730kRP7"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Cython init\n",
    "import pyximport\n",
    "pyximport.install(setup_args={'include_dirs':[np.get_include()]})\n",
    "from cyOPTICS import runCyOPTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dm2nqCPfU2GU"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from pydrive.auth import GoogleAuth\n",
    "from google.colab import files\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from google.colab import drive\n",
    "from pydrive.drive import GoogleDrive\n",
    "\n",
    "from GradientClustering import gradientClustering, plotClusteringReachability, filterLargeClusters, \\\n",
    "    mergeSimilarClusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XYCSDf9KqNk"
   },
   "source": [
    "## Data treatment  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmVzmbrXKyIK"
   },
   "source": [
    "Table T0001 has a problem with noise. The solution for it is to use the new algorithm and it's possible to merge a part of it before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zxYnuQ_KwVF"
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('df1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SRK0KVLxOg8M"
   },
   "outputs": [],
   "source": [
    "# Remove points from coordinates 0 to 3000, 2000 to 3000, and 5500 to 6000 for X, and 0 to 3000 and 3000 to 4000 for Y\n",
    "df1 = df1[~((df1['X'] >= 0) & (df1['X'] <= 3000)) | ~((df1['Y'] >= 0) & (df1['Y'] <= 3000))]\n",
    "df1 = df1[~((df1['X'] >= 2000) & (df1['X'] <= 6000)) | ~((df1['Y'] >= 3100) & (df1['Y'] <= 3700))]\n",
    "df1 = df1[~((df1['X'] >= 4200) & (df1['X'] <= 6400)) | ~((df1['Y'] >= 3000) & (df1['Y'] <= 6200))]\n",
    "df1 = df1[~((df1['X'] >= 5100) & (df1['X'] <= 6000)) | ~((df1['Y'] >= 0) & (df1['Y'] <= 3000))]\n",
    "df1 = df1[~((df1['X'] >= 2000) & (df1['X'] <= 3000)) | ~((df1['Y'] >= 3000) & (df1['Y'] <= 4000))]\n",
    "df1.to_csv('df1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cqfHKfZBOjIv"
   },
   "outputs": [],
   "source": [
    "T1 = df1[['X', 'Y']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hc1IlfplA_Xy"
   },
   "outputs": [],
   "source": [
    "df.to_csv('file_name.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCCd0oaDi-D_"
   },
   "source": [
    "## Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbtVK8a2joLH"
   },
   "source": [
    "### Optics Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iykWPxdMjPbL"
   },
   "source": [
    "The first part of the notebook aims to understand some particular values trough an Optics alogrithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "locVvHzAjZAx"
   },
   "source": [
    "To copy Copy of runCyoptics.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lz-YpUlQcrql"
   },
   "outputs": [],
   "source": [
    "def runOPTICS(input_list, eps, min_pts):\n",
    "    \"\"\" A wrapper funtion for the OPTICS clustering Cython implementation.\n",
    "    Arguments:\n",
    "        input_list: [ndarray] 2D numpy array containing the input data (1 datum per row)\n",
    "        eps: [float] epsilon parameter - maximum distance between points\n",
    "        min_pts: [int] minimum points in the cluster\n",
    "\n",
    "    Return:\n",
    "        point_list: [ndarray] 2D numpy array containing information about every processed point, the columns\n",
    "            of the array are:\n",
    "            - processed: 0 for not processed, 1 for processed - upon returning, processed values of all\n",
    "                entries should be 1\n",
    "            - reachability distance: -1 for first points in the cluster, positive for all others\n",
    "            - core distance: -1 for noise, positive otherwise (the notion of noise can change with regard to\n",
    "                the different input values eps and min_pts)\n",
    "            - input data points (the input data colums are appended to the right)\n",
    "    \"\"\"\n",
    "\n",
    "    return runCyOPTICS(input_list, eps, min_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ac5NfssxjadR"
   },
   "outputs": [],
   "source": [
    "def plotPoints(points, ordered_list=[], clusters=[], title=''):\n",
    "\n",
    "    # Plot all points\n",
    "    plt.scatter(points[:,0], points[:,1], c='k', linewidth=0.2, edgecolor='w', facecolor=None)\n",
    "\n",
    "    # Plot clusters, if any\n",
    "    if clusters:\n",
    "\n",
    "        # Generate a list of colors for each cluster and randomize their order (so close clusters would have\n",
    "        # significcantly different color)\n",
    "        colors = cm.inferno(np.linspace(0.3, 1, len(clusters)))\n",
    "        color_order = random.sample(range(len(colors)), len(colors))\n",
    "\n",
    "        # Plot the clusters in 2D\n",
    "        for color, cluster in zip(colors[color_order], clusters):\n",
    "            plt.scatter(ordered_list[cluster][:,3], ordered_list[cluster][:,4], c=color, linewidth=0.2,\n",
    "                edgecolor='w')\n",
    "\n",
    "    # Set the title\n",
    "    plt.title(title)\n",
    "\n",
    "    # Turn on the grid, set color to grey\n",
    "    plt.gca().grid(color='0.5')\n",
    "\n",
    "    # Set background color to black\n",
    "    plt.gca().set_facecolor('black')\n",
    "\n",
    "    # Set the ratio to the window size 1:1\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h39UKLSzkXx_"
   },
   "outputs": [],
   "source": [
    "def saveResults(df, ordered_list, time, clusters=[], filename='results.csv'):\n",
    "  # Create an empty dataframe to store the results\n",
    "  results = pd.DataFrame(df)\n",
    "\n",
    "  # Plot clusters, if any\n",
    "  if clusters:\n",
    "        # Generate a list of colors for each cluster and randomize their order (so close clusters would have\n",
    "        # significcantly different color)\n",
    "        colors = cm.inferno(np.linspace(0.3, 1, len(clusters)))\n",
    "        color_order = random.sample(range(len(colors)), len(colors))\n",
    "\n",
    "        # Create a tuple to map each unique color to a unique integer ID\n",
    "        tuple_colors = [tuple(color) for color in colors]\n",
    "        unique_colors = list(set(tuple_colors))\n",
    "        color_to_id = {color: i for i, color in enumerate(unique_colors)}\n",
    "\n",
    "        results['Labels'] = np.nan\n",
    "        results['Time'] = np.nan\n",
    "        # Store the data in the results dataframe\n",
    "        for color, cluster in zip(colors[color_order], clusters):\n",
    "            x = ordered_list[cluster][:,3]\n",
    "            y = ordered_list[cluster][:,4]\n",
    "            labels = [color_to_id[tuple(color)]] * len(x)\n",
    "            # Replace column X and Y with x and y\n",
    "            results.loc[cluster, 'X'] = x\n",
    "            results.loc[cluster, 'Y'] = y\n",
    "\n",
    "             # Create the Labels column and store labels in the dataframe df\n",
    "            results.loc[cluster, 'Labels'] = labels\n",
    "            results.loc[cluster, 'Time'] = time\n",
    "\n",
    "  # Save the results to a CSV file\n",
    "  results = results.dropna(subset=['Labels'])\n",
    "  results.to_csv(filename, index=False)\n",
    "  print(\"succesfully saved the results to the cyoptics-clustering folder\")\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrmEL2yHUt-K"
   },
   "outputs": [],
   "source": [
    "def plot_csv(csv_file):\n",
    "   # Load the data from the CSV file\n",
    "    results = pd.read_csv(csv_file)\n",
    "\n",
    "    # Create a scatter plot\n",
    "    fig, ax = plt.subplots()\n",
    "    for label, group in results.groupby('Labels'):\n",
    "        ax.scatter(group['X'], group['Y'], label=label)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWPK0Dedhp2Y"
   },
   "outputs": [],
   "source": [
    "def plotPointsFromCSV(data, title=''):\n",
    "    # Extract X, Y, and Labels columns from the DataFrame\n",
    "    points = data[['X', 'Y']].to_numpy()\n",
    "    labels = data['Labels']\n",
    "\n",
    "    # Find unique cluster labels\n",
    "    unique_labels = labels.unique()\n",
    "\n",
    "    # Create clusters based on unique labels\n",
    "    clusters = [points[labels == label] for label in unique_labels]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot all points in black\n",
    "    ax.scatter(points[:, 0], points[:, 1], c='k', linewidth=0.2, edgecolor='w', facecolor=None)\n",
    "\n",
    "    # Plot clusters using the 'viridis' colormap\n",
    "    colors = cm.viridis(np.linspace(0, 1, len(unique_labels)))\n",
    "\n",
    "    for color, cluster in zip(colors, clusters):\n",
    "        ax.scatter(cluster[:, 0], cluster[:, 1], c=color, linewidth=0.2, edgecolor='w')\n",
    "\n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Set the X and Y limits to ensure a fixed size of 6500x6500 units\n",
    "    ax.set_xlim(0, 6500)\n",
    "    ax.set_ylim(0, 6500)\n",
    "\n",
    "    # Turn on the grid, set color to grey\n",
    "    ax.grid(color='0.5')\n",
    "\n",
    "    # Set background color to black\n",
    "    ax.set_facecolor('black')\n",
    "\n",
    "    # Set the ratio to the window size 1:1\n",
    "    ax.set_aspect('equal')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "arOZUzkqUwZW"
   },
   "outputs": [],
   "source": [
    "def save_plot_and_show(plot_function, filename, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Plot using the provided plot_function, save the figure to the given filename,\n",
    "    and then display the plot.\n",
    "\n",
    "    Parameters:\n",
    "    - plot_function: The function to create the plot.\n",
    "    - filename: The name of the file to save the plot.\n",
    "    - *args, **kwargs: Additional arguments to pass to the plot_function.\n",
    "    \"\"\"\n",
    "    # Create the plot using the provided function and arguments\n",
    "    fig = plot_function(*args, **kwargs)\n",
    "\n",
    "    # Save the plot to the specified filename\n",
    "    fig.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wO4TFvqJU8ym"
   },
   "outputs": [],
   "source": [
    "def run_clustering(input_data, min_points, epsilon, w, max_points_ratio, cluster_similarity_threshold, t=150):\n",
    "  input_data = input_data.values\n",
    "\n",
    "  # Plot input data\n",
    "  plotPoints(input_data, title='Input data')\n",
    "\n",
    "\n",
    "  t1 = time.process_time()\n",
    "\n",
    "  # Run OPTICS ordering\n",
    "  ordered_list = runOPTICS(input_data, epsilon, min_points)\n",
    "\n",
    "  print('Total time for processing', time.process_time() - t1, 's')\n",
    "\n",
    "  # Plot the reachability diagram\n",
    "  plotClusteringReachability(ordered_list[:,1])\n",
    "\n",
    "  # Do the gradient clustering\n",
    "  clusters = gradientClustering(ordered_list[:,1], min_points, t, w)\n",
    "\n",
    "  # Remove very large clusters\n",
    "  filtered_clusters = filterLargeClusters(clusters, len(ordered_list), max_points_ratio)\n",
    "\n",
    "  # Plot the results, reachability diagram\n",
    "  plotClusteringReachability(ordered_list[:,1], filtered_clusters)\n",
    "\n",
    "\n",
    "  # Merge similar clusters by looking at the ratio of their intersection and their total number\n",
    "  filtered_clusters = mergeSimilarClusters(filtered_clusters, cluster_similarity_threshold)\n",
    "\n",
    "  # Plot the results, reachability diagram\n",
    "  plotClusteringReachability(ordered_list[:,1], filtered_clusters)\n",
    "\n",
    "  # Plot the final results\n",
    "  plotPoints(input_data, ordered_list, filtered_clusters, title='Final results')\n",
    "  return filtered_clusters, ordered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fG_L8D9YZFi"
   },
   "outputs": [],
   "source": [
    "def run_clustering_without_plots(input_data, min_points, epsilon, w, max_points_ratio, cluster_similarity_threshold, t=150):\n",
    "  input_data = input_data.values\n",
    "\n",
    "  # Plot input data\n",
    "  plotPoints(input_data, title='Input data')\n",
    "\n",
    "\n",
    "  t1 = time.process_time()\n",
    "\n",
    "  # Run OPTICS ordering\n",
    "  ordered_list = runOPTICS(input_data, epsilon, min_points)\n",
    "\n",
    "  print('Total time for processing', time.process_time() - t1, 's')\n",
    "\n",
    "  # Do the gradient clustering\n",
    "  clusters = gradientClustering(ordered_list[:,1], min_points, t, w)\n",
    "\n",
    "  # Remove very large clusters\n",
    "  filtered_clusters = filterLargeClusters(clusters, len(ordered_list), max_points_ratio)\n",
    "\n",
    "  # Merge similar clusters by looking at the ratio of their intersection and their total number\n",
    "  filtered_clusters = mergeSimilarClusters(filtered_clusters, cluster_similarity_threshold)\n",
    "\n",
    "  return filtered_clusters, ordered_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rF_KTEqwjnV0"
   },
   "source": [
    "### Obtain parameter for clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LN1DrTQdjE_y"
   },
   "outputs": [],
   "source": [
    "# Import values from Github\n",
    "\n",
    "import pandas as pd\n",
    "df02 = pd.read_csv('df2.csv')\n",
    "df10 = pd.read_csv('df10.csv')\n",
    "df20 = pd.read_csv('df20.csv')\n",
    "df30 = pd.read_csv('df30.csv')\n",
    "df40 = pd.read_csv('df40.csv')\n",
    "df60 = pd.read_csv('df60.csv')\n",
    "df80 = pd.read_csv('df80.csv')\n",
    "df100 = pd.read_csv('df100.csv')\n",
    "df118 = pd.read_csv('df118.csv')\n",
    "\n",
    "df02 = df02[['Area', 'X', 'Y', 'IntDen']].copy()\n",
    "df10 = df10[['Area', 'X', 'Y', 'IntDen']].copy()\n",
    "df20 = df20[['Area', 'X', 'Y', 'IntDen']].copy()\n",
    "df30 = df30[['Area', 'X', 'Y', 'IntDen']].copy()\n",
    "df40 = df40[['Area', 'X', 'Y', 'IntDen']].copy()\n",
    "df60 = df60[['Area', 'X', 'Y', 'IntDen']].copy()\n",
    "df80 = df80[['Area', 'X', 'Y', 'IntDen']].copy()\n",
    "df100 = df100[['Area', 'X', 'Y', 'IntDen']].copy()\n",
    "df118 = df118[['Area', 'X', 'Y', 'IntDen']].copy()\n",
    "\n",
    "T02 = df02[['X', 'Y']].copy()\n",
    "T10 = df10[['X', 'Y']].copy()      #There was no ROI.zip file for Point 1 by the origional Macro Provided.\n",
    "T20 = df20[['X', 'Y']].copy()\n",
    "T30 = df30[['X', 'Y']].copy()\n",
    "T40 = df40[['X', 'Y']].copy()\n",
    "T60 = df60[['X', 'Y']].copy()\n",
    "T80 = df80[['X', 'Y']].copy()\n",
    "T100 = df100[['X', 'Y']].copy()\n",
    "T118 = df118[['X', 'Y']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_GHzmgL67Ry"
   },
   "outputs": [],
   "source": [
    "df60 = pd.read_csv('df60.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NA7dyOek690s"
   },
   "outputs": [],
   "source": [
    "T60 = df60[['X', 'Y']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GaL-6XAmT3aD"
   },
   "source": [
    "showcase just a few examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FbOcmAwBuLZ"
   },
   "source": [
    "#### Selected Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Z0IpRhJPEYh"
   },
   "source": [
    "##### T01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pU3A9YmxPRkP",
    "outputId": "d8e7d977-d763-489e-cce6-bcd57f766168"
   },
   "outputs": [],
   "source": [
    "clusters, coord = run_clustering(T1, min_points=325, epsilon=260, w=10, max_points_ratio=0.8, cluster_similarity_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIa4cINvWD11"
   },
   "source": [
    "##### T02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q2vgROd7WEZS",
    "outputId": "67e926df-c149-4c87-f0c3-b5fc254b80cc"
   },
   "outputs": [],
   "source": [
    "clusters, coord = run_clustering(T02, min_points=400, epsilon=260, w=20, max_points_ratio=0.8, cluster_similarity_threshold=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVqSpyXqQNcn"
   },
   "source": [
    "##### T10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FuyJr2t3TdJH",
    "outputId": "f7089883-b951-4960-bb0a-5c8877366c85"
   },
   "outputs": [],
   "source": [
    "clusters, coord = run_clustering(T10, min_points=200, epsilon=300, w=5.5, max_points_ratio=0.6, cluster_similarity_threshold=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIND83pJQU4Z"
   },
   "source": [
    "##### T20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2FQFgzt6TdZr",
    "outputId": "eed27231-230e-406b-9007-97aa659f18ba"
   },
   "outputs": [],
   "source": [
    "clusters, coord = run_clustering(T20, min_points=150, epsilon=300, w=5.25, max_points_ratio=0.6, cluster_similarity_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L70TyypIQXWS"
   },
   "source": [
    "##### T30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RaXmcGqgQZGr",
    "outputId": "43dedf3b-bc98-4f34-b6ba-b18b002314ce"
   },
   "outputs": [],
   "source": [
    "clusters, coord = run_clustering(T30, min_points=75, epsilon=300, w=4.75, max_points_ratio=0.6, cluster_similarity_threshold=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVjZQyGOQiaj"
   },
   "source": [
    "##### T40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZEcmUZAaQlwP",
    "outputId": "9f461eb5-ac2d-4e17-8a9c-c43db108bbc2"
   },
   "outputs": [],
   "source": [
    "clusters, coord = run_clustering(T40, min_points=40, epsilon=300, w=4.75, max_points_ratio=0.6, cluster_similarity_threshold=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rdNrqKuQSjf"
   },
   "source": [
    "##### T60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6sDjOFA9T1Yq",
    "outputId": "9cee2451-99f9-40b5-c008-1c9e73558e79"
   },
   "outputs": [],
   "source": [
    "clusters, coord = run_clustering(T60, min_points=25,\n",
    "epsilon=250,\n",
    "w=4.5,\n",
    "max_points_ratio=0.8,\n",
    "cluster_similarity_threshold=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwX7fboF8zsQ"
   },
   "source": [
    "##### T80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UvPzwZsKT29J",
    "outputId": "db6dbc5d-e09d-48b4-8f1a-880b10fdd6cc"
   },
   "outputs": [],
   "source": [
    "clusters, coord = run_clustering(T80, min_points=25,epsilon=250,w=4.4,max_points_ratio=0.8,cluster_similarity_threshold=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85l7oUs481cf"
   },
   "source": [
    "##### T100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eST1RZ5oUEh_",
    "outputId": "c963c069-6bac-4528-a464-25906fbdecb6"
   },
   "outputs": [],
   "source": [
    "clusters, coord = run_clustering(T100, min_points=7,epsilon=200,w=3.5,max_points_ratio=0.8,cluster_similarity_threshold=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yi_BFp6f822q"
   },
   "source": [
    "##### T118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_e24Mr_8UAvS",
    "outputId": "70e9e6ba-9902-423e-8d80-083b9d9b509e"
   },
   "outputs": [],
   "source": [
    "clusters, coord = run_clustering(T118, min_points=3, epsilon=200,w=3,max_points_ratio=0.8,cluster_similarity_threshold=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_gCh8Gcjv_T"
   },
   "source": [
    "### generalise the analysis to all the values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejOfs3SRVa1K"
   },
   "source": [
    "Here there are the obtained results from the previous paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVvzqsVWCqwY"
   },
   "source": [
    "#### Old Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "r7PGeo1K52zs",
    "outputId": "185a59e8-1385-41d4-b8e3-3d674247daa2"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "t_data = np.array([1, 2, 10, 20, 30, 40, 60, 80, 100, 118])\n",
    "w_data = np.array([10, 20, 5.5, 5.25, 4.75, 4.75, 4.5, 4.4, 3.5, 3])\n",
    "\n",
    "# Create an interp1d object with 'slinear' interpolation\n",
    "interp_func = interp1d(t_data, w_data, kind='slinear')\n",
    "\n",
    "# Generate finer x values for plotting\n",
    "t_fine = np.linspace(min(t_data), max(t_data), max(t_data))\n",
    "w_fine = interp_func(t_fine)\n",
    "\n",
    "# Plot the original data and the interpolated curve\n",
    "plt.scatter(t_data, w_data, label='Data')\n",
    "plt.plot(t_fine, w_fine, label='Interpolated Curve', color='red')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('w')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "2ZbDe2u3V3_W",
    "outputId": "af7d9ee7-ab4b-42d1-d6e5-5c0b5c5915d0"
   },
   "outputs": [],
   "source": [
    "t_data = np.array([1, 2, 10, 20, 30, 40, 60, 80, 100, 118])\n",
    "max_points_ratio_data = np.array([0.8, 0.8, 0.6, 0.6, 0.6, 0.6, 0.8, 0.8, 0.8, 0.8])\n",
    "\n",
    "# Create an interp1d object with 'slinear' interpolation\n",
    "interp_func = interp1d(t_data, max_points_ratio_data, kind='slinear')\n",
    "\n",
    "# Generate finer x values for plotting\n",
    "t_fine = np.linspace(min(t_data), max(t_data), max(t_data))\n",
    "max_points_ratio_fine = interp_func(t_fine)\n",
    "\n",
    "# Plot the original data and the interpolated curve\n",
    "plt.scatter(t_data, max_points_ratio_data, label='Data')\n",
    "plt.plot(t_fine, max_points_ratio_fine, label='Interpolated Curve', color='red')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('max_points_ratio_data')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "b4BFj9ZZV9pL",
    "outputId": "2e6c16d5-3d29-4da3-8aff-737c5f5f7184"
   },
   "outputs": [],
   "source": [
    "t_data = np.array([1, 2, 10, 20, 30, 40, 60, 80, 100, 118])\n",
    "cluster_similarity_threshold_data = np.array([0.5, 0.7, 0.6, 0.5, 0.4, 0.4, 0.4, 0.3, 0.1, 0.1])\n",
    "\n",
    "# Create an interp1d object with 'slinear' interpolation\n",
    "interp_func = interp1d(t_data, cluster_similarity_threshold_data, kind='slinear')\n",
    "\n",
    "# Generate finer x values for plotting\n",
    "t_fine = np.linspace(min(t_data), max(t_data), max(t_data))\n",
    "cluster_similarity_threshold_fine = interp_func(t_fine)\n",
    "\n",
    "# Plot the original data and the interpolated curve\n",
    "plt.scatter(t_data, cluster_similarity_threshold_data, label='Data')\n",
    "plt.plot(t_fine, cluster_similarity_threshold_fine, label='Interpolated Curve', color='red')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('cluster_similarity_threshold_data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "lLd5Tdi8WBfj",
    "outputId": "096e5449-9867-4935-dc02-09cc5b7d8520"
   },
   "outputs": [],
   "source": [
    "t_data = np.array([1, 2, 10, 20, 30, 40, 60, 80, 100, 118])\n",
    "min_points_data = np.array([325, 400, 200, 150, 75, 40, 25, 25, 7, 3])\n",
    "\n",
    "# Create an interp1d object with 'slinear' interpolation\n",
    "interp_func = interp1d(t_data, min_points_data, kind='slinear')\n",
    "\n",
    "# Generate finer x values for plotting\n",
    "t_fine = np.linspace(min(t_data), max(t_data), max(t_data))\n",
    "min_points_fine = interp_func(t_fine)\n",
    "\n",
    "# Plot the original data and the interpolated curve\n",
    "plt.scatter(t_data, min_points_data, label='Data')\n",
    "plt.plot(t_fine, min_points_fine, label='Interpolated Curve', color='red')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('min_points_data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "AhRBUPERWGuB",
    "outputId": "1c0f2eb0-5806-45a6-ccdf-e8273a822a1e"
   },
   "outputs": [],
   "source": [
    "t_data = np.array([1, 2, 10, 20, 30, 40, 60, 80, 100, 118])\n",
    "epsilon_data = np.array([260, 260, 300, 300, 300, 300, 250, 250, 200, 200])\n",
    "\n",
    "# Create an interp1d object with 'slinear' interpolation\n",
    "interp_func = interp1d(t_data, epsilon_data, kind='slinear')\n",
    "\n",
    "# Generate finer x values for plotting\n",
    "t_fine = np.linspace(min(t_data), max(t_data), max(t_data))\n",
    "epsilon_fine = interp_func(t_fine)\n",
    "\n",
    "# Plot the original data and the interpolated curve\n",
    "plt.scatter(t_data, epsilon_data, label='Data')\n",
    "plt.plot(t_fine, epsilon_fine, label='Interpolated Curve', color='red')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('epsilon_data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "6sHljBk778ig",
    "outputId": "d0aef201-b7dc-433a-c331-befe7b7f629e"
   },
   "outputs": [],
   "source": [
    "t_data = np.array([0, 40, 48, 65, 72, 90, 97, 98, 100, 118])\n",
    "p_data = np.array([0, 0, 200, 300, 400, 500, 700, 850, 850, 1000])\n",
    "\n",
    "# Create an interp1d object with 'slinear' interpolation\n",
    "interp_func = interp1d(t_data, p_data, kind='slinear')\n",
    "\n",
    "# Generate finer x values for plotting\n",
    "t_fine = np.linspace(min(t_data), max(t_data), max(t_data))\n",
    "p_fine = interp_func(t_fine)\n",
    "\n",
    "# Plot the original data and the interpolated curve\n",
    "plt.scatter(t_data, p_data, label='Data')\n",
    "plt.plot(t_fine, p_fine, label='Interpolated Curve', color='red')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('p_data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "WKlp0oncCrBT",
    "outputId": "221eb997-3a44-4525-9b8c-60b95cad913d"
   },
   "outputs": [],
   "source": [
    "t_data = np.array([0, 1, 118])\n",
    "s_data = np.array([-750, 4000, 4000])\n",
    "\n",
    "# Create an interp1d object with 'slinear' interpolation\n",
    "interp_func = interp1d(t_data, s_data, kind='slinear')\n",
    "\n",
    "# Generate finer x values for plotting\n",
    "t_fine = np.linspace(min(t_data), max(t_data), max(t_data))\n",
    "s_fine = interp_func(t_fine)\n",
    "\n",
    "# Plot the original data and the interpolated curve\n",
    "plt.scatter(t_data, s_data, label='Data')\n",
    "plt.plot(t_fine, s_fine, label='Interpolated Curve', color='red')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('s_data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "joRemBvzCwx_"
   },
   "source": [
    "#### New Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "EVQgIDnHCyyE",
    "outputId": "34304a34-1f52-4a9e-9dc6-e995ad9e9fb4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t_data = np.array([1, 20, 60, 80, 100, 118])\n",
    "w_data = np.array([4.8, 4, 2, 1, 0.05, 0.01])\n",
    "\n",
    "# Create an interp1d object with 'slinear' interpolation\n",
    "interp_func = interp1d(t_data, w_data, kind='slinear')\n",
    "\n",
    "# Generate finer x values for plotting\n",
    "t_fine = np.linspace(min(t_data), max(t_data), max(t_data))\n",
    "w_fine_new = interp_func(t_fine)\n",
    "\n",
    "# Plot the original data and the interpolated curve\n",
    "plt.scatter(t_data, w_data, label='Data')\n",
    "plt.plot(t_fine, w_fine_new, label='Interpolated Curve', color='red')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('w')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "Fy4AlMd8CzYH",
    "outputId": "19a60fdb-d566-4ead-814c-3cd660f45c19"
   },
   "outputs": [],
   "source": [
    "t_data = np.array([1, 20, 60, 80, 100, 118])\n",
    "max_points_ratio_data = np.array([0.6, 0.6, 0.8, 0.8, 0.8, 0.9])\n",
    "\n",
    "# Create an interp1d object with 'slinear' interpolation\n",
    "interp_func = interp1d(t_data, max_points_ratio_data, kind='slinear')\n",
    "\n",
    "# Generate finer x values for plotting\n",
    "t_fine = np.linspace(min(t_data), max(t_data), max(t_data))\n",
    "max_points_ratio_fine_new = interp_func(t_fine)\n",
    "\n",
    "# Plot the original data and the interpolated curve\n",
    "plt.scatter(t_data, max_points_ratio_data, label='Data')\n",
    "plt.plot(t_fine, max_points_ratio_fine_new, label='Interpolated Curve', color='red')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('max_points_ratio_data')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "9ArRhYygCznO",
    "outputId": "0c382958-afb0-4cae-de60-2196a65e7be6"
   },
   "outputs": [],
   "source": [
    "t_data = np.array([1, 20, 60, 80, 100, 118])\n",
    "cluster_similarity_threshold_data = np.array([0.6, 0.5, 0.03, 0.02, 0.02, 0.02])\n",
    "\n",
    "# Create an interp1d object with 'slinear' interpolation\n",
    "interp_func = interp1d(t_data, cluster_similarity_threshold_data, kind='slinear')\n",
    "\n",
    "# Generate finer x values for plotting\n",
    "t_fine = np.linspace(min(t_data), max(t_data), max(t_data))\n",
    "cluster_similarity_threshold_fine_new = interp_func(t_fine)\n",
    "\n",
    "# Plot the original data and the interpolated curve\n",
    "plt.scatter(t_data, cluster_similarity_threshold_data, label='Data')\n",
    "plt.plot(t_fine, cluster_similarity_threshold_fine_new, label='Interpolated Curve', color='red')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('cluster_similarity_threshold_data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "s3CqXph-Cz69",
    "outputId": "b71bfa6a-e41e-42b4-9f0b-5cba12fbc098"
   },
   "outputs": [],
   "source": [
    "t_data = np.array([1, 20, 60, 80, 100, 118])\n",
    "min_points_data = np.array([100, 95, 50, 15, 5, 3])\n",
    "\n",
    "# Create an interp1d object with 'slinear' interpolation\n",
    "interp_func = interp1d(t_data, min_points_data, kind='slinear')\n",
    "\n",
    "# Generate finer x values for plotting\n",
    "t_fine = np.linspace(min(t_data), max(t_data), max(t_data))\n",
    "min_points_fine_new = interp_func(t_fine)\n",
    "\n",
    "# Plot the original data and the interpolated curve\n",
    "plt.scatter(t_data, min_points_data, label='Data')\n",
    "plt.plot(t_fine, min_points_fine_new, label='Interpolated Curve', color='red')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('min_points_data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "xP07FTYGC0Ji",
    "outputId": "4003b24a-05db-411a-f8bd-0fe4d299fb64"
   },
   "outputs": [],
   "source": [
    "t_data = np.array([1, 20, 60, 80, 100, 118])\n",
    "epsilon_data = np.array([500, 450, 300, 300, 250, 250])\n",
    "\n",
    "# Create an interp1d object with 'slinear' interpolation\n",
    "interp_func = interp1d(t_data, epsilon_data, kind='slinear')\n",
    "\n",
    "# Generate finer x values for plotting\n",
    "t_fine = np.linspace(min(t_data), max(t_data), max(t_data))\n",
    "epsilon_fine_new = interp_func(t_fine)\n",
    "\n",
    "# Plot the original data and the interpolated curve\n",
    "plt.scatter(t_data, epsilon_data, label='Data')\n",
    "plt.plot(t_fine, epsilon_fine_new, label='Interpolated Curve', color='red')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('epsilon_data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1938yZG7j_u9"
   },
   "source": [
    "#### cluster all tables from Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjV5y3ztFKdN"
   },
   "source": [
    "##### importing values from drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0opYugQiq9Zi",
    "outputId": "84409e60-4616-4a6b-dd6d-1ab7364ca912"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SSylmgUbqyUy",
    "outputId": "679f90a0-87ed-4ca0-d9fd-c51d3c84e37b"
   },
   "outputs": [],
   "source": [
    "# Define the folder path in your Google Drive containing the CSV files\n",
    "folder_path = '/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/Original_Algorithm/Raw CSV'\n",
    "\n",
    "# List all CSV files in the folder and sort them alphabetically\n",
    "csv_files = sorted([csv_file for csv_file in os.listdir(folder_path) if csv_file.endswith('.csv')])\n",
    "\n",
    "csv_data = []  # List to store dataframes\n",
    "\n",
    "for csv_file in tqdm(csv_files, desc=\"Processing CSV files\"):\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "    csv_data.append(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmipN9Fx_ZiV"
   },
   "source": [
    "Note, we have a particular value for df1 that would correspond to csv_file[117]. So it's inverted. For this reason, we can invert the list (rather than inverting 6 lists that is the other possibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CRG1znlI9efu",
    "outputId": "22989f78-7a8a-4869-8331-11bf88a25a77"
   },
   "outputs": [],
   "source": [
    "t_data = []\n",
    "for i in tqdm(range(len((csv_data)))):\n",
    "  t_data.append(csv_data[i][['X', 'Y']].copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BsRQbmaGcNl"
   },
   "source": [
    "The DataFrame is at index 0. SO for df1, the position is 0. It's in otder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPI2isHakCV1"
   },
   "source": [
    "##### Increase robustness merging clusters together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "reZMcz56y55B"
   },
   "outputs": [],
   "source": [
    "def delete_outliers(data):\n",
    "  L_max = 1000\n",
    "    # Iterate through unique cluster labels\n",
    "  for cluster_label in data['Labels'].unique():\n",
    "        # Get the rows corresponding to the current cluster\n",
    "        cluster = data[data['Labels'] == cluster_label]\n",
    "\n",
    "        # Calculate the mean 'X' and 'Y' values for the current cluster\n",
    "        X_cluster_location = cluster['X'].mean()\n",
    "        Y_cluster_location = cluster['Y'].mean()\n",
    "\n",
    "        # Create a mask to identify outliers in the cluster\n",
    "        mask = (abs(cluster['X'] - X_cluster_location) <= L_max) & (abs(cluster['Y'] - Y_cluster_location) <= L_max)\n",
    "\n",
    "        # Remove outliers from the cluster\n",
    "        cluster = cluster[mask]\n",
    "\n",
    "        # Update the data DataFrame with the modified cluster\n",
    "        data = pd.concat([data[data['Labels'] != cluster_label], cluster])\n",
    "\n",
    "  return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9DQb84FkHuz"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import gmean\n",
    "import pandas as pd\n",
    "\n",
    "def data_cluster(cluster):\n",
    "  Ari_light_intensity = sum(cluster['IntDen']) / len(cluster)\n",
    "  Geo_light_intensity = geometric_mean = gmean(cluster.loc[:,'IntDen'])\n",
    "  Ari_cell_size = sum(cluster['Area']) / len(cluster)\n",
    "  Geo_cell_size = geometric_mean = gmean(cluster.loc[:,'Area'])\n",
    "  Cluster_dimension = sum(cluster['Area'])\n",
    "  N_cell = len(cluster)\n",
    "  X_cluster_location = sum(cluster['X']) / len(cluster)\n",
    "  Y_cluster_location = sum(cluster['Y']) / len(cluster)\n",
    "  Labels = sum(cluster['Labels']) / len(cluster)\n",
    "\n",
    "  cluster = {\n",
    "      'Ari_light_intensity': [Ari_light_intensity],\n",
    "      'Geo_light_intensity': [Geo_light_intensity],\n",
    "      'Ari_cell_size': [Ari_cell_size],\n",
    "      'Geo_cell_size': [Geo_cell_size],\n",
    "      'Cluster_dimension': [Cluster_dimension],\n",
    "      'N_cell': [N_cell],\n",
    "      'X_cluster_location': [X_cluster_location],\n",
    "      'Y_cluster_location': [Y_cluster_location],\n",
    "      'Labels': [Labels]\n",
    "  }\n",
    "\n",
    "  cluster_df = pd.DataFrame(cluster)\n",
    "  return cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1y_0p3EmMj_"
   },
   "outputs": [],
   "source": [
    "def merge_clusters_and_list(cluster_df, p=0):\n",
    "    cluster_list = []\n",
    "    T_x = 1000 - p\n",
    "    T_y = 1000 - p\n",
    "    i = len(cluster_df) - 1\n",
    "\n",
    "    while i >= 0:\n",
    "        j = i + 1  # Initialize j\n",
    "\n",
    "        while j < len(cluster_df):\n",
    "            distance_x, distance_y = calculate_distance(cluster_df.iloc[i]['X_cluster_location'], cluster_df.iloc[i]['Y_cluster_location'], cluster_df.iloc[j]['X_cluster_location'], cluster_df.iloc[j]['Y_cluster_location'])\n",
    "\n",
    "            if distance_x < T_x and distance_y < T_y and i != j:\n",
    "                # save the values in the cluster list\n",
    "                cluster_list.append(cluster_df['Labels'][i])\n",
    "                cluster_list.append(cluster_df['Labels'][j])\n",
    "                # Update new_x and new_y based on the average of the two points\n",
    "                new_Ari_light_intensity = (cluster_df.iloc[i]['Ari_light_intensity']*cluster_df.iloc[i]['N_cell'] + cluster_df.iloc[j]['Ari_light_intensity']*cluster_df.iloc[j]['N_cell'])/ (cluster_df.iloc[i]['N_cell'] + cluster_df.iloc[j]['N_cell'])\n",
    "                log_Geo_light_intensity = (np.log(cluster_df.iloc[i]['Geo_light_intensity'])*cluster_df.iloc[i]['N_cell'] + np.log(cluster_df.iloc[j]['Geo_light_intensity'])*cluster_df.iloc[j]['N_cell'])/ (cluster_df.iloc[i]['N_cell'] + cluster_df.iloc[j]['N_cell'])\n",
    "                new_Geo_light_intensity = np.exp(log_Geo_light_intensity)\n",
    "                new_Ari_cell_size = (cluster_df.iloc[i]['Ari_light_intensity']*cluster_df.iloc[i]['N_cell'] + cluster_df.iloc[j]['Ari_light_intensity']*cluster_df.iloc[j]['N_cell'])/ (cluster_df.iloc[i]['N_cell'] + cluster_df.iloc[j]['N_cell'])\n",
    "                log_Geo_cell_size = (np.log(cluster_df.iloc[i]['Geo_cell_size'])*cluster_df.iloc[i]['N_cell'] + np.log(cluster_df.iloc[j]['Geo_cell_size'])*cluster_df.iloc[j]['N_cell'])/ (cluster_df.iloc[i]['N_cell'] + cluster_df.iloc[j]['N_cell'])\n",
    "                new_Geo_cell_size = np.exp(log_Geo_cell_size)\n",
    "                new_Cluster_dimension = cluster_df.iloc[i]['Ari_cell_size'] + cluster_df.iloc[j]['Ari_cell_size']\n",
    "                new_N_cell = cluster_df.iloc[i]['N_cell'] + cluster_df.iloc[j]['N_cell']\n",
    "                new_X_cluster_location = (cluster_df.iloc[i]['X_cluster_location']*cluster_df.iloc[i]['N_cell'] + cluster_df.iloc[j]['X_cluster_location']*cluster_df.iloc[j]['N_cell'])/ (cluster_df.iloc[i]['N_cell'] + cluster_df.iloc[j]['N_cell'])\n",
    "                new_Y_cluster_location = (cluster_df.iloc[i]['Y_cluster_location']*cluster_df.iloc[i]['N_cell'] + cluster_df.iloc[j]['Y_cluster_location']*cluster_df.iloc[j]['N_cell'])/ (cluster_df.iloc[i]['N_cell'] + cluster_df.iloc[j]['N_cell'])\n",
    "                Labels = cluster_df['Labels'][j]\n",
    "                # New row data\n",
    "                new_row = [new_Ari_light_intensity, new_Geo_light_intensity, new_Ari_cell_size, new_Geo_cell_size, new_Cluster_dimension, new_N_cell, new_X_cluster_location, new_Y_cluster_location, Labels]\n",
    "\n",
    "                # Add the new row using DataFrame.loc\n",
    "                cluster_df.loc[len(cluster_df)] = new_row\n",
    "\n",
    "                # Positions (indices) of the rows to be removed\n",
    "                positions_to_remove = [i, j]\n",
    "\n",
    "                # Drop rows by position\n",
    "                cluster_df.drop(positions_to_remove, inplace=True)\n",
    "                cluster_df.reset_index(drop=True, inplace=True)\n",
    "                j = i + 1  # Increment j\n",
    "            else:\n",
    "                j += 1  # Increment j\n",
    "\n",
    "        i -= 1  # Decrement i\n",
    "\n",
    "    return cluster_list, cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SrxITAkCnAeF"
   },
   "outputs": [],
   "source": [
    "def change_labels(cluster_list, data):\n",
    "    for i in range(len(cluster_list)):\n",
    "        if i % 2 == 0:  # Check if 'i' is even\n",
    "            data.loc[data['Labels'] == cluster_list[i], 'Labels'] = cluster_list[i + 1]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ZAmLYX6A00m"
   },
   "outputs": [],
   "source": [
    "def listing_clusters(dataframe):\n",
    "    # Create an empty list to store DataFrames for each cluster\n",
    "    cluster_list = []\n",
    "\n",
    "    # Get unique cluster values from the 'Cluster' column\n",
    "    unique_clusters = dataframe['Labels'].unique()\n",
    "\n",
    "    # Iterate through unique cluster values and create a DataFrame for each\n",
    "    for cluster_value in unique_clusters:\n",
    "        # Filter the DataFrame for the current cluster value\n",
    "        cluster_df = dataframe[dataframe['Labels'] == cluster_value]\n",
    "\n",
    "        # Append the filtered DataFrame to the list\n",
    "        cluster_list.append(cluster_df)\n",
    "\n",
    "    return cluster_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F39T6QI6kL1X"
   },
   "outputs": [],
   "source": [
    "def merge_cluster_single_points(data, p=0):\n",
    "  cluster_list = listing_clusters(data)\n",
    "\n",
    "  stor = []\n",
    "  for i in range(len(cluster_list)):\n",
    "    stor.append(data_cluster(cluster_list[i]))\n",
    "\n",
    "  # Combine the dataframes in stor into a single dataframe\n",
    "  combined_df = pd.concat(stor, ignore_index=True)\n",
    "\n",
    "  cluster_list, merged_cluster = merge_clusters_and_list(combined_df, p)\n",
    "  data = change_labels(cluster_list, data)\n",
    "  return data, merged_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYLECdPU137x"
   },
   "outputs": [],
   "source": [
    "def run_clustering_no_plots(input_data, min_points=150, epsilon=500, t=150, w=5, max_points_ratio=0.5, cluster_similarity_threshold=0.8):\n",
    "  input_data = input_data.values\n",
    "\n",
    "  # Run OPTICS ordering\n",
    "  ordered_list = runOPTICS(input_data, epsilon, min_points)\n",
    "\n",
    "  # Do the gradient clustering\n",
    "  clusters = gradientClustering(ordered_list[:,1], min_points, t, w)\n",
    "\n",
    "  # Remove very large clusters\n",
    "  filtered_clusters = filterLargeClusters(clusters, len(ordered_list), max_points_ratio)\n",
    "\n",
    "  # Merge similar clusters by looking at the ratio of their intersection and their total number\n",
    "  filtered_clusters = mergeSimilarClusters(filtered_clusters, cluster_similarity_threshold)\n",
    "\n",
    "  return filtered_clusters, ordered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VgU2EQ_32EU9"
   },
   "outputs": [],
   "source": [
    "def saveResults(df, ordered_list, time, clusters=[], filename='results.csv'):\n",
    "  # Create an empty dataframe to store the results\n",
    "  results = pd.DataFrame(df)\n",
    "\n",
    "  # Plot clusters, if any\n",
    "  if clusters:\n",
    "        # Generate a list of colors for each cluster and randomize their order (so close clusters would have\n",
    "        # significcantly different color)\n",
    "        colors = cm.inferno(np.linspace(0.3, 1, len(clusters)))\n",
    "        color_order = random.sample(range(len(colors)), len(colors))\n",
    "\n",
    "        # Create a tuple to map each unique color to a unique integer ID\n",
    "        tuple_colors = [tuple(color) for color in colors]\n",
    "        unique_colors = list(set(tuple_colors))\n",
    "        color_to_id = {color: i for i, color in enumerate(unique_colors)}\n",
    "\n",
    "        results['Labels'] = np.nan\n",
    "        results['Time'] = np.nan\n",
    "        # Store the data in the results dataframe\n",
    "        for color, cluster in zip(colors[color_order], clusters):\n",
    "            x = ordered_list[cluster][:,3]\n",
    "            y = ordered_list[cluster][:,4]\n",
    "            labels = [color_to_id[tuple(color)]] * len(x)\n",
    "            # Replace column X and Y with x and y\n",
    "            results.loc[cluster, 'X'] = x\n",
    "            results.loc[cluster, 'Y'] = y\n",
    "\n",
    "             # Create the Labels column and store labels in the dataframe df\n",
    "            results.loc[cluster, 'Labels'] = labels\n",
    "            results.loc[cluster, 'Time'] = time\n",
    "\n",
    "  # Save the results to a CSV file\n",
    "  results = results.dropna(subset=['Labels'])\n",
    "  results.to_csv(filename, index=False)\n",
    "  return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YaXb8tcBi7jz"
   },
   "source": [
    "#### Example with some points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQ4Ghv4XjD77"
   },
   "source": [
    "##### T02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jzeeD3SnjDjf"
   },
   "outputs": [],
   "source": [
    "clusters, coord = run_clustering_no_plots(T10, min_points=200, epsilon=300, w=5.5, max_points_ratio=0.6, cluster_similarity_threshold=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0TwFGDIV5Fz4"
   },
   "outputs": [],
   "source": [
    "df10 = saveResults(df10, coord, 10, clusters, 'Results_010.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52z_lJjf69AT"
   },
   "outputs": [],
   "source": [
    "plotPointsFromCSV(df10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4HNDazz8q-x"
   },
   "outputs": [],
   "source": [
    "df02 = delete_outliers(df02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8DjG5rvezW6"
   },
   "source": [
    "This approach can allow to have info either about the clusters, or about the single points. Moreover, there would be the same label between clusters and single points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CDs9W1THyha8"
   },
   "source": [
    "##### T10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LE7KTDhymWZ"
   },
   "outputs": [],
   "source": [
    "clusters, coord = run_clustering_no_plots(T10, min_points=200, epsilon=300, w=5.5, max_points_ratio=0.6, cluster_similarity_threshold=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9RHELnuylv7"
   },
   "outputs": [],
   "source": [
    "df10 = saveResults(df10, coord, 10, clusters, 'Results_010.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76YM-2o-y2mU",
    "outputId": "61afa6b6-4008-4501-ad2a-f80e2db7b629"
   },
   "outputs": [],
   "source": [
    "plotPointsFromCSV(df10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlEWHnnZy35y"
   },
   "outputs": [],
   "source": [
    "df10 = delete_outliers(df10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mNhNDJScy4_6",
    "outputId": "7cc05831-43e1-475d-9752-66620dc6362c"
   },
   "outputs": [],
   "source": [
    "plotPointsFromCSV(df10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dz3RDoUazvdD"
   },
   "source": [
    "##### T40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RFfQmlx7z0iG"
   },
   "outputs": [],
   "source": [
    "clusters, coord = run_clustering(T40, min_points=40, epsilon=300, w=4.75, max_points_ratio=0.6, cluster_similarity_threshold=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZWPe9U4z9Zl"
   },
   "outputs": [],
   "source": [
    "df40 = saveResults(df40, coord, 40, clusters, 'Results_040.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z2rnWg4v0A2D",
    "outputId": "b4e8b05b-986e-4c36-b212-01d07eb963bd"
   },
   "outputs": [],
   "source": [
    "plotPointsFromCSV(df40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0YiP0v40djB"
   },
   "outputs": [],
   "source": [
    "df40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ED6ONCYw04lw"
   },
   "outputs": [],
   "source": [
    "cluster_list = listing_clusters(df40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNxXIG8s1EVf"
   },
   "outputs": [],
   "source": [
    "  stor = []\n",
    "  for i in range(len(cluster_list)):\n",
    "    stor.append(data_cluster(cluster_list[i]))\n",
    "\n",
    "  # Combine the dataframes in stor into a single dataframe\n",
    "  combined_df = pd.concat(stor, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FD_jynQ-1a9y"
   },
   "outputs": [],
   "source": [
    "cluster_list, merged_cluster = merge_clusters_and_list(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ZoBZm1M06WZ"
   },
   "outputs": [],
   "source": [
    "data = change_labels(cluster_list, df40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_e4JvoAX0KGV",
    "outputId": "924ffc39-ef04-46ff-f84e-6ffc3612a4c2"
   },
   "outputs": [],
   "source": [
    "plotPointsFromCSV(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ckn_7gLs2Y-P"
   },
   "source": [
    "##### T60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Btwb2Pry2f60"
   },
   "outputs": [],
   "source": [
    "clusters, coord = run_clustering_no_plots(T60, min_points=25,\n",
    "epsilon=250,\n",
    "w=4.5,\n",
    "max_points_ratio=0.8,\n",
    "cluster_similarity_threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jCfK_3TO2mZG"
   },
   "outputs": [],
   "source": [
    "df60 = saveResults(df60, coord, 60, clusters, 'Results_060.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HD2iF1pJ2n_R",
    "outputId": "7a202d06-99fa-4003-b47a-0f8b9befea72"
   },
   "outputs": [],
   "source": [
    "plotPointsFromCSV(df60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IeqWqfHs3Bjz"
   },
   "outputs": [],
   "source": [
    "df60_new, merged_cluster = merge_cluster_single_points(df60, p =300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "avMYO9nW3KF0",
    "outputId": "251af57e-7a39-478d-9116-33ce5cb8f891"
   },
   "outputs": [],
   "source": [
    "plotPointsFromCSV(df60_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSULPDG566XV"
   },
   "source": [
    "#### Generalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_JpDlJjGjj_"
   },
   "outputs": [],
   "source": [
    "df_list = []\n",
    "cluster_list = []\n",
    "for i in tqdm(range(len((csv_data)))):\n",
    "  clusters, coord = run_clustering_no_plots(t_data[i], min_points=min_points_fine[i], epsilon=epsilon_fine[i], w=w_fine[i], max_points_ratio=max_points_ratio_fine[i], cluster_similarity_threshold = cluster_similarity_threshold_fine[i])\n",
    "  df_tmp = saveResults(csv_data[i], coord, int(i + 1), clusters, 'Results_'+ str(i + 1) +'.csv')\n",
    "  df_tmp = delete_outliers(df_tmp)\n",
    "  df_tmp, merged_cluster = merge_cluster_single_points(df_tmp, p =p_fine[i])\n",
    "  df_list.append(df_tmp)\n",
    "  merged_cluster['Time'] = int(i + 1)\n",
    "  cluster_list.append(merged_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-83WO5gK061"
   },
   "source": [
    "We can save the value so they won't get lost in case of a problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGFsAMZ-gVY3"
   },
   "source": [
    "If I want to save all the files together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_f-JTlYK2z1"
   },
   "outputs": [],
   "source": [
    "# Specify the file path where you want to save the list\n",
    "file_path_df = '/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/Original_Algorithm/df_list.pkl'\n",
    "file_path_cluster = '/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/Original_Algorithm/cluster_list.pkl'\n",
    "# Open the file in binary write mode and save the list using pickle.dump\n",
    "with open(file_path_df, 'wb') as file:\n",
    "    pickle.dump(df_list, file)\n",
    "\n",
    "with open(file_path_cluster, 'wb') as file:\n",
    "    pickle.dump(cluster_list, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4-AVrtEgcYF"
   },
   "source": [
    "If I want to take my file back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZOli6FdK3pI"
   },
   "outputs": [],
   "source": [
    "# Specify the same file path where you saved the list\n",
    "file_path_df = '/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/Original_Algorithm/df_list.pkl'\n",
    "file_path_cluster = '/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/Original_Algorithm/cluster_list.pkl'\n",
    "\n",
    "\n",
    "# Load the first file (assuming it's a pickled object)\n",
    "with open(file_path_df, 'rb') as file:\n",
    "    df_list = pickle.load(file)\n",
    "\n",
    "# Load the second file (assuming it's a pickled object)\n",
    "with open(file_path_cluster, 'rb') as file:\n",
    "    cluster_list = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oxttv4LzgYKU"
   },
   "source": [
    "If I want to save the files indipendently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMGCkJ49gaUw"
   },
   "outputs": [],
   "source": [
    "file_path_cluster = '/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/Original_Algorithm/cluster_list.pkl'\n",
    "# Open the file in binary write mode and save the list using pickle.dump\n",
    "\n",
    "with open(file_path_cluster, 'wb') as file:\n",
    "    pickle.dump(cluster_list, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPmmr39SUbpm"
   },
   "source": [
    "#### Link clusters together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2dPkD8fQ0Qq"
   },
   "source": [
    "##### Used functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nabKU1I6MLUW"
   },
   "source": [
    "Now it's the turn to work with the files to link the clusters together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uhUtCQTEIFtP"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_distance_euclide(x1, y1, x2, y2):\n",
    "    distance_x = x2 - x1\n",
    "    distance_y = y2 - y1\n",
    "    euclidean_distance = math.sqrt(distance_x**2 + distance_y**2)\n",
    "    return euclidean_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nxnf-LDoNpjg"
   },
   "outputs": [],
   "source": [
    "def calculate_distance(x1, y1, x2, y2):\n",
    "    distance_x = abs(x2 - x1)\n",
    "    distance_y = abs(y2 - y1)\n",
    "    return distance_x, distance_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3x-CLS2rm-JJ"
   },
   "outputs": [],
   "source": [
    "def link_clusters(far_time_point, recent_time_point, linked_clusters, p = 0, s = 0):\n",
    "    k = 1\n",
    "    linked_clusters = linked_clusters.copy()\n",
    "    far_time_point = far_time_point.copy()\n",
    "    recent_time_point = recent_time_point.copy()\n",
    "    swap_list = []\n",
    "    N_min = 5000 - s\n",
    "    for i in range(len(recent_time_point)):\n",
    "        T_min = float('inf')  # Reset T_min for each recent_time_point iteration\n",
    "        predefined = 1000 - p\n",
    "        closest = None  # Initialize closest index\n",
    "\n",
    "        for j in range(len(far_time_point)):\n",
    "            distance_euclide = calculate_distance_euclide(recent_time_point.iloc[i]['X_cluster_location'], recent_time_point.iloc[i]['Y_cluster_location'], far_time_point.iloc[j]['X_cluster_location'], far_time_point.iloc[j]['Y_cluster_location'])\n",
    "            difference_cells = abs(recent_time_point.iloc[i]['N_cell'] - far_time_point.iloc[j]['N_cell'])\n",
    "            if distance_euclide < T_min and distance_euclide < predefined and difference_cells < N_min:\n",
    "                T_min = distance_euclide  # Update T_min with the minimum distance\n",
    "                closest = j\n",
    "\n",
    "        if closest is not None:\n",
    "            # I want the row to have the same label\n",
    "            swap_list.append(recent_time_point.iloc[i]['Labels'])\n",
    "            swap_list.append(-far_time_point.iloc[closest]['Labels'])\n",
    "            recent_time_point.loc[i, 'Labels'] = far_time_point.loc[closest, 'Labels']\n",
    "            # Remove the cluster considered from far_time_point\n",
    "            far_time_point.drop(index=closest, inplace=True)\n",
    "            far_time_point.reset_index(drop=True, inplace=True)\n",
    "        else:\n",
    "            swap_list.append(recent_time_point.iloc[i]['Labels'])\n",
    "            recent_time_point.loc[i, 'Labels'] = (max(linked_clusters['Labels']) + k)\n",
    "            k = k + 1\n",
    "            swap_list.append(recent_time_point.iloc[i]['Labels'])\n",
    "\n",
    "\n",
    "    dataframes_to_concat = [linked_clusters, recent_time_point]\n",
    "\n",
    "    # Use pd.concat to concatenate the DataFrames\n",
    "    linked_clusters = pd.concat(dataframes_to_concat, axis=0, ignore_index=True)\n",
    "\n",
    "    return linked_clusters, recent_time_point, swap_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QDYhHXSEiAxp"
   },
   "outputs": [],
   "source": [
    "def swapping_labels(df_all, df_new, swap_list):\n",
    "    df_all = df_all.copy()\n",
    "    df_new = df_new.copy()\n",
    "\n",
    "    df_new = change_labels(swap_list, df_new)\n",
    "    df_new['Labels'] = -df_new['Labels']\n",
    "    df_new['Labels'] = abs(df_new['Labels'])\n",
    "\n",
    "    dataframes_to_concat = [df_all, df_new]\n",
    "    # Use pd.concat to concatenate the DataFrames\n",
    "    df_all = pd.concat(dataframes_to_concat, axis=0, ignore_index=True)\n",
    "\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjRtAJVa9YyS"
   },
   "outputs": [],
   "source": [
    "def change_labels(cluster_list, data):\n",
    "    data = data.copy()\n",
    "    for i in range(len(cluster_list)):\n",
    "        if i % 2 == 0:  # Check if 'i' is even\n",
    "            if (cluster_list[i + 1] == 0):\n",
    "                cluster_list[i + 1] = -1000  # just a big number\n",
    "            data.loc[data['Labels'].astype(str) == str(cluster_list[i]), 'Labels'] = cluster_list[i + 1]\n",
    "\n",
    "    data.loc[data['Labels'] == -1000, 'Labels'] = 0\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "px-Fkp0Mt4dJ"
   },
   "outputs": [],
   "source": [
    "def cluster_display_plots_in_colab(data):\n",
    "    # Iterate over each timepoint in your DataFrame\n",
    "    for timepoint in data['Time'].unique():\n",
    "        # Create a sub-dataframe for the current timepoint\n",
    "        sub_data = data[data['Time'] == timepoint]\n",
    "\n",
    "        # Create a new figure\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        # Call your plotting function with the sub-dataframe\n",
    "        cluster_same_colour(sub_data, title=f\"Timepoint {timepoint}\")\n",
    "\n",
    "        # Display the current figure in Colab\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NxLJ96mBvmB5"
   },
   "outputs": [],
   "source": [
    "def display_png_plots(data):\n",
    "    # Iterate over each timepoint in your DataFrame\n",
    "    for timepoint in data['Time'].unique():\n",
    "        # Create a sub-dataframe for the current timepoint\n",
    "        sub_data = data[data['Time'] == timepoint]\n",
    "\n",
    "        # Create a new figure\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        # Call your plotting function with the sub-dataframe\n",
    "        plotPointsFromCSV_same_colour(sub_data, title=f\"Timepoint {timepoint}\")\n",
    "\n",
    "        # Display the current figure in the Colab notebook\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvZsK1FrPhVX"
   },
   "outputs": [],
   "source": [
    "def plotPointsFromCSV_same_colour(data, title=''):\n",
    "    # Extract X, Y, and Labels columns from the DataFrame\n",
    "    points = data[['X', 'Y']].to_numpy()\n",
    "    labels = data['Labels']\n",
    "\n",
    "    # Find unique cluster labels\n",
    "    unique_labels = labels.unique()\n",
    "\n",
    "    # Define a color palette for the first 10 labels\n",
    "    palette = sns.color_palette(\"Set1\", n_colors=100)\n",
    "\n",
    "    # Create a dictionary to map labels to colors\n",
    "    label_to_color = {label: color for label, color in zip(unique_labels, palette)}\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot all points in black\n",
    "    ax.scatter(points[:, 0], points[:, 1], c='k', linewidth=0.2, edgecolor='w', facecolor=None)\n",
    "\n",
    "    for label in unique_labels:\n",
    "        color = label_to_color.get(label, 'k')  # Use black for labels not in the first 10\n",
    "        cluster = points[labels == label]\n",
    "        ax.scatter(cluster[:, 0], cluster[:, 1], color=color, linewidth=0.2, edgecolor='w', label=str(label))\n",
    "\n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Set the X and Y limits to ensure a fixed size of 6500x6500 units\n",
    "    ax.set_xlim(0, 6500)\n",
    "    ax.set_ylim(0, 6500)\n",
    "\n",
    "    # Turn on the grid, set color to grey\n",
    "    ax.grid(color='0.5')\n",
    "\n",
    "    # Set background color to black\n",
    "    ax.set_facecolor('black')\n",
    "\n",
    "    # Set the ratio to the window size 1:1\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Add a legend for labels\n",
    "    ax.legend(title='Labels', loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahTpLpFKRvCF"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plotPointsFromCSV_same_colour_unique(data, title=''):\n",
    "    # Extract X, Y, and Labels columns from the DataFrame\n",
    "    points = data[['X', 'Y']].to_numpy()\n",
    "    labels = data['Labels']\n",
    "\n",
    "    # Find unique cluster labels\n",
    "    unique_labels = labels.unique()\n",
    "\n",
    "    # Define a color palette for the labels\n",
    "    palette = sns.color_palette(\"Set1\", n_colors=len(unique_labels))\n",
    "\n",
    "    # Create a dictionary to map labels to colors\n",
    "    label_to_color = {label: color for label, color in zip(unique_labels, palette)}\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot all points in black\n",
    "    ax.scatter(points[:, 0], points[:, 1], c='k', linewidth=0.2, edgecolor='w', facecolor=None)\n",
    "\n",
    "    for label in unique_labels:\n",
    "        color = label_to_color.get(label, 'k')  # Use black for labels not in the palette\n",
    "        cluster = points[labels == label]\n",
    "        ax.scatter(cluster[:, 0], cluster[:, 1], color=color, linewidth=0.2, edgecolor='w', label=str(label))\n",
    "\n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Set the X and Y limits to ensure a fixed size of 6500x6500 units\n",
    "    ax.set_xlim(0, 6500)\n",
    "    ax.set_ylim(0, 6500)\n",
    "\n",
    "    # Turn on the grid, set color to grey\n",
    "    ax.grid(color='0.5')\n",
    "\n",
    "    # Set background color to black\n",
    "    ax.set_facecolor('black')\n",
    "\n",
    "    # Set the ratio to the window size 1:1\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Add a legend for labels\n",
    "    ax.legend(title='Labels', loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dzfoQMv9QMQa"
   },
   "outputs": [],
   "source": [
    "def cluster_same_colour(data, title=''):\n",
    "    # Extract X, Y, and Labels columns from the DataFrame\n",
    "    points = data[['X_cluster_location', 'Y_cluster_location']].to_numpy()\n",
    "    labels = data['Labels']\n",
    "\n",
    "    # Find unique cluster labels\n",
    "    unique_labels = labels.unique()\n",
    "\n",
    "    # Define a color palette for the first 10 labels\n",
    "    palette = sns.color_palette(\"Set1\", n_colors=10)\n",
    "\n",
    "    # Create a dictionary to map labels to colors\n",
    "    label_to_color = {label: color for label, color in zip(unique_labels, palette)}\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot all points in black\n",
    "    ax.scatter(points[:, 0], points[:, 1], c='k', linewidth=0.2, edgecolor='w', facecolor=None)\n",
    "\n",
    "    for label in unique_labels:\n",
    "        color = label_to_color.get(label, 'k')  # Use black for labels not in the first 10\n",
    "        cluster = points[labels == label]\n",
    "        ax.scatter(cluster[:, 0], cluster[:, 1], color=color, linewidth=0.2, edgecolor='w', label=str(label))\n",
    "\n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Set the X and Y limits to ensure a fixed size of 6500x6500 units\n",
    "    ax.set_xlim(0, 6500)\n",
    "    ax.set_ylim(0, 6500)\n",
    "\n",
    "    # Turn on the grid, set color to grey\n",
    "    ax.grid(color='0.5')\n",
    "\n",
    "    # Set background color to black\n",
    "    ax.set_facecolor('black')\n",
    "\n",
    "    # Set the ratio to the window size 1:1\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Add a legend for labels\n",
    "    ax.legend(title='Labels', loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-RMJU7JQNod"
   },
   "outputs": [],
   "source": [
    "def cluster_display_plots_in_colab(data):\n",
    "    # Iterate over each timepoint in your DataFrame\n",
    "    for timepoint in data['Time'].unique():\n",
    "        # Create a sub-dataframe for the current timepoint\n",
    "        sub_data = data[data['Time'] == timepoint]\n",
    "\n",
    "        # Create a new figure\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        # Call your plotting function with the sub-dataframe\n",
    "        cluster_same_colour(sub_data, title=f\"Timepoint {timepoint}\")\n",
    "\n",
    "        # Display the current figure in Colab\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def display_plots_in_colab(data):\n",
    "    # Iterate over each timepoint in your DataFrame\n",
    "    for timepoint in data['Time'].unique():\n",
    "        # Create a sub-dataframe for the current timepoint\n",
    "        sub_data = data[data['Time'] == timepoint]\n",
    "\n",
    "        # Create a new figure\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        # Call your plotting function with the sub-dataframe\n",
    "        plotPointsFromCSV_same_colour(sub_data, title=f\"Timepoint {timepoint}\")\n",
    "\n",
    "        # Display the current figure in Colab\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppWzZx2RQeV1"
   },
   "source": [
    "perform Linking values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vKguf8WQkZu"
   },
   "source": [
    "##### Iteration for Original_Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXrQN4NxFQ9I"
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "clusters_ori = pd.read_csv('/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/Original_Algorithm/linked_clusters.csv')\n",
    "df_ori = pd.read_csv('/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/Original_Algorithm/df_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6rm1dNhJpij"
   },
   "outputs": [],
   "source": [
    "# Specify the same file path where you saved the list\n",
    "file_path_df_or = '/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/Original_Algorithm/df_list.pkl'\n",
    "file_path_cluster_or = '/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/Original_Algorithm/cluster_list.pkl'\n",
    "\n",
    "# Load the first file (assuming it's a pickled object)\n",
    "with open(file_path_df_or, 'rb') as file:\n",
    "    df_list = pickle.load(file)\n",
    "\n",
    "# Load the second file (assuming it's a pickled object)\n",
    "with open(file_path_cluster_or, 'rb') as file:\n",
    "    cluster_list = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sE8ScOsUkBYU",
    "outputId": "5d4a3e14-1409-4fdc-899c-33c5ef788270"
   },
   "outputs": [],
   "source": [
    "linked_clusters = pd.DataFrame()\n",
    "df_all = pd.DataFrame()\n",
    "linked_clusters = cluster_list[0].copy()\n",
    "df_all = df_list[0].copy()\n",
    "new_old_time_point = cluster_list[0].copy()\n",
    "for i in tqdm(range(len((df_list)) - 1)):\n",
    "  linked_clusters, new_old_time_point, swap_list = link_clusters(new_old_time_point, cluster_list[i + 1], linked_clusters, p_fine[i+1], s_fine[i])\n",
    "  df_all = swapping_labels(df_all, df_list[i + 1], swap_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9cXiHfWGglY"
   },
   "source": [
    "It may be better to save the new results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MN757i9YGfh1"
   },
   "outputs": [],
   "source": [
    "# Save df1 to Google Drive\n",
    "linked_clusters.to_csv('/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/Original_Algorithm/linked_clusters_v2.csv', index=False)\n",
    "\n",
    "# Save df2 to Google Drive\n",
    "df_all.to_csv('/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/Original_Algorithm/df_all_v2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwKdMgd4seJo"
   },
   "source": [
    "#### Iteration for New_algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHDZmd9Vso9K"
   },
   "source": [
    "This code has the function of including the segmentations coming from New_algorithm. As less cells were considered (and mostly those of the 4 main clusters included), it may be easier to have constant values, as well as study main clusters over time. A comparison between multiple algorithms can be useful to understand multiple points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHRp6L4gJ80r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KMCgOj45J9Vo",
    "outputId": "a7deb8a1-2d2f-4496-f376-6bd0270e6374"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z1W0gFxytCYr",
    "outputId": "a1d959d4-5959-4622-f40b-16f43eb924a8"
   },
   "outputs": [],
   "source": [
    "# Define the folder path in your Google Drive containing the CSV files\n",
    "folder_path = '/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/New_algorithm/Raw CSV'\n",
    "\n",
    "# List all CSV files in the folder and sort them alphabetically\n",
    "csv_files_new = sorted([csv_file for csv_file in os.listdir(folder_path) if csv_file.endswith('.csv')])\n",
    "\n",
    "csv_data_new = []  # List to store dataframes\n",
    "\n",
    "for csv_file_new in tqdm(csv_files_new, desc=\"Processing CSV files\"):\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file_new))\n",
    "    csv_data_new.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WSseK28ytCmM",
    "outputId": "154aa721-e86d-4d14-f839-018301166add"
   },
   "outputs": [],
   "source": [
    "t_data_new = []\n",
    "for i in tqdm(range(len((csv_data)))):\n",
    "  t_data_new.append(csv_data_new[i][['X', 'Y']].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OnipTjbzHxVz",
    "outputId": "6fd56472-21b7-4e72-a27e-80110adab6d5"
   },
   "outputs": [],
   "source": [
    "df_list_new = []\n",
    "cluster_list_new = []\n",
    "for i in tqdm(range(len((csv_data_new)))):\n",
    "  clusters, coord = run_clustering_no_plots(t_data_new[i], min_points=min_points_fine_new[i], epsilon=epsilon_fine_new[i], w=w_fine_new[i], max_points_ratio=max_points_ratio_fine_new[i], cluster_similarity_threshold = cluster_similarity_threshold_fine_new[i])\n",
    "  df_tmp = saveResults(csv_data_new[i], coord, int(i + 1), clusters, 'Results_'+ str(i + 1) +'.csv')\n",
    "  df_tmp = delete_outliers(df_tmp)\n",
    "  df_tmp, merged_cluster = merge_cluster_single_points(df_tmp, p =p_fine[i])\n",
    "  df_list_new.append(df_tmp)\n",
    "  merged_cluster['Time'] = int(i + 1)\n",
    "  cluster_list_new.append(merged_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxMQoPhzpKj2"
   },
   "source": [
    "To save the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ucyaP9mGFg6"
   },
   "source": [
    "Here there is a previous part that has been considered. In this case the purpose of this part is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKiOFjudKL_N"
   },
   "outputs": [],
   "source": [
    "# Specify the file path where you want to save the list\n",
    "file_path_df_new = '/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/New_algorithm/df_list_new.pkl'\n",
    "file_path_cluster_new = '/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/New_algorithm/cluster_list_new.pkl'\n",
    "# Open the file in binary write mode and save the list using pickle.dump\n",
    "with open(file_path_df_new, 'wb') as file:\n",
    "    pickle.dump(df_list_new, file)\n",
    "\n",
    "with open(file_path_cluster_new, 'wb') as file:\n",
    "    pickle.dump(cluster_list_new, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bC43qop-pf13"
   },
   "source": [
    "To take the list that has been saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1wxu7EnpiIm"
   },
   "outputs": [],
   "source": [
    "# Specify the same file path where you saved the list\n",
    "file_path_df_new = '/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/New_algorithm/df_list_new.pkl'\n",
    "file_path_cluster_new = '/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/New_algorithm/cluster_list_new.pkl'\n",
    "\n",
    "# Load the first file (assuming it's a pickled object)\n",
    "with open(file_path_df_new, 'rb') as file:\n",
    "    df_list_new = pickle.load(file)\n",
    "\n",
    "# Load the second file (assuming it's a pickled object)\n",
    "with open(file_path_cluster_new, 'rb') as file:\n",
    "    cluster_list_new = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Revl9X8H9dl",
    "outputId": "0073dbd8-eb95-4d75-92fe-ec4f2218d72a"
   },
   "outputs": [],
   "source": [
    "linked_clusters_new = pd.DataFrame()\n",
    "df_all_new = pd.DataFrame()\n",
    "linked_clusters_new = cluster_list_new[0].copy()\n",
    "df_all_new = df_list_new[0].copy()\n",
    "new_old_time_point = cluster_list_new[0].copy()\n",
    "for i in tqdm(range(len((df_list_new)) - 1)):\n",
    "  linked_clusters_new, new_old_time_point, swap_list = link_clusters(new_old_time_point, cluster_list_new[i + 1], linked_clusters_new, p_fine[i+1], s_fine[i])\n",
    "  df_all_new = swapping_labels(df_all_new, df_list_new[i + 1], swap_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ah7iXczKKoqV"
   },
   "outputs": [],
   "source": [
    "# Save df1 to Google Drive\n",
    "linked_clusters_new.to_csv('/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/New_algorithm/linked_clusters_new.csv', index=False)\n",
    "\n",
    "# Save df2 to Google Drive\n",
    "df_all_new.to_csv('/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/New_algorithm/df_all_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aAbDMScNDM6T"
   },
   "outputs": [],
   "source": [
    "linked_clusters_new = pd.read_csv('/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/New_algorithm/linked_clusters_new.csv')\n",
    "df_all_new = pd.read_csv('/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/New_algorithm/df_all_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7usew3jSgj4"
   },
   "source": [
    "###### Code that I have to put above. It's the \"right one\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P114PKdzjAaY"
   },
   "source": [
    "## Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iAKoT6-FFuZ"
   },
   "source": [
    "### General plot analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpqPIZlgR_FH"
   },
   "source": [
    "#### Used functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyNw5oSfQVCJ"
   },
   "source": [
    "Obtain a PDF from values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fdpd9kl8QiYQ"
   },
   "source": [
    "You can download .png files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4XVgQso7W1sl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def create_png_plots(data, output_folder):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Iterate over each timepoint in your DataFrame\n",
    "    for timepoint in data['Time'].unique():\n",
    "        # Create a sub-dataframe for the current timepoint\n",
    "        sub_data = data[data['Time'] == timepoint]\n",
    "\n",
    "        # Call your plotting function with the sub-dataframe and get the figure object\n",
    "        fig = plotPointsFromCSV_same_colour(sub_data, title=f\"Timepoint {timepoint}\")\n",
    "\n",
    "        # Define the output PNG file path and name\n",
    "        output_file = os.path.join(output_folder, f\"timepoint_{timepoint}.png\")\n",
    "\n",
    "        # Save the current figure as a PNG file\n",
    "        fig.savefig(output_file, dpi=300)  # You can adjust the DPI as needed\n",
    "\n",
    "        # Close the current figure to free up memory\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jO5TvVNURQ6J"
   },
   "outputs": [],
   "source": [
    "def merge_pngs_to_pdf(gdrive_folder, output_pdf_name):\n",
    "    # Mount Google Drive to access the folder with PNG files\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Define the full path to the folder containing PNG files\n",
    "    folder_path = os.path.join('/content/drive/My Drive', gdrive_folder)\n",
    "\n",
    "    # List PNG files in the folder\n",
    "    png_files = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if filename.endswith('.png')]\n",
    "\n",
    "    if not png_files:\n",
    "        print(\"No PNG files found in the folder.\")\n",
    "        return\n",
    "\n",
    "    # Initialize the PDF document\n",
    "    pdf_path = os.path.join('/content', output_pdf_name)\n",
    "    c = canvas.Canvas(pdf_path, pagesize=letter)\n",
    "\n",
    "    # Iterate through PNG files and add them to the PDF\n",
    "    for png_file in tqdm(png_files, desc=\"Merging to PDF\"):\n",
    "        img = Image.open(png_file)\n",
    "        img_width, img_height = img.size\n",
    "        img_width //= 3.5\n",
    "        img_height //= 3.5\n",
    "        # Add the PNG image to the PDF page\n",
    "        # Adjust the X-coordinate to move the image to the left\n",
    "        x_offset = 20  # You can adjust this value as needed\n",
    "        c.drawImage(png_file, x_offset, 0, width=img_width, height=img_height)\n",
    "\n",
    "        # Add a new page for the next image (if any)\n",
    "        c.showPage()\n",
    "\n",
    "    # Save the PDF document\n",
    "    c.save()\n",
    "\n",
    "    return pdf_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "muqdpHFhS3Rg"
   },
   "outputs": [],
   "source": [
    "def adjust_clusters_disappeared(data, threshold = 70):\n",
    "  data = data.copy()\n",
    "  # Sample dataframe 'df' with columns 'Label', 'X', and 'Y'\n",
    "  # threshold is your desired threshold value\n",
    "\n",
    "  # Group the dataframe by 'Label' and calculate the mean for each cluster\n",
    "  cluster_means = data.groupby('Labels')[['X', 'Y']].mean()\n",
    "\n",
    "  # Initialize a dictionary to store the minimum label for each cluster\n",
    "  min_label_for_cluster = {}\n",
    "\n",
    "  # Iterate through the cluster means\n",
    "  for label, row in cluster_means.iterrows():\n",
    "      x_mean = row['X']\n",
    "      y_mean = row['Y']\n",
    "\n",
    "      # Convert label to an integer\n",
    "      label = int(label)\n",
    "\n",
    "      # Check if the difference between the current cluster and clusters with smaller labels is under the threshold\n",
    "      for smaller_label in range(label):\n",
    "        if smaller_label != 1:\n",
    "              smaller_x_mean, smaller_y_mean = cluster_means.loc[smaller_label]\n",
    "              if abs(x_mean - smaller_x_mean) < threshold and abs(y_mean - smaller_y_mean) < threshold:\n",
    "                  min_label_for_cluster[label] = smaller_label\n",
    "                  break\n",
    "\n",
    "  # Update the 'Label' column in the original dataframe based on the dictionary\n",
    "  for label, min_label in min_label_for_cluster.items():\n",
    "      data.loc[data['Labels'] == label, 'Labels'] = min_label\n",
    "  return data\n",
    "# Now 'df' contains updated labels based on the smallest label within the threshold difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IVFVQZqUS38_"
   },
   "outputs": [],
   "source": [
    "def order_labels(df):\n",
    "  df = df.copy()\n",
    "  # Create a mapping dictionary\n",
    "  mapping = {val: idx for idx, val in enumerate(sorted(df['Labels'].unique()))}\n",
    "\n",
    "  # Apply the mapping to the \"Labels\" column\n",
    "  df['Labels'] = df['Labels'].map(mapping)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hP6aLdx8gt6G"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import gmean\n",
    "import pandas as pd\n",
    "\n",
    "def data_cluster_updated(cluster):\n",
    "    Geo_light_intensity = gmean(cluster['IntDen'])\n",
    "    Geo_cell_size = gmean(cluster['Area'])\n",
    "    Cluster_dimension = sum(cluster['Area'])\n",
    "    N_cell = len(cluster)\n",
    "    X = sum(cluster['X']) / len(cluster)\n",
    "    Y = sum(cluster['Y']) / len(cluster)\n",
    "    Labels = cluster['Labels'].iloc[0]  # Get the label value for this cluster\n",
    "\n",
    "    return pd.Series({\n",
    "        'Geo_light_intensity': Geo_light_intensity,\n",
    "        'Geo_cell_size': Geo_cell_size,\n",
    "        'Cluster_dimension': Cluster_dimension,\n",
    "        'N_cell': N_cell,\n",
    "        'X': X,\n",
    "        'Y': Y,\n",
    "        'Cluster_Labels': Labels\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIAzNGogf2YE"
   },
   "source": [
    "#### New Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8uj4FH-QSuL"
   },
   "outputs": [],
   "source": [
    "df = df_all_new_updated.copy()\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "df = df[df['Labels'].isin(selected_labels)]\n",
    "\n",
    "data = df\n",
    "output_folder_png = '/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/New_algorithm/clusters/N=0123'\n",
    "create_png_plots(data, output_folder_png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8L-B1GCtRnD1"
   },
   "outputs": [],
   "source": [
    "gdrive_folder = '/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/New_algorithm/clusters/N=0123'\n",
    "output_pdf_name = '/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/New_algorithm/clusters/N=0123/new_model.pdf'\n",
    "\n",
    "result_pdf_path = merge_pngs_to_pdf(gdrive_folder, output_pdf_name)\n",
    "\n",
    "print(f\"PDF saved at: {result_pdf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyx0tcahj7HM"
   },
   "source": [
    "##### Fixing some values due to code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6Unci6ni0MS"
   },
   "source": [
    "What is possible to notice is that the value here is really close. And there are just minimal differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VUmsaSeKmyZE"
   },
   "outputs": [],
   "source": [
    "df_all_new = adjust_clusters_disappeared(df_all_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-VHY8acsnop3"
   },
   "outputs": [],
   "source": [
    "data = df_all_new\n",
    "output_folder_png = '/content/drive/MyDrive/Academic Work/Utkarsha Khanal/CSV_files/New_algorithm/png_files_updated'\n",
    "create_png_plots(data, output_folder_png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W3gcuSmuneJu",
    "outputId": "c5575957-1be3-4ae9-9479-a9d810bbc931"
   },
   "outputs": [],
   "source": [
    "gdrive_folder = '/content/drive/MyDrive/Academic Work/Utkarsha Khanal/CSV_files/New_algorithm/png_files_updated'\n",
    "output_pdf_name = '/content/drive/MyDrive/Academic Work/Utkarsha Khanal/CSV_files/New_algorithm/png_files/new_model_updated.pdf'\n",
    "\n",
    "result_pdf_path = merge_pngs_to_pdf(gdrive_folder, output_pdf_name)\n",
    "\n",
    "print(f\"PDF saved at: {result_pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DsQ6Tn61g_-"
   },
   "outputs": [],
   "source": [
    "output_folder_png = '/content/drive/MyDrive/Academic Work/Utkarsha Khanal/CSV_files/New_algorithm/png_files_updated_to_delete'\n",
    "create_png_plots(proviamo, output_folder_png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p2ucpYUD1ha7",
    "outputId": "45f61fbb-7a67-4c54-f421-8186077b4df3"
   },
   "outputs": [],
   "source": [
    "gdrive_folder = '/content/drive/MyDrive/Academic Work/Utkarsha Khanal/CSV_files/New_algorithm/png_files_updated_to_delete'\n",
    "output_pdf_name = '/content/drive/MyDrive/Academic Work/Utkarsha Khanal/CSV_files/New_algorithm/png_files_updated_to_delete/new_model_updated_to_delete.pdf'\n",
    "\n",
    "result_pdf_path = merge_pngs_to_pdf(gdrive_folder, output_pdf_name)\n",
    "\n",
    "print(f\"PDF saved at: {result_pdf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxtGLtxt5D1L"
   },
   "source": [
    "perform final adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BxnpRBMgUMjS"
   },
   "outputs": [],
   "source": [
    "df_all_new = order_labels(df_all_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDj27he-5Fwf"
   },
   "outputs": [],
   "source": [
    "# Define replacement dictionary\n",
    "replacement_dict = {35: 3, 97:3, 24:3, 106:3, 30:1, 22:1, 84:1, 95:1, 103:2, 28:2, 129:0, 39:0}\n",
    "\n",
    "# Replace values in the 'label' column using the dictionary\n",
    "df_all_new['Labels'] = df_all_new['Labels'].replace(replacement_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFAj0wfR6reG"
   },
   "outputs": [],
   "source": [
    "df_all_new = order_labels(df_all_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrIxn1nY6uXp"
   },
   "source": [
    "Here we can save our values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uhuijknQ838_"
   },
   "outputs": [],
   "source": [
    "output_folder_png = '/content/drive/MyDrive/Academic Work/Utkarsha Khanal/CSV_files/New_algorithm/png_files_updated_completev2'\n",
    "create_png_plots(proviamo, output_folder_png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KHw88HRg86vt",
    "outputId": "2242db4d-c9f4-44c9-a597-f5933a74fac3"
   },
   "outputs": [],
   "source": [
    "gdrive_folder = '/content/drive/MyDrive/Academic Work/Utkarsha Khanal/CSV_files/New_algorithm/png_files_updated_completev2'\n",
    "output_pdf_name = '/content/drive/MyDrive/Academic Work/Utkarsha Khanal/CSV_files/New_algorithm/png_files_updated_completev2/new_model_updated_complete.pdf'\n",
    "\n",
    "result_pdf_path = merge_pngs_to_pdf(gdrive_folder, output_pdf_name)\n",
    "\n",
    "print(f\"PDF saved at: {result_pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FxEkaNm7yIkj",
    "outputId": "1bab2277-b942-4f36-d438-6dff0440a221"
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "  selected_label = i\n",
    "  selected_cluster = df_all_new_updated[df_all_new_updated['Labels'] == selected_label]\n",
    "\n",
    "  output_folder_png = '/content/drive/MyDrive/Academic Work/Utkarsha Khanal/CSV_files/New_algorithm/clusters/N='+ str(i)\n",
    "  create_png_plots(selected_cluster, output_folder_png)\n",
    "\n",
    "  gdrive_folder = '/content/drive/MyDrive/Academic Work/Utkarsha Khanal/CSV_files/New_algorithm/clusters/N='+ str(i)\n",
    "  output_pdf_name = '/content/drive/MyDrive/Academic Work/Utkarsha Khanal/CSV_files/New_algorithm/clusters/N='+ str(i) + '/complete_pdf_N=' + str(i) + '.pdf'\n",
    "\n",
    "  result_pdf_path = merge_pngs_to_pdf(gdrive_folder, output_pdf_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aovlPkopF5S"
   },
   "source": [
    "As there is the same data frame, it's also possible to select just one particular cluster and to see how it would develop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3huwTCbzqPD"
   },
   "source": [
    "after we have saved the final value we can also create a new cluster from it. So we can analyse our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1uMySVsIyuu"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XaoaWw3-1Sh1"
   },
   "outputs": [],
   "source": [
    "df_all_new_updated = pd.read_csv('/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/New_algorithm/df_all_new_updated_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQtO0jT0KTT-"
   },
   "source": [
    "Obtain updated clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qzDw53I-NEvj"
   },
   "outputs": [],
   "source": [
    "cluster_partial_1 = df_all_new_updated.groupby(['Labels', 'Time']).mean().reset_index()\n",
    "\n",
    "# Group by 'Labels' and 'Time' and apply the custom aggregation function\n",
    "cluster_partial_2 = df_all_new_updated.copy()\n",
    "\n",
    "# Group by 'Old_Labels' and 'Time' and apply the custom aggregation function\n",
    "cluster_partial_2 = cluster_partial_2.groupby(['Labels', 'Time']).apply(data_cluster_updated).reset_index()\n",
    "\n",
    "# Merge based on the 'Labels' and 'Time' columns using an inner join\n",
    "merged_df = pd.merge(result_df, result_df1, how='inner', on=['Labels', 'Time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYxYjJo2vhF1"
   },
   "outputs": [],
   "source": [
    "merged_df.to_csv('/content/drive/MyDrive/Academic Work/Utkarsha Khanal/CSV_files/New_algorithm/cluster_new_updated_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1wsRB328wd1I"
   },
   "outputs": [],
   "source": [
    "cluster_new_updated = pd.read_csv('/content/drive/MyDrive/Academic Work/Utkarsha Khanal/CSV_files/New_algorithm/cluster_new_updated_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeXqjw1jhXlN"
   },
   "source": [
    "###### Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "id": "pgRBoqxvf-zf",
    "outputId": "0c8461c5-18be-4f8c-ff03-2b6c3218410a"
   },
   "outputs": [],
   "source": [
    "df = cluster_new_updated.copy()\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "df = df[df['Labels'].isin(selected_labels)]\n",
    "\n",
    "# Get unique labels from the DataFrame\n",
    "unique_labels = df['Labels'].unique()\n",
    "\n",
    "# Create a 3D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot each label's X_x and Y_y positions over time\n",
    "for label in unique_labels:\n",
    "    label_data = df[df['Labels'] == label]\n",
    "\n",
    "    ax.plot(label_data['Time'], label_data['X_x'], label_data['Y_y'], label=label)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('X_x')\n",
    "ax.set_zlabel('Y_y')\n",
    "plt.title('3D Plot of Position vs. Time for Each Label')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dmi86iDhedt"
   },
   "source": [
    "###### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xqs5z9z3ZvyB"
   },
   "outputs": [],
   "source": [
    "# Calculate Pearson correlation\n",
    "correlation_matrix_pearson = clustered_df.corr()\n",
    "\n",
    "# Calculate Spearman correlation\n",
    "correlation_matrix_spearman = clustered_df.corr(method='spearman')\n",
    "\n",
    "# Calculate Kendall correlation\n",
    "correlation_matrix_kendall = clustered_df.corr(method='kendall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    },
    "id": "8p8WPWBOaVE2",
    "outputId": "a534ac7b-9340-4307-bb7a-6b67a36a0142"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "correlation_matrix = correlation_matrix_pearson\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the correlation matrix with a background gradient\n",
    "cax = ax.matshow(correlation_matrix, cmap='coolwarm')\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = fig.colorbar(cax)\n",
    "\n",
    "# Set the precision of the colorbar labels to 2 decimal places\n",
    "cbar.set_label('Correlation', rotation=270, labelpad=15)\n",
    "cbar.set_ticks([-1, -0.5, 0, 0.5, 1])\n",
    "cbar.set_ticklabels([-1, -0.5, 0, 0.5, 1])\n",
    "\n",
    "# Set the x and y axis labels and titles\n",
    "plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=90)\n",
    "plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)\n",
    "plt.title('Correlation Matrix')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    },
    "id": "gRC2CfEDaiCr",
    "outputId": "af55b3ad-c7dc-48a5-fc92-b369605121af"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "correlation_matrix = correlation_matrix_spearman\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the correlation matrix with a background gradient\n",
    "cax = ax.matshow(correlation_matrix, cmap='coolwarm')\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = fig.colorbar(cax)\n",
    "\n",
    "# Set the precision of the colorbar labels to 2 decimal places\n",
    "cbar.set_label('Correlation', rotation=270, labelpad=15)\n",
    "cbar.set_ticks([-1, -0.5, 0, 0.5, 1])\n",
    "cbar.set_ticklabels([-1, -0.5, 0, 0.5, 1])\n",
    "\n",
    "# Set the x and y axis labels and titles\n",
    "plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=90)\n",
    "plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)\n",
    "plt.title('Correlation Matrix')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    },
    "id": "SWVYbRFKahxx",
    "outputId": "71680737-be93-4a1d-e7ec-a62c0c6dd259"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "correlation_matrix = correlation_matrix_kendall\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the correlation matrix with a background gradient\n",
    "cax = ax.matshow(correlation_matrix, cmap='coolwarm')\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = fig.colorbar(cax)\n",
    "\n",
    "# Set the precision of the colorbar labels to 2 decimal places\n",
    "cbar.set_label('Correlation', rotation=270, labelpad=15)\n",
    "cbar.set_ticks([-1, -0.5, 0, 0.5, 1])\n",
    "cbar.set_ticklabels([-1, -0.5, 0, 0.5, 1])\n",
    "\n",
    "# Set the x and y axis labels and titles\n",
    "plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=90)\n",
    "plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)\n",
    "plt.title('Correlation Matrix')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWIVpDw1bqqR"
   },
   "source": [
    "From this graph is possible to understand some key components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ao-SXuyTt6Zk"
   },
   "outputs": [],
   "source": [
    "# Select specific columns from the DataFrame\n",
    "selected_columns = cluster_new_updated[['Area', 'Geo_light_intensity', 'IntDen', 'Mean', 'N_cell', 'Mean']]\n",
    "\n",
    "# If you want to create a new DataFrame with these columns, you can do so:\n",
    "clustered_df = selected_columns.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    },
    "id": "Ybj1sNMGvHfy",
    "outputId": "7e9370e7-5d41-4c1f-d117-569296bb5e72"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataFrame is clustered_df\n",
    "correlation_matrix = clustered_df.corr(method='kendall')\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the correlation matrix with a background gradient\n",
    "cax = ax.matshow(correlation_matrix, cmap='coolwarm')\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = fig.colorbar(cax)\n",
    "\n",
    "# Set the precision of the colorbar labels to 2 decimal places\n",
    "cbar.set_label('Correlation', rotation=270, labelpad=15)\n",
    "cbar.set_ticks([-1, -0.5, 0, 0.5, 1])\n",
    "cbar.set_ticklabels([-1, -0.5, 0, 0.5, 1])\n",
    "\n",
    "# Set the x and y axis labels and titles\n",
    "plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=90)\n",
    "plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)\n",
    "plt.title('Correlation Matrix')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    },
    "id": "AMYzv5qpv4E5",
    "outputId": "3b46df7d-d14b-4224-829c-3c08daa38709"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataFrame is clustered_df\n",
    "correlation_matrix = clustered_df.corr(method='kendall')\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the correlation matrix with a background gradient\n",
    "cax = ax.matshow(correlation_matrix, cmap='coolwarm')\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = fig.colorbar(cax)\n",
    "\n",
    "# Set the precision of the colorbar labels to 2 decimal places\n",
    "cbar.set_label('Correlation', rotation=270, labelpad=15)\n",
    "cbar.set_ticks([-1, -0.5, 0, 0.5, 1])\n",
    "cbar.set_ticklabels([-1, -0.5, 0, 0.5, 1])\n",
    "\n",
    "# Set the x and y axis labels and titles\n",
    "plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=90)\n",
    "plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)\n",
    "plt.title('Correlation Matrix')\n",
    "\n",
    "# Add correlation values as text annotations\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(len(correlation_matrix.columns)):\n",
    "        text = f\"{correlation_matrix.iloc[i, j]:.2f}\"\n",
    "        ax.text(j, i, text, ha='center', va='center', color='black', fontsize=10)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574
    },
    "id": "lLdBqNm_whVi",
    "outputId": "dd207ded-1bdd-4aeb-8229-5747e99822ee"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataFrame is clustered_df\n",
    "correlation_matrix = clustered_df.corr(method='kendall')\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the correlation matrix with a background gradient\n",
    "cax = ax.matshow(correlation_matrix, cmap='coolwarm')\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = fig.colorbar(cax)\n",
    "\n",
    "# Set the precision of the colorbar labels to 2 decimal places\n",
    "cbar.set_label('Correlation', rotation=270, labelpad=15)\n",
    "cbar.set_ticks([-1, -0.5, 0, 0.5, 1])\n",
    "cbar.set_ticklabels([-1, -0.5, 0, 0.5, 1])\n",
    "\n",
    "# Set the x-axis labels at the bottom\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.set_xticks(range(len(correlation_matrix.columns)))\n",
    "ax.set_xticklabels(correlation_matrix.columns, rotation=90)\n",
    "\n",
    "# Set the y-axis labels on the left\n",
    "ax.set_yticks(range(len(correlation_matrix.columns)))\n",
    "ax.set_yticklabels(correlation_matrix.columns)\n",
    "\n",
    "# Add correlation values as text annotations\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(len(correlation_matrix.columns)):\n",
    "        text = f\"{correlation_matrix.iloc[i, j]:.2f}\"\n",
    "        ax.text(j, i, text, ha='center', va='center', color='black', fontsize=10)\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "awhwu-2cbuYY",
    "outputId": "8d761f74-6822-4eb3-9675-c241b11d464e"
   },
   "outputs": [],
   "source": [
    "# Example - Major - Minor\n",
    "\n",
    "Column1 = 'Major'\n",
    "Column2 = 'Minor'\n",
    "correlation_pearson = clustered_df[Column1].corr(clustered_df[Column2], method='spearman')\n",
    "\n",
    "correlation_spearman = clustered_df[Column1].corr(clustered_df[Column2], method='spearman')\n",
    "\n",
    "correlation_kendall = clustered_df[Column1].corr(clustered_df[Column2], method='spearman')\n",
    "\n",
    "# Print or inspect the Spearman correlation value\n",
    "print(correlation_pearson, correlation_spearman, correlation_kendall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zr2UbtOt0Mal"
   },
   "source": [
    "###### Plotting graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pD7WhYKT0RMD"
   },
   "source": [
    "As see in the cluster, there are some values that are missing. That is mostly due to the segmentation algorithm and or some particular values that have been excluded through the clustering. For this reason one possibility is to use some ML system to solve this problem. It may be hard to understand the number of cells, or their location. However, it can also be possible to use some ML algorithms to solve this problem. Creating synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Rtj0TeA68O9"
   },
   "source": [
    "There may be some works in another notebook, as it can require more than expected + several iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnDqxSig8V4f"
   },
   "source": [
    "###### Geo_light_intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gpdKjqG1kOR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named 'clustered_df' with columns 'Labels', 'Time', and 'Geo_light_intensity'\n",
    "\n",
    "# Get unique labels\n",
    "unique_labels = clustered_df['Labels'].unique()\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "Geo_light_intensity_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each unique label and apply the spline fitting\n",
    "for label in unique_labels:\n",
    "    # Select data for the current label\n",
    "    label_data = clustered_df[clustered_df['Labels'] == label]\n",
    "\n",
    "    # Check if the cluster has at least 2 elements\n",
    "    if len(label_data) < 2:\n",
    "        continue  # Skip clusters with less than 2 elements\n",
    "\n",
    "    # Extract Time and Value columns\n",
    "    t_data = label_data['Time']\n",
    "    w_data = label_data['Geo_light_intensity']\n",
    "\n",
    "    # Create a cubic spline object\n",
    "    spline = CubicSpline(t_data, w_data)\n",
    "\n",
    "    # Generate finer x values for plotting\n",
    "    t_fine = np.linspace(min(t_data), max(t_data), int(max(t_data)))\n",
    "    w_fine = spline(t_fine)\n",
    "\n",
    "    # Create a new DataFrame for the current label's results\n",
    "    label_results = pd.DataFrame({'Time': t_fine, 'Geo_light_intensity': w_fine})\n",
    "\n",
    "    # Add a 'Labels' column with the current label value\n",
    "    label_results['Labels'] = label\n",
    "\n",
    "    # Append the current label's results to the overall results DataFrame\n",
    "    Geo_light_intensity_df = pd.concat([Geo_light_intensity_df, label_results])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "TngBkfVEBePA",
    "outputId": "06f4c040-c6e5-49f6-ab9c-24e883a15301"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Sort the DataFrame by \"Time\" in descending order\n",
    "df_sorted = Geo_light_intensity_df.sort_values(by='Time', ascending=False)\n",
    "\n",
    "# Filter the DataFrame to include only labels 0, 1, 2, and 3\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "selected_clusters = df_sorted[df_sorted['Labels'].isin(selected_labels)]\n",
    "\n",
    "# Create a color cycle for plotting\n",
    "color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate through the selected clusters and plot them with different colors\n",
    "for cluster_label in selected_labels:\n",
    "    cluster_data = selected_clusters[selected_clusters['Labels'] == cluster_label]\n",
    "\n",
    "    color = next(color_cycle)\n",
    "    plt.plot(cluster_data['Time'], cluster_data['Geo_light_intensity'], label=f'Cluster {cluster_label}', color=color)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Geo_light_intensity')\n",
    "plt.title('Geo_light_intensity Over Time for Labels 0, 1, 2, and 3')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbuMkbFb8Zgn"
   },
   "source": [
    "###### Geo_cell_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58tGQ7gj8RYI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named 'clustered_df' with columns 'Labels', 'Time', and 'Geo_light_intensity'\n",
    "\n",
    "# Get unique labels\n",
    "unique_labels = clustered_df['Labels'].unique()\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "Geo_cell_size_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each unique label and apply the spline fitting\n",
    "for label in unique_labels:\n",
    "    # Select data for the current label\n",
    "    label_data = clustered_df[clustered_df['Labels'] == label]\n",
    "\n",
    "    # Check if the cluster has at least 2 elements\n",
    "    if len(label_data) < 2:\n",
    "        continue  # Skip clusters with less than 2 elements\n",
    "\n",
    "    # Extract Time and Value columns\n",
    "    t_data = label_data['Time']\n",
    "    w_data = label_data['Geo_cell_size']\n",
    "\n",
    "    # Create a cubic spline object\n",
    "    spline = CubicSpline(t_data, w_data)\n",
    "\n",
    "    # Generate finer x values for plotting\n",
    "    t_fine = np.linspace(min(t_data), max(t_data), int(max(t_data)))\n",
    "    w_fine = spline(t_fine)\n",
    "\n",
    "    # Create a new DataFrame for the current label's results\n",
    "    label_results = pd.DataFrame({'Time': t_fine, 'Geo_cell_size': w_fine})\n",
    "\n",
    "    # Add a 'Labels' column with the current label value\n",
    "    label_results['Labels'] = label\n",
    "\n",
    "    # Append the current label's results to the overall results DataFrame\n",
    "    Geo_cell_size_df = pd.concat([Geo_cell_size_df, label_results])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "-1-KcrUqBp0d",
    "outputId": "5b8ce557-44a7-43c1-e480-f854b116b706"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Sort the DataFrame by \"Time\" in descending order\n",
    "df_sorted = Geo_cell_size_df.sort_values(by='Time', ascending=False)\n",
    "\n",
    "# Filter the DataFrame to include only labels 0, 1, 2, and 3\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "selected_clusters = df_sorted[df_sorted['Labels'].isin(selected_labels)]\n",
    "\n",
    "# Create a color cycle for plotting\n",
    "color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate through the selected clusters and plot them with different colors\n",
    "for cluster_label in selected_labels:\n",
    "    cluster_data = selected_clusters[selected_clusters['Labels'] == cluster_label]\n",
    "\n",
    "    color = next(color_cycle)\n",
    "    plt.plot(cluster_data['Time'], cluster_data['Geo_cell_size'], label=f'Cluster {cluster_label}', color=color)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Geo_cell_size')\n",
    "plt.title('Geo_cell_size Over Time for Labels 0, 1, 2, and 3')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlHL0qth8cI1"
   },
   "source": [
    "###### Cluster_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRwQw9bm8Rnb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named 'clustered_df' with columns 'Labels', 'Time', and 'Geo_light_intensity'\n",
    "\n",
    "# Get unique labels\n",
    "unique_labels = clustered_df['Labels'].unique()\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "Cluster_dimension_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each unique label and apply the spline fitting\n",
    "for label in unique_labels:\n",
    "    # Select data for the current label\n",
    "    label_data = clustered_df[clustered_df['Labels'] == label]\n",
    "\n",
    "    # Check if the cluster has at least 2 elements\n",
    "    if len(label_data) < 2:\n",
    "        continue  # Skip clusters with less than 2 elements\n",
    "\n",
    "    # Extract Time and Value columns\n",
    "    t_data = label_data['Time']\n",
    "    w_data = label_data['Cluster_dimension']\n",
    "\n",
    "    # Create a cubic spline object\n",
    "    spline = CubicSpline(t_data, w_data)\n",
    "\n",
    "    # Generate finer x values for plotting\n",
    "    t_fine = np.linspace(min(t_data), max(t_data), int(max(t_data)))\n",
    "    w_fine = spline(t_fine)\n",
    "\n",
    "    # Create a new DataFrame for the current label's results\n",
    "    label_results = pd.DataFrame({'Time': t_fine, 'Cluster_dimension': w_fine})\n",
    "\n",
    "    # Add a 'Labels' column with the current label value\n",
    "    label_results['Labels'] = label\n",
    "\n",
    "    # Append the current label's results to the overall results DataFrame\n",
    "    Cluster_dimension_df = pd.concat([Cluster_dimension_df, label_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "r4-aGb8LSumB",
    "outputId": "a65ee137-f83c-405d-d2fb-fd320aaeb123"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Sort the DataFrame by \"Time\" in descending order\n",
    "df_sorted = Cluster_dimension_df.sort_values(by='Time', ascending=False)\n",
    "\n",
    "# Filter the DataFrame to include only labels 0, 1, 2, and 3\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "selected_clusters = df_sorted[df_sorted['Labels'].isin(selected_labels)]\n",
    "\n",
    "# Create a color cycle for plotting\n",
    "color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate through the selected clusters and plot them with different colors\n",
    "for cluster_label in selected_labels:\n",
    "    cluster_data = selected_clusters[selected_clusters['Labels'] == cluster_label]\n",
    "\n",
    "    color = next(color_cycle)\n",
    "    plt.plot(cluster_data['Time'], cluster_data['Cluster_dimension'], label=f'Cluster {cluster_label}', color=color)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cluster_dimension')\n",
    "plt.title('Cluster_dimension Over Time for Labels 0, 1, 2, and 3')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "RtJkEgybTGex",
    "outputId": "95c214df-40e3-41e8-9384-adcff1b332cd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Sort the DataFrame by \"Time\" in descending order\n",
    "df_sorted = Cluster_dimension_df.sort_values(by='Time', ascending=False)\n",
    "\n",
    "# Filter the DataFrame to include only labels 0, 1, 2, and 3\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "selected_clusters = df_sorted[df_sorted['Labels'].isin(selected_labels)]\n",
    "\n",
    "# Create a color cycle for plotting\n",
    "color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate through the selected clusters and plot moving averages with different colors\n",
    "for cluster_label in selected_labels:\n",
    "    cluster_data = selected_clusters[selected_clusters['Labels'] == cluster_label]\n",
    "\n",
    "    color = next(color_cycle)\n",
    "\n",
    "    # Calculate the moving average with a window size of, for example, 5\n",
    "    window_size = 5\n",
    "    moving_avg = cluster_data['Cluster_dimension'].rolling(window=window_size).mean()\n",
    "\n",
    "    plt.plot(cluster_data['Time'], moving_avg, label=f'Cluster {cluster_label} (Moving Avg)', color=color)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Moving Average of Cluster_dimension')\n",
    "plt.title('Moving Average of Cluster_dimension Over Time for Labels 0, 1, 2, and 3')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "x0CiSDOFVxzC",
    "outputId": "48a39951-a994-49dc-f2d5-b112a50ce683"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Sort the DataFrame by \"Time\" in descending order\n",
    "df_sorted = Cluster_dimension_df.sort_values(by='Time', ascending=False)\n",
    "\n",
    "# Filter the DataFrame to include only labels 0, 1, 2, and 3\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "selected_clusters = df_sorted[df_sorted['Labels'].isin(selected_labels)]\n",
    "\n",
    "# Create a color cycle for plotting\n",
    "color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate through the selected clusters and plot them with different colors\n",
    "for cluster_label in selected_labels:\n",
    "    cluster_data = selected_clusters[selected_clusters['Labels'] == cluster_label]\n",
    "\n",
    "    color = next(color_cycle)\n",
    "    plt.plot(cluster_data['Time'], cluster_data['Cluster_dimension'], label=f'Cluster {cluster_label}', color=color)\n",
    "\n",
    "    # Calculate and plot the moving average with a window of your choice, e.g., window=5\n",
    "    moving_average = cluster_data['Cluster_dimension'].rolling(window=5).mean()\n",
    "    plt.plot(cluster_data['Time'], moving_average, label=f'Moving Avg {cluster_label}', linestyle='--', color=color)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cluster_dimension')\n",
    "plt.title('Cluster_dimension Over Time for Labels 0, 1, 2, and 3')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmahJj-O8eXZ"
   },
   "source": [
    "###### N_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SeFt76DJHymN"
   },
   "outputs": [],
   "source": [
    "clustered_df = cluster_new_updated.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01o1S7-a8Rza"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named 'clustered_df' with columns 'Labels', 'Time', and 'Geo_light_intensity'\n",
    "\n",
    "# Get unique labels\n",
    "unique_labels = clustered_df['Labels'].unique()\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "N_cell_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each unique label and apply the spline fitting\n",
    "for label in unique_labels:\n",
    "    # Select data for the current label\n",
    "    label_data = clustered_df[clustered_df['Labels'] == label]\n",
    "\n",
    "    # Check if the cluster has at least 2 elements\n",
    "    if len(label_data) < 2:\n",
    "        continue  # Skip clusters with less than 2 elements\n",
    "\n",
    "    # Extract Time and Value columns\n",
    "    t_data = label_data['Time']\n",
    "    w_data = label_data['N_cell']\n",
    "\n",
    "    # Create a cubic spline object\n",
    "    spline = CubicSpline(t_data, w_data)\n",
    "\n",
    "    # Generate finer x values for plotting\n",
    "    t_fine = np.linspace(min(t_data), max(t_data), int(max(t_data)))\n",
    "    w_fine = spline(t_fine)\n",
    "\n",
    "    # Create a new DataFrame for the current label's results\n",
    "    label_results = pd.DataFrame({'Time': t_fine, 'N_cell': w_fine})\n",
    "\n",
    "    # Add a 'Labels' column with the current label value\n",
    "    label_results['Labels'] = label\n",
    "\n",
    "    # Append the current label's results to the overall results DataFrame\n",
    "    N_cell_df = pd.concat([N_cell_df, label_results])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "_EsCxLJg8SBk",
    "outputId": "11666f2d-1a96-4249-cdda-561b92445ffc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Sort the DataFrame by \"Time\" in descending order\n",
    "df_sorted = N_cell_df.sort_values(by='Time', ascending=False)\n",
    "\n",
    "# Filter the DataFrame to include only labels 0, 1, 2, and 3\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "selected_clusters = df_sorted[df_sorted['Labels'].isin(selected_labels)]\n",
    "\n",
    "# Create a color cycle for plotting\n",
    "color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate through the selected clusters and plot them with different colors\n",
    "for cluster_label in selected_labels:\n",
    "    cluster_data = selected_clusters[selected_clusters['Labels'] == cluster_label]\n",
    "\n",
    "    color = next(color_cycle)\n",
    "    plt.plot(cluster_data['Time'], cluster_data['N_cell'], label=f'Cluster {cluster_label}', color=color)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('N_cell')\n",
    "plt.title('N_cell Over Time for Labels 0, 1, 2, and 3')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "jM0oWnacVMnd",
    "outputId": "ad87d0a5-4c3a-4c63-f7fa-5624a6d671a5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Sort the DataFrame by \"Time\" in descending order\n",
    "df_sorted = N_cell_df.sort_values(by='Time', ascending=False)\n",
    "\n",
    "# Filter the DataFrame to include only labels 0, 1, 2, and 3\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "selected_clusters = df_sorted[df_sorted['Labels'].isin(selected_labels)]\n",
    "\n",
    "# Create a color cycle for plotting\n",
    "color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate through the selected clusters and plot them with different colors\n",
    "for cluster_label in selected_labels:\n",
    "    cluster_data = selected_clusters[selected_clusters['Labels'] == cluster_label]\n",
    "\n",
    "    color = next(color_cycle)\n",
    "    plt.plot(cluster_data['Time'], cluster_data['N_cell'], label=f'Cluster {cluster_label}', color=color)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('N_cell')\n",
    "plt.title('N_cell Over Time for Labels 0, 1, 2, and 3')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Set the y-axis to log scale\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "8ZkK5PdeVRqg",
    "outputId": "88f008d4-d980-45f7-d20b-c9ab3e3334d1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Sort the DataFrame by \"Time\" in descending order\n",
    "df_sorted = N_cell_df.sort_values(by='Time', ascending=False)\n",
    "\n",
    "# Filter the DataFrame to include only labels 0, 1, 2, and 3\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "selected_clusters = df_sorted[df_sorted['Labels'].isin(selected_labels)]\n",
    "\n",
    "# Create a color cycle for plotting\n",
    "color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate through the selected clusters and plot them with different colors\n",
    "for cluster_label in selected_labels:\n",
    "    cluster_data = selected_clusters[selected_clusters['Labels'] == cluster_label]\n",
    "\n",
    "    color = next(color_cycle)\n",
    "\n",
    "    # Plot the original data\n",
    "    plt.plot(cluster_data['Time'], cluster_data['N_cell'], label=f'Cluster {cluster_label}', color=color)\n",
    "\n",
    "    # Compute and plot the moving average with a window of your choice, e.g., window=5\n",
    "    moving_average = cluster_data['N_cell'].rolling(window=5).mean()\n",
    "    plt.plot(cluster_data['Time'], moving_average, label=f'Moving Avg {cluster_label}', linestyle='--', color=color)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('N_cell')\n",
    "plt.title('N_cell Over Time for Labels 0, 1, 2, and 3')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Set the y-axis to log scale\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjQpyli5UcSV"
   },
   "source": [
    "We can understand the closest exponential function achievable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "1pdNRQ5YSfKw",
    "outputId": "d93c9e2a-816b-458b-dcc9-2972ea723535"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sort the DataFrame by \"Time\" in descending order\n",
    "df_sorted = N_cell_df.sort_values(by='Time', ascending=False)\n",
    "\n",
    "# Filter the DataFrame to include only labels 0, 1, 2, and 3\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "selected_clusters = df_sorted[df_sorted['Labels'].isin(selected_labels)]\n",
    "\n",
    "# Create a color cycle for plotting\n",
    "color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate through the selected clusters and plot them with different colors\n",
    "for cluster_label in selected_labels:\n",
    "    cluster_data = selected_clusters[selected_clusters['Labels'] == cluster_label]\n",
    "\n",
    "    color = next(color_cycle)\n",
    "\n",
    "    # Calculate the moving average with a window size of, for example, 5\n",
    "    window_size = 5\n",
    "    moving_avg = cluster_data['N_cell'].rolling(window=window_size).mean()\n",
    "\n",
    "    plt.plot(cluster_data['Time'], moving_avg, label=f'Cluster {cluster_label} (Moving Avg)', color=color)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('N_cell')\n",
    "plt.title('Moving Average of N_cell Over Time for Labels 0, 1, 2, and 3')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYOS1G9rTZID"
   },
   "source": [
    "###### Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYs8LHrSAx2y"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named 'clustered_df' with columns 'Labels', 'Time', and 'Geo_light_intensity'\n",
    "\n",
    "# Get unique labels\n",
    "unique_labels = clustered_df['Labels'].unique()\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "Mean_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each unique label and apply the spline fitting\n",
    "for label in unique_labels:\n",
    "    # Select data for the current label\n",
    "    label_data = clustered_df[clustered_df['Labels'] == label]\n",
    "\n",
    "    # Check if the cluster has at least 2 elements\n",
    "    if len(label_data) < 2:\n",
    "        continue  # Skip clusters with less than 2 elements\n",
    "\n",
    "    # Extract Time and Value columns\n",
    "    t_data = label_data['Time']\n",
    "    w_data = label_data['Mean']\n",
    "\n",
    "    # Create a cubic spline object\n",
    "    spline = CubicSpline(t_data, w_data)\n",
    "\n",
    "    # Generate finer x values for plotting\n",
    "    t_fine = np.linspace(min(t_data), max(t_data), int(max(t_data)))\n",
    "    w_fine = spline(t_fine)\n",
    "\n",
    "    # Create a new DataFrame for the current label's results\n",
    "    label_results = pd.DataFrame({'Time': t_fine, 'Mean': w_fine})\n",
    "\n",
    "    # Add a 'Labels' column with the current label value\n",
    "    label_results['Labels'] = label\n",
    "\n",
    "    # Append the current label's results to the overall results DataFrame\n",
    "    Mean_df = pd.concat([Mean_df, label_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "VNt9a0nXTcVR",
    "outputId": "71da5b79-5ded-4807-d60e-51d4f089ebf5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Sort the DataFrame by \"Time\" in descending order\n",
    "df_sorted = Mean_df.sort_values(by='Time', ascending=False)\n",
    "\n",
    "# Filter the DataFrame to include only labels 0, 1, 2, and 3\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "selected_clusters = df_sorted[df_sorted['Labels'].isin(selected_labels)]\n",
    "\n",
    "# Create a color cycle for plotting\n",
    "color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate through the selected clusters and plot them with different colors\n",
    "for cluster_label in selected_labels:\n",
    "    cluster_data = selected_clusters[selected_clusters['Labels'] == cluster_label]\n",
    "\n",
    "    color = next(color_cycle)\n",
    "    plt.plot(cluster_data['Time'], cluster_data['Mean'], label=f'Cluster {cluster_label}', color=color)\n",
    "\n",
    "    # Calculate and plot the moving average with a window of your choice, e.g., window=5\n",
    "    moving_average = cluster_data['Mean'].rolling(window=5).mean()\n",
    "    plt.plot(cluster_data['Time'], moving_average, label=f'Moving Avg {cluster_label}', linestyle='--', color=color)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Mean')\n",
    "plt.title('Mean Over Time for Labels 0, 1, 2, and 3')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Rl5WKBmTZg5"
   },
   "source": [
    "###### IntDen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tB2qrCtTcxI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named 'clustered_df' with columns 'Labels', 'Time', and 'Geo_light_intensity'\n",
    "\n",
    "# Get unique labels\n",
    "unique_labels = clustered_df['Labels'].unique()\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "IntDen_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each unique label and apply the spline fitting\n",
    "for label in unique_labels:\n",
    "    # Select data for the current label\n",
    "    label_data = clustered_df[clustered_df['Labels'] == label]\n",
    "\n",
    "    # Check if the cluster has at least 2 elements\n",
    "    if len(label_data) < 2:\n",
    "        continue  # Skip clusters with less than 2 elements\n",
    "\n",
    "    # Extract Time and Value columns\n",
    "    t_data = label_data['Time']\n",
    "    w_data = label_data['IntDen']\n",
    "\n",
    "    # Create a cubic spline object\n",
    "    spline = CubicSpline(t_data, w_data)\n",
    "\n",
    "    # Generate finer x values for plotting\n",
    "    t_fine = np.linspace(min(t_data), max(t_data), int(max(t_data)))\n",
    "    w_fine = spline(t_fine)\n",
    "\n",
    "    # Create a new DataFrame for the current label's results\n",
    "    label_results = pd.DataFrame({'Time': t_fine, 'IntDen': w_fine})\n",
    "\n",
    "    # Add a 'Labels' column with the current label value\n",
    "    label_results['Labels'] = label\n",
    "\n",
    "    # Append the current label's results to the overall results DataFrame\n",
    "    IntDen_df = pd.concat([IntDen_df, label_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "RvLcXSD4BMkO",
    "outputId": "48491caa-ac32-4786-ee16-4c05941f6d6d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Sort the DataFrame by \"Time\" in descending order\n",
    "df_sorted = IntDen_df.sort_values(by='Time', ascending=False)\n",
    "\n",
    "# Filter the DataFrame to include only labels 0, 1, 2, and 3\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "selected_clusters = df_sorted[df_sorted['Labels'].isin(selected_labels)]\n",
    "\n",
    "# Create a color cycle for plotting\n",
    "color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate through the selected clusters and plot them with different colors\n",
    "for cluster_label in selected_labels:\n",
    "    cluster_data = selected_clusters[selected_clusters['Labels'] == cluster_label]\n",
    "\n",
    "    color = next(color_cycle)\n",
    "    plt.plot(cluster_data['Time'], cluster_data['IntDen'], label=f'Cluster {cluster_label}', color=color)\n",
    "\n",
    "    # Calculate and plot the moving average with a window of your choice, e.g., window=5\n",
    "    moving_average = cluster_data['IntDen'].rolling(window=5).mean()\n",
    "    plt.plot(cluster_data['Time'], moving_average, label=f'Moving Avg {cluster_label}', linestyle='--', color=color)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('IntDen')\n",
    "plt.title('IntDen Over Time for Labels 0, 1, 2, and 3')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGQheoLFTZ6D"
   },
   "source": [
    "###### StdDev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YkfrpTqwTdV-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named 'clustered_df' with columns 'Labels', 'Time', and 'Geo_light_intensity'\n",
    "\n",
    "# Get unique labels\n",
    "unique_labels = clustered_df['Labels'].unique()\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "StdDev_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each unique label and apply the spline fitting\n",
    "for label in unique_labels:\n",
    "    # Select data for the current label\n",
    "    label_data = clustered_df[clustered_df['Labels'] == label]\n",
    "\n",
    "    # Check if the cluster has at least 2 elements\n",
    "    if len(label_data) < 2:\n",
    "        continue  # Skip clusters with less than 2 elements\n",
    "\n",
    "    # Extract Time and Value columns\n",
    "    t_data = label_data['Time']\n",
    "    w_data = label_data['StdDev']\n",
    "\n",
    "    # Create a cubic spline object\n",
    "    spline = CubicSpline(t_data, w_data)\n",
    "\n",
    "    # Generate finer x values for plotting\n",
    "    t_fine = np.linspace(min(t_data), max(t_data), int(max(t_data)))\n",
    "    w_fine = spline(t_fine)\n",
    "\n",
    "    # Create a new DataFrame for the current label's results\n",
    "    label_results = pd.DataFrame({'Time': t_fine, 'StdDev': w_fine})\n",
    "\n",
    "    # Add a 'Labels' column with the current label value\n",
    "    label_results['Labels'] = label\n",
    "\n",
    "    # Append the current label's results to the overall results DataFrame\n",
    "    StdDev_df = pd.concat([StdDev_df, label_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "MgfN5bASTejn",
    "outputId": "e79df58e-672c-45c0-877e-27eb10bb4d1a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Sort the DataFrame by \"Time\" in descending order\n",
    "df_sorted = StdDev_df.sort_values(by='Time', ascending=False)\n",
    "\n",
    "# Filter the DataFrame to include only labels 0, 1, 2, and 3\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "selected_clusters = df_sorted[df_sorted['Labels'].isin(selected_labels)]\n",
    "\n",
    "# Create a color cycle for plotting\n",
    "color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate through the selected clusters and plot them with different colors\n",
    "for cluster_label in selected_labels:\n",
    "    cluster_data = selected_clusters[selected_clusters['Labels'] == cluster_label]\n",
    "\n",
    "    color = next(color_cycle)\n",
    "    plt.plot(cluster_data['Time'], cluster_data['StdDev'], label=f'Cluster {cluster_label}', color=color)\n",
    "\n",
    "    # Calculate and plot the moving average with a window of your choice, e.g., window=5\n",
    "    moving_average = cluster_data['StdDev'].rolling(window=5).mean()\n",
    "    plt.plot(cluster_data['Time'], moving_average, label=f'Moving Avg {cluster_label}', linestyle='--', color=color)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('StdDev')\n",
    "plt.title('StdDev Over Time for Labels 0, 1, 2, and 3')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tbp0pGuTbFy"
   },
   "source": [
    "###### Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QK3t5i0Te7g"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named 'clustered_df' with columns 'Labels', 'Time', and 'Geo_light_intensity'\n",
    "\n",
    "# Get unique labels\n",
    "unique_labels = clustered_df['Labels'].unique()\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "Area_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each unique label and apply the spline fitting\n",
    "for label in unique_labels:\n",
    "    # Select data for the current label\n",
    "    label_data = clustered_df[clustered_df['Labels'] == label]\n",
    "\n",
    "    # Check if the cluster has at least 2 elements\n",
    "    if len(label_data) < 2:\n",
    "        continue  # Skip clusters with less than 2 elements\n",
    "\n",
    "    # Extract Time and Value columns\n",
    "    t_data = label_data['Time']\n",
    "    w_data = label_data['Area']\n",
    "\n",
    "    # Create a cubic spline object\n",
    "    spline = CubicSpline(t_data, w_data)\n",
    "\n",
    "    # Generate finer x values for plotting\n",
    "    t_fine = np.linspace(min(t_data), max(t_data), int(max(t_data)))\n",
    "    w_fine = spline(t_fine)\n",
    "\n",
    "    # Create a new DataFrame for the current label's results\n",
    "    label_results = pd.DataFrame({'Time': t_fine, 'Area': w_fine})\n",
    "\n",
    "    # Add a 'Labels' column with the current label value\n",
    "    label_results['Labels'] = label\n",
    "\n",
    "    # Append the current label's results to the overall results DataFrame\n",
    "    Area_df = pd.concat([Area_df, label_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "whyWx3-vBuJh",
    "outputId": "b7013076-f3e8-4791-9d51-f35792300aca"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Sort the DataFrame by \"Time\" in descending order\n",
    "df_sorted = Area_df.sort_values(by='Time', ascending=False)\n",
    "\n",
    "# Filter the DataFrame to include only labels 0, 1, 2, and 3\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "selected_clusters = df_sorted[df_sorted['Labels'].isin(selected_labels)]\n",
    "\n",
    "# Create a color cycle for plotting\n",
    "color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate through the selected clusters and plot them with different colors\n",
    "for cluster_label in selected_labels:\n",
    "    cluster_data = selected_clusters[selected_clusters['Labels'] == cluster_label]\n",
    "\n",
    "    color = next(color_cycle)\n",
    "    plt.plot(cluster_data['Time'], cluster_data['Area'], label=f'Cluster {cluster_label}', color=color)\n",
    "\n",
    "    # Calculate and plot the moving average with a window of your choice, e.g., window=5\n",
    "    moving_average = cluster_data['Area'].rolling(window=5).mean()\n",
    "    plt.plot(cluster_data['Time'], moving_average, label=f'Moving Avg {cluster_label}', linestyle='--', color=color)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Area')\n",
    "plt.title('Area Over Time for Labels 0, 1, 2, and 3')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PX5OyiXeby97"
   },
   "source": [
    "##### Analysis of a particular cluster in space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rX-QX7Hj0d8"
   },
   "source": [
    "###### Functions used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NE5w-jFJj75C"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_distance_vs_cell_location(data, window_size=2):\n",
    "    df = data.copy()\n",
    "\n",
    "    # Calculate the barycenter (centroid)\n",
    "    Barycenter_x = df['X'].mean()\n",
    "    Barycenter_y = df['Y'].mean()\n",
    "\n",
    "    # Calculate the distance for each data point\n",
    "    df['Distance'] = df.apply(lambda row: math.sqrt((row['X'] - Barycenter_x)**2 + (row['Y'] - Barycenter_y)**2), axis=1)\n",
    "\n",
    "    # Sort the data by distance\n",
    "    df.sort_values(by='Distance', inplace=True)\n",
    "\n",
    "    # Calculate the cumulative count of cells with distances lower than or equal to each distance\n",
    "    df['Cumulative_Count'] = np.arange(1, len(df) + 1)\n",
    "\n",
    "    # Calculate the derivative (change in cumulative count) using numpy's gradient function\n",
    "    df['Derivative'] = np.gradient(df['Cumulative_Count'], df['Distance'])\n",
    "\n",
    "    # Create subplots with two graphs (main cumulative distribution and derivative)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Main cumulative distribution plot\n",
    "    ax1.plot(df['Distance'], df['Cumulative_Count'], marker='o', linestyle='-', color='b')\n",
    "    ax1.set_xlabel('Distance from Barycenter')\n",
    "    ax1.set_ylabel('Number of Cells (Cumulative)')\n",
    "    ax1.set_title('Cumulative Distribution of Cell Distances from Barycenter')\n",
    "\n",
    "    # Derivative plot\n",
    "    ax2.plot(df['Distance'], df['Derivative'], marker='o', linestyle='-', color='r')\n",
    "    ax2.set_xlabel('Distance from Barycenter')\n",
    "    ax2.set_ylabel('Derivative (Change in Cumulative Count)')\n",
    "    ax2.set_title('Derivative of Cumulative Distribution')\n",
    "\n",
    "    # Optionally, you can save the plot to a file or display it\n",
    "    plt.savefig('cumulative_and_derivative_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a DataFrame 'data' with columns 'X' and 'Y', representing cell coordinates,\n",
    "# and 'Distance', 'Cumulative_Count', and 'Derivative' columns calculated using your function.\n",
    "# plot_distance_vs_cell_location(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4tbV1ingj7zv"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def plot_distance_vs_value(data):\n",
    "    df = data.copy()\n",
    "    # Calculate the barycenter (centroid)\n",
    "    Barycenter_x = df['X'].mean()\n",
    "    Barycenter_y = df['Y'].mean()\n",
    "\n",
    "    # Calculate the distance for each data point\n",
    "    df['Distance'] = df.apply(lambda row: math.sqrt((row['X'] - Barycenter_x)**2 + (row['Y'] - Barycenter_y)**2), axis=1)\n",
    "\n",
    "    # Create a scatter plot of 'Distance' vs 'IntDen'\n",
    "    plt.scatter(df['Distance'], df['IntDen'], alpha=0.7)\n",
    "    plt.xlabel('Distance to Barycenter')\n",
    "    plt.ylabel('IntDen')\n",
    "    plt.title('IntDen vs. Distance to Barycenter')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# plot_distance_vs_value(your_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tmqYAIUj7mC"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def plot_distance_vs_value_with_histogram(data, bins=10):\n",
    "    df = data.copy()\n",
    "    # Calculate the barycenter (centroid)\n",
    "    Barycenter_x = df['X'].mean()\n",
    "    Barycenter_y = df['Y'].mean()\n",
    "\n",
    "    # Calculate the distance for each data point\n",
    "    df['Distance'] = df.apply(lambda row: math.sqrt((row['X'] - Barycenter_x)**2 + (row['Y'] - Barycenter_y)**2), axis=1)\n",
    "\n",
    "    # Create a histogram of the 'Distance' column\n",
    "    plt.hist(df['Distance'], bins=bins, edgecolor='k', alpha=0.7)\n",
    "    plt.xlabel('Distance to Barycenter')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distance to Barycenter Histogram')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MxpgXQUEj7Va"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def plot_moving_average(data, window_size=3500):\n",
    "    df = data.copy()\n",
    "    # Calculate the barycenter (centroid)\n",
    "    Barycenter_x = df['X'].mean()\n",
    "    Barycenter_y = df['Y'].mean()\n",
    "\n",
    "    # Calculate the distance for each data point\n",
    "    df['Distance'] = df.apply(lambda row: math.sqrt((row['X'] - Barycenter_x)**2 + (row['Y'] - Barycenter_y)**2), axis=1)\n",
    "\n",
    "    # Create a new DataFrame for plotting\n",
    "    plot_df = df[['Distance', 'IntDen']]\n",
    "\n",
    "    # Sort the DataFrame by distance for a cleaner plot\n",
    "    plot_df = plot_df.sort_values(by='Distance')\n",
    "\n",
    "    # Calculate the moving average of the 'IntDen' column with a window size of 30\n",
    "    plot_df['Moving_Average'] = plot_df['IntDen'].rolling(window=window_size).mean()\n",
    "\n",
    "    # Plot the moving average\n",
    "    plt.plot(plot_df['Distance'], plot_df['Moving_Average'], linestyle='--', label='Moving Average')\n",
    "    plt.xlabel('Distance to Barycenter')\n",
    "    plt.ylabel('Moving Average of IntDen')\n",
    "    plt.title(f'Moving Average of IntDen vs. Distance to Barycenter (Window Size {window_size})')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LqVuYDCAkTD6"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_values_based_on_the_area_in_space(data, d_value=7.5):\n",
    "    # Calculate the barycenter (centroid)\n",
    "    Barycenter_x = data['X'].mean()\n",
    "    Barycenter_y = data['Y'].mean()\n",
    "\n",
    "    # Calculate the distance for each data point\n",
    "    data['Distance'] = ((data['X'] - Barycenter_x)**2 + (data['Y'] - Barycenter_y)**2).apply(math.sqrt)\n",
    "\n",
    "    # Create a new DataFrame for plotting\n",
    "    plot_df = data[['Distance', 'IntDen']]\n",
    "\n",
    "    # Sort the DataFrame by distance for a cleaner plot\n",
    "    plot_df = plot_df.sort_values('Distance')\n",
    "\n",
    "    # Assign labels based on distance\n",
    "    plot_df['Label'] = plot_df['Distance'].apply(lambda distance: math.ceil(distance / d_value))\n",
    "    plot_df['New_Distance'] = plot_df['Label']*d_value - d_value / 2\n",
    "\n",
    "    # Calculate area and sum of IntDen\n",
    "    plot_df['Area'] = (math.pi * (plot_df['Label']**2 - (plot_df['Label'] - 1)**2))*d_value * d_value\n",
    "    plot_df['Sum_IntDen'] = plot_df.groupby('Label')['IntDen'].transform('sum')\n",
    "    plot_df['Ratio'] = plot_df['Sum_IntDen'] / plot_df['Area']\n",
    "\n",
    "    # Plotting the function\n",
    "    plt.plot(plot_df['New_Distance'], plot_df['Ratio'])\n",
    "    plt.xlabel('New Distance')\n",
    "    plt.ylabel('Ratio')\n",
    "    plt.title('Ratio vs New Distance')\n",
    "    plt.show()\n",
    "\n",
    "    return plot_df\n",
    "\n",
    "# Example usage:\n",
    "# plot_values_based_on_the_area_in_space(selected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_HV-I0dkTAt"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "from google.colab import drive\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    # Extract and return the numeric part of the filename\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\\d+)', s)]\n",
    "\n",
    "def create_video_from_pngs(folder_path, output_video_path, frame_size=(1920, 1080), frame_rate=10):\n",
    "    \"\"\"\n",
    "    Create a video from PNG files in a Google Drive folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder containing PNG files.\n",
    "        output_video_path (str): The path where the output video will be saved.\n",
    "        frame_size (tuple): The frame size of the output video (width, height).\n",
    "        frame_rate (int): The frame rate of the output video.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Sort the PNG files by their filenames using natural sorting\n",
    "    png_files = sorted([os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if filename.endswith('.png')], key=natural_sort_key)\n",
    "\n",
    "    # Initialize the video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # You can use other codecs like 'XVID' or 'MJPG'\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, frame_rate, frame_size)\n",
    "\n",
    "    # Create the video from PNG files\n",
    "    for png_file in png_files:\n",
    "        frame = cv2.imread(png_file)\n",
    "        frame = cv2.resize(frame, frame_size)\n",
    "        out.write(frame)\n",
    "\n",
    "    # Release the video writer\n",
    "    out.release()\n",
    "\n",
    "    # Unmount Google Drive to avoid errors when saving the video\n",
    "    drive.flush_and_unmount()\n",
    "\n",
    "    print(f\"Video '{output_video_path}' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bDkBlFYUkS5G"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_distance_vs_moving_average_with_labels(data, labels=[0, 1, 2, 3], window_size=2, interruption_point=None):\n",
    "    df = data.copy()\n",
    "\n",
    "    # Filter the DataFrame to select rows with specific labels\n",
    "    df = df[df['Labels'].isin(labels)]\n",
    "\n",
    "    # Initialize an empty dictionary to store barycenters for each label\n",
    "    barycenters = {}\n",
    "\n",
    "    # Calculate the barycenter (centroid) for each selected label\n",
    "    for label in labels:\n",
    "        label_df = df[df['Labels'] == label]\n",
    "        barycenter_x = label_df['X'].mean()\n",
    "        barycenter_y = label_df['Y'].mean()\n",
    "        barycenters[label] = (barycenter_x, barycenter_y)\n",
    "\n",
    "    # Calculate the distance for each data point based on its label-dependent barycenter\n",
    "    df['Distance'] = df.apply(lambda row: math.sqrt((row['X'] - barycenters[row['Labels']][0])**2 + (row['Y'] - barycenters[row['Labels']][1])**2), axis=1)\n",
    "\n",
    "    # Determine the maximum distance for the x-axis\n",
    "    max_distance = df['Distance'].max()\n",
    "\n",
    "    if interruption_point is not None:\n",
    "        # Filter the DataFrame to include only data points with distances less than the interruption_point\n",
    "        df = df[df['Distance'] < interruption_point]\n",
    "\n",
    "    # Create a new DataFrame for plotting\n",
    "    plot_df = df[['Distance', 'IntDen']]\n",
    "\n",
    "    # Sort the DataFrame by distance for a cleaner plot\n",
    "    plot_df = plot_df.sort_values(by='Distance')\n",
    "\n",
    "    # Calculate the moving average of the 'IntDen' column\n",
    "    plot_df['Moving_Average'] = plot_df['IntDen'].rolling(window=window_size).mean()\n",
    "\n",
    "    # Plot only the moving average\n",
    "    plt.plot(plot_df['Distance'], plot_df['Moving_Average'], linestyle='--', label='Moving Average')\n",
    "    plt.xlabel('Distance to Barycenter')\n",
    "    plt.ylabel('Moving Average of IntDen')\n",
    "    plt.title('Moving Average of IntDen vs. Distance to Barycenter')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a DataFrame 'df' with columns 'X', 'Y', 'IntDen', and 'Labels'\n",
    "# plot_distance_vs_moving_average_with_labels(df, labels=[0, 1, 2, 3], window_size=2, interruption_point=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgbG_FLDlpqJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def calculate_correlation(data, labels=[0, 1, 2, 3], interruption_point=None):\n",
    "    df = data.copy()\n",
    "\n",
    "    # Filter the DataFrame to select rows with specific labels\n",
    "    df = df[df['Labels'].isin(labels)]\n",
    "\n",
    "    # Initialize an empty dictionary to store barycenters for each label\n",
    "    barycenters = {}\n",
    "\n",
    "    # Calculate the barycenter (centroid) for each selected label\n",
    "    for label in labels:\n",
    "        label_df = df[df['Labels'] == label]\n",
    "        barycenter_x = label_df['X'].mean()\n",
    "        barycenter_y = label_df['Y'].mean()\n",
    "        barycenters[label] = (barycenter_x, barycenter_y)\n",
    "\n",
    "    # Calculate the distance for each data point based on its label-dependent barycenter\n",
    "    df['Distance'] = df.apply(lambda row: math.sqrt((row['X'] - barycenters[row['Labels']][0])**2 + (row['Y'] - barycenters[row['Labels']][1])**2), axis=1)\n",
    "\n",
    "    if interruption_point is not None:\n",
    "        # Filter the DataFrame to include only data points with distances less than the interruption_point\n",
    "        df = df[df['Distance'] < interruption_point]\n",
    "\n",
    "    # Calculate the correlation between 'Distance' and 'IntDen' columns\n",
    "    correlation = df['Distance'].corr(df['IntDen'])\n",
    "\n",
    "    return correlation\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a DataFrame 'df' with columns 'X', 'Y', 'IntDen', and 'Labels'\n",
    "# correlation = calculate_correlation(df, labels=[0, 1, 2, 3], interruption_point=10)\n",
    "# print(\"Correlation between Distance and IntDen:\", correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-vQQoF-lphw"
   },
   "outputs": [],
   "source": [
    "def calculate_correlation_single(data, labels=[0, 1, 2, 3], interruption_point=None):\n",
    "    df = data.copy()\n",
    "\n",
    "    # Initialize an empty dictionary to store barycenters for each label\n",
    "    barycenters = {}\n",
    "\n",
    "    correlations = {}\n",
    "\n",
    "    for label in labels:\n",
    "        # Filter the DataFrame to select rows with the current label\n",
    "        label_df = df[df['Labels'] == label]\n",
    "\n",
    "        if label_df.empty:\n",
    "            continue  # Skip labels with no data\n",
    "\n",
    "        # Calculate the barycenter (centroid) for the current label\n",
    "        barycenter_x = label_df['X'].mean()\n",
    "        barycenter_y = label_df['Y'].mean()\n",
    "        barycenters[label] = (barycenter_x, barycenter_y)\n",
    "\n",
    "        # Calculate the distance for each data point based on its label-dependent barycenter\n",
    "        label_df['Distance'] = label_df.apply(lambda row: math.sqrt((row['X'] - barycenters[label][0])**2 + (row['Y'] - barycenters[label][1])**2), axis=1)\n",
    "\n",
    "        if interruption_point is not None:\n",
    "            # Filter the DataFrame to include only data points with distances less than the interruption_point\n",
    "            label_df = label_df[label_df['Distance'] < interruption_point]\n",
    "\n",
    "        # Calculate the correlation between 'Distance' and 'IntDen' columns for the current label\n",
    "        correlation = label_df['Distance'].corr(label_df['IntDen'])\n",
    "        correlations[label] = correlation\n",
    "\n",
    "\n",
    "    return correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9U4mOCoclpe8"
   },
   "outputs": [],
   "source": [
    "def calculate_correlation_single_Area(data, labels=[0, 1, 2, 3], interruption_point=None):\n",
    "    df = data.copy()\n",
    "\n",
    "    # Initialize an empty dictionary to store barycenters for each label\n",
    "    barycenters = {}\n",
    "\n",
    "    correlations = {}\n",
    "\n",
    "    for label in labels:\n",
    "        # Filter the DataFrame to select rows with the current label\n",
    "        label_df = df[df['Labels'] == label]\n",
    "\n",
    "        if label_df.empty:\n",
    "            continue  # Skip labels with no data\n",
    "\n",
    "        # Calculate the barycenter (centroid) for the current label\n",
    "        barycenter_x = label_df['X'].mean()\n",
    "        barycenter_y = label_df['Y'].mean()\n",
    "        barycenters[label] = (barycenter_x, barycenter_y)\n",
    "\n",
    "        # Calculate the distance for each data point based on its label-dependent barycenter\n",
    "        label_df['Distance'] = label_df.apply(lambda row: math.sqrt((row['X'] - barycenters[label][0])**2 + (row['Y'] - barycenters[label][1])**2), axis=1)\n",
    "\n",
    "        if interruption_point is not None:\n",
    "            # Filter the DataFrame to include only data points with distances less than the interruption_point\n",
    "            label_df = label_df[label_df['Distance'] < interruption_point]\n",
    "\n",
    "        # Calculate the correlation between 'Distance' and 'IntDen' columns for the current label\n",
    "        correlation = label_df['Distance'].corr(label_df['Area'])\n",
    "        correlations[label] = correlation\n",
    "\n",
    "\n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Utmhsqp2lpZG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c70XdwxOj5hY"
   },
   "source": [
    "###### Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "W4CWXucSxHwf",
    "outputId": "0643d10f-f193-4069-9801-92328d2b3df0"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Replace 'data' with your actual data\n",
    "data = df_all_new_updated['IntDen']\n",
    "\n",
    "# Create probability plots for different distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Normal distribution\n",
    "plt.subplot(231)\n",
    "stats.probplot(data, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Normal Distribution\")\n",
    "\n",
    "# Exponential distribution\n",
    "plt.subplot(232)\n",
    "stats.probplot(data, dist=\"expon\", plot=plt)\n",
    "plt.title(\"Exponential Distribution\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "A6YGjuq0uVO6",
    "outputId": "d46a1920-6ab2-4a93-f835-01bcbf559cc0"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'points_cluster_0_time_5' is your DataFrame with the 'IntDen' column\n",
    "\n",
    "# Plot the Gaussian distribution using seaborn\n",
    "sns.histplot(points_cluster_0_time_5['IntDen'], kde=True, bins=20, color='blue')\n",
    "plt.xlabel('IntDen')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Gaussian Distribution of IntDen')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "pjnK-ugBqEOc",
    "outputId": "696bc26d-70c2-4e11-de0e-3d7a63745170"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_value_with_moving_average(points_cluster_0_time_5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "Y6Bq1cpEw_aL",
    "outputId": "dfc0fc5d-1220-4fe3-b99e-4ba8e95177d6"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_value(selected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "LnBJ9oJYyEYC",
    "outputId": "fd1df514-abe5-4b05-bf83-56bd050fb984"
   },
   "outputs": [],
   "source": [
    "plot_moving_average(selected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PrlB31uhhb9i",
    "outputId": "00abdad8-525c-4267-ab5c-90666d7beb88"
   },
   "outputs": [],
   "source": [
    "folder_id = '/content/drive/MyDrive/Academic Work/Utkarsha Khanal/CSV_files/New_algorithm/clusters/N=0123'\n",
    "output_video_path = '/content/drive/MyDrive/Academic Work/Utkarsha Khanal/CSV_files/New_algorithm/clusters/N=0123/video.mp4'\n",
    "create_video_from_pngs(folder_id,output_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579
    },
    "id": "mw1SxRCi44ne",
    "outputId": "ad1281b0-8148-4784-c93a-3de825cc147f"
   },
   "outputs": [],
   "source": [
    "visual = plot_values_based_on_the_area_in_space(selected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "n6ftqT7OGOb-",
    "outputId": "89bcbd35-68be-4d58-d0b9-02a16e3f34c0"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Replace 'data' with your actual data\n",
    "data = visual['IntDen']\n",
    "\n",
    "# Create probability plots for different distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Normal distribution\n",
    "plt.subplot(231)\n",
    "stats.probplot(data, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Normal Distribution\")\n",
    "\n",
    "# Exponential distribution\n",
    "plt.subplot(232)\n",
    "stats.probplot(data, dist=\"expon\", plot=plt)\n",
    "plt.title(\"Exponential Distribution\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "l5R6zVqggPS9",
    "outputId": "36d73946-1a02-4b04-9e66-b3fdc4ad649f"
   },
   "outputs": [],
   "source": [
    "f0 = plot_distance_histogram_with_custom_ticks(df_distance_C0_T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "id": "syOqa5J5XD8m",
    "outputId": "13700db8-f96c-4cab-92e2-196ebb064d63"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_cell_location(df_distance_C0_T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Fl05Q0MgH0p",
    "outputId": "8108be6f-9d25-49bb-eae6-ed71a34d8c69"
   },
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "pmfhQknXgDSl",
    "outputId": "2491abd5-0f3a-4068-ce50-6c441a44d3f0"
   },
   "outputs": [],
   "source": [
    "f1 = plot_distance_histogram_with_custom_ticks(df_distance_C1_T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "id": "6Gq5zInZXYAB",
    "outputId": "2df5c2b6-7e2c-43aa-8f0f-ac4c0c9247bd"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_cell_location(df_distance_C1_T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "id": "xSFwTJYlXX0X",
    "outputId": "98d96fe2-52bb-410d-8abb-5132a66cfe80"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_cell_location(df_distance_C2_T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k9mK2-cofWyw",
    "outputId": "2ebb4662-8dc5-4315-eda3-1f076b0503c5"
   },
   "outputs": [],
   "source": [
    "f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "3ghh_pKIfSzV",
    "outputId": "1c66e896-7a8e-43d2-b467-82c8ac727aad"
   },
   "outputs": [],
   "source": [
    "f2 = plot_distance_histogram_with_custom_ticks(df_distance_C2_T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "oBK1D_IHXXim",
    "outputId": "20a81507-141c-4385-d2eb-f3442e96e220"
   },
   "outputs": [],
   "source": [
    "df_see = plot_distance_vs_cell_location(df_distance_C3_T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "TCbghhEFcPo2",
    "outputId": "1c9d1763-f3de-4633-ac84-40beeca03202"
   },
   "outputs": [],
   "source": [
    "plot_distance_histogram_with_custom_ticks(df_distance_C3_T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "JXmL63-QeyWz",
    "outputId": "7c1441c2-e3ea-4ef9-ebaf-ed99d5e5c9b7"
   },
   "outputs": [],
   "source": [
    "first_d = plot_distance_histogram_with_custom_ticks(df_distance_C3_T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cGQsWx8Xe16m",
    "outputId": "d5d305f2-31dd-4ddc-fbb0-f6cdb6b76b49"
   },
   "outputs": [],
   "source": [
    "first_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ns0OsUwjcOmh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_distance_histogram_with_custom_ticks(data):\n",
    "    df = data.copy()\n",
    "\n",
    "    # Calculate the barycenter (centroid)\n",
    "    Barycenter_x = df['X'].mean()\n",
    "    Barycenter_y = df['Y'].mean()\n",
    "\n",
    "    # Calculate the distance for each data point\n",
    "    df['Distance'] = df.apply(lambda row: math.sqrt((row['X'] - Barycenter_x)**2 + (row['Y'] - Barycenter_y)**2), axis=1)\n",
    "    custom_ticks = int(max(df['Distance']) / 7.5)\n",
    "    # Create a histogram of distances with custom ticks\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    n, bins, patches = plt.hist(df['Distance'], bins=custom_ticks, color='b', alpha=0.7)\n",
    "    plt.xlabel('Distance from Barycenter')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Cell Distances from Barycenter')\n",
    "\n",
    "    # Find the first bin with a count of zero or close to zero\n",
    "    first_zero_bin = None\n",
    "    for i, count in enumerate(n):\n",
    "        if count <= 1e-6:  # You can adjust this threshold as needed\n",
    "            first_zero_bin = bins[i]\n",
    "            break\n",
    "\n",
    "    plt.show()\n",
    "    return first_zero_bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "he_GXkWos8RI",
    "outputId": "58268420-f200-47df-fd96-ad86984f1250"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_moving_average(points_cluster_0_time_5, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEkW-c-wtG-A"
   },
   "source": [
    "study other points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6T9dp-htMVw"
   },
   "outputs": [],
   "source": [
    "points_cluster_0_time_25 = df_all_new_updated[(df_all_new_updated['Time'] == 25) & (df_all_new_updated['Labels'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "Aa31RWH5tWAC",
    "outputId": "5751f022-7889-4da4-c33e-c84f20acfba0"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_moving_average(points_cluster_0_time_25, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZH8bfdltbGN"
   },
   "outputs": [],
   "source": [
    "points_cluster_0_time_50 = df_all_new_updated[(df_all_new_updated['Time'] == 50) & (df_all_new_updated['Labels'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "37FpyDeBtaL6",
    "outputId": "7e3b12e3-72ee-4b7f-b5c5-0b1e9552b6ca"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_moving_average(points_cluster_0_time_50, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QdspBTfMtalV"
   },
   "outputs": [],
   "source": [
    "points_cluster_0_time_75 = df_all_new_updated[(df_all_new_updated['Time'] == 75) & (df_all_new_updated['Labels'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "3xZgf_ZItv1z",
    "outputId": "32b96a6e-aa4b-4b2d-e6ad-56895f41455b"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_moving_average(points_cluster_0_time_75, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhW_FeFFt7Su"
   },
   "outputs": [],
   "source": [
    "points_cluster_0_time_100 = df_all_new_updated[(df_all_new_updated['Time'] == 100) & (df_all_new_updated['Labels'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "x9ScTJgAt8z2",
    "outputId": "35b6ed2c-4e1d-443c-d198-2e849afb1b29"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_moving_average(points_cluster_0_time_100, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2azvHw_6vhl3"
   },
   "source": [
    "I can take all the values and calculate a distance based on all of them. I can see what happens with several points. The distance can be more covered in this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "0BqPQFNyvpis",
    "outputId": "9061693d-3e94-4182-d97c-8301f397ea5e"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_moving_average_with_labels(df_all_new_updated, window_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "fVtZSkiRvfqK",
    "outputId": "db188c2c-8f60-4ebe-8be7-efbb409ba200"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_moving_average_with_labels(df_all_new_updated, window_size = 20, interruption_point = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYHozdFy0YET"
   },
   "source": [
    "Just a few of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "3XKCpWbh0ZYl",
    "outputId": "5cf77fae-2975-43b9-81ca-f1b44fe13eee"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_moving_average_with_labels(df_all_new_updated, labels = [0], window_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "y8n_p6ZV0Z5q",
    "outputId": "db75ba1c-5f0c-4643-ff1d-30d9a2517379"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_moving_average_with_labels(df_all_new_updated, labels = [1], window_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "erqVsL5o0Z2t",
    "outputId": "86d63bf6-ce9f-48f6-98d6-2196608a0b3d"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_moving_average_with_labels(df_all_new_updated, labels = [2], window_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "WZGgxq2t0ZvC",
    "outputId": "b3fd577c-ffcf-4b32-b0a8-109a79a60650"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_moving_average_with_labels(df_all_new_updated, labels = [3], window_size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICuTk0X6vdzi"
   },
   "source": [
    "After this analysis on the intensity, it may be useful to understand how the values are correlated. We can study the correlation considering single clusters or all the values taken together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmkyGnQ6l4EW"
   },
   "source": [
    "###### Correlation with IntDen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ROCs4BEJ8Q8N"
   },
   "outputs": [],
   "source": [
    "correlation_df = pd.DataFrame.from_dict(correlations, orient='index', columns=['Correlation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adUdl57O8UPq"
   },
   "outputs": [],
   "source": [
    "correlation_df.loc['Total'] = calculate_correlation(df_all_new_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "AAhF1hoA6s3V",
    "outputId": "9251cbee-3d16-477c-a5e4-8a03733748de"
   },
   "outputs": [],
   "source": [
    "# Create a heat map\n",
    "sns.heatmap(correlation_df, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Correlations')\n",
    "plt.title('Correlation Heat Map')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihMXWuUy8mTw"
   },
   "source": [
    "Another question that may be useful to answer is whether there is a difference between the area and the distance. That can define if it's possible to identify any correlation between the area and the distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F7O6s29H8fJF",
    "outputId": "a9ead9f6-1746-474b-8f0f-0d5fb46fd9ff"
   },
   "outputs": [],
   "source": [
    "area_correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_3o9QnZnIuL"
   },
   "source": [
    "##### Additional Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "dNgHV-QM5zLs",
    "outputId": "2b2fef38-866c-4443-e306-dd60de680d12"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df_all_new_updated is your DataFrame\n",
    "data = df_all_new_updated['IntDen']\n",
    "\n",
    "# Create a histogram\n",
    "plt.hist(data, bins=10, color='blue', edgecolor='black')\n",
    "\n",
    "# Add labels and a title\n",
    "plt.xlabel('IntDen')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of IntDen')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVgP5LoI8lxU"
   },
   "outputs": [],
   "source": [
    "# Select specific columns from the DataFrame\n",
    "selected_columns = cluster_new_updated[['Area', 'Geo_light_intensity', 'IntDen', 'Mean', 'N_cell', 'Mean', 'Labels']]\n",
    "selected_columns2 = cluster_new_updated[['Area', 'Geo_light_intensity', 'IntDen', 'Mean', 'N_cell', 'Mean']]\n",
    "\n",
    "# If you want to create a new DataFrame with these columns, you can do so:\n",
    "clustered_df = selected_columns.copy()\n",
    "measurement_data = selected_columns2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a07GWR0z8wIA"
   },
   "outputs": [],
   "source": [
    "clustered_df_L0 = clustered_df[clustered_df['Labels'] == 0]\n",
    "clustered_df_L1 = clustered_df[clustered_df['Labels'] == 1]\n",
    "clustered_df_L2 = clustered_df[clustered_df['Labels'] == 2]\n",
    "clustered_df_L3 = clustered_df[clustered_df['Labels'] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fQqFAdnX8mLp"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "luminescence_pca = pca.fit_transform(clustered_df_L0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "9P_c8szW9D_0",
    "outputId": "008a1c89-c2e2-4908-959c-e7deeada585f"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming you have already performed PCA on 'clustered_df_L0'\n",
    "pca = PCA()\n",
    "luminescence_pca = pca.fit_transform(clustered_df)\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(8, 6))  # Adjust the figure size as needed\n",
    "plt.scatter(luminescence_pca[:, 0], luminescence_pca[:, 1], alpha=0.5)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA Scatter Plot')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DS8hcnrd-OSO"
   },
   "outputs": [],
   "source": [
    "pip install scanpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "id": "6bwfh7w89PfJ",
    "outputId": "8f7fbbb2-1cca-4297-da1e-b590ed26e5ed"
   },
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "# Perform PCA with optional data normalization\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named measurement_data\n",
    "# Convert your DataFrame to an AnnData object\n",
    "adata = sc.AnnData(X=measurement_data.values)\n",
    "#sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "\n",
    "# Perform PCA with optional data normalization\n",
    "sc.tl.pca(adata)\n",
    "\n",
    "# Visualize PCA results\n",
    "sc.pl.pca(adata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "id": "4GKVkaZc_Kx-",
    "outputId": "5a73db6c-9589-49a5-8792-8708d6c4a500"
   },
   "outputs": [],
   "source": [
    "adata = sc.AnnData(X=measurement_data.values)\n",
    "\n",
    "# Perform PCA with optional data normalization\n",
    "sc.tl.pca(adata)\n",
    "\n",
    "# Visualize PCA results\n",
    "sc.pl.pca(adata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kYQN_L88AZjT",
    "outputId": "22a02913-cd58-459a-e781-034191df1c9c"
   },
   "outputs": [],
   "source": [
    "!pip install leidenalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "25SBTNVRAJK0",
    "outputId": "476b0fed-5465-48ea-b493-70c4c64aa602"
   },
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "# Assuming you have an AnnData object named 'adata'\n",
    "\n",
    "# Calculate neighbors\n",
    "sc.pp.neighbors(adata, n_pcs=6)\n",
    "\n",
    "# Perform clustering using the Leiden algorithm\n",
    "sc.tl.leiden(adata, resolution=1.0)  # You can adjust the resolution parameter\n",
    "\n",
    "# UMAP embedding\n",
    "sc.tl.umap(adata)\n",
    "\n",
    "# Visualize the UMAP plot with cluster coloring\n",
    "sc.pl.umap(adata, color='leiden')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jAuwpqdNAI8u"
   },
   "outputs": [],
   "source": [
    "pip install pysal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYuEfrCsnlUq"
   },
   "source": [
    "###### Spatial Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZ5mGp6fDk2U"
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import libpysal\n",
    "import esda\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select specific columns from the DataFrame\n",
    "selected_columns_points = df_all_new_updated[['X', 'Y', 'IntDen']]\n",
    "\n",
    "# Create a GeoDataFrame with Point geometries\n",
    "geometry = [Point(x, y) for x, y in zip(selected_columns_points['X'], selected_columns_points['Y'])]\n",
    "gdf = gpd.GeoDataFrame(selected_columns_points, geometry=geometry)\n",
    "\n",
    "# Create a spatial weights matrix (W) using Queen contiguity\n",
    "w = libpysal.weights.Queen.from_dataframe(gdf)\n",
    "\n",
    "# Extract the 'IntDen' column for Moran's I analysis\n",
    "y = selected_columns_points['IntDen']\n",
    "\n",
    "# Perform Moran's I analysis\n",
    "moran = esda.Moran(y, w)\n",
    "\n",
    "# Access Moran's I results\n",
    "moran_I = moran.I\n",
    "moran_EI = moran.EI\n",
    "moran_p_value = moran.p_sim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 856
    },
    "id": "YyQMN8DTEwk4",
    "outputId": "8c601f41-d21e-4cd3-e376-89a82a817c3a"
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from splot.esda import plot_moran\n",
    "\n",
    "# Plot Moran scatterplot\n",
    "plot_moran(moran, zstandard=True, figsize=(10, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KvWERQWGGK2F",
    "outputId": "a35974da-1545-4389-f49b-f4ffd7de7a7a"
   },
   "outputs": [],
   "source": [
    "!pip install pykrige"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "qBv5v6NnNefZ",
    "outputId": "62e3da60-2d94-4979-b6d6-07bd723abc58"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Generate example data\n",
    "n_points = len(df1)\n",
    "X = df1['X']  # X coordinates\n",
    "Y = df1['Y']  # Y coordinates\n",
    "temperature = df1['IntDen'].values  # Temperature values\n",
    "\n",
    "# Create a grid for prediction\n",
    "x_grid, y_grid = np.meshgrid(np.linspace(0, 20, 150), np.linspace(0, 20, 150))\n",
    "\n",
    "# Define the IDW interpolation function\n",
    "def idw_interpolation(x, y, X, Y, values, power=2):\n",
    "    distances = distance.cdist(np.column_stack((x.flatten(), y.flatten())), np.column_stack((X, Y)))\n",
    "    weights = 1.0 / (distances**power)\n",
    "    weighted_values = values * weights\n",
    "    interpolated_values = np.sum(weighted_values, axis=1) / np.sum(weights, axis=1)\n",
    "    return interpolated_values\n",
    "\n",
    "# Perform the IDW interpolation\n",
    "z_grid = idw_interpolation(x_grid, y_grid, X, Y, temperature)\n",
    "\n",
    "# Reshape the grid and predicted values\n",
    "z_grid = z_grid.reshape(x_grid.shape)\n",
    "\n",
    "# Create a contour plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(x_grid, y_grid, z_grid, levels=100, cmap=\"viridis\")\n",
    "plt.colorbar(label=\"IntDen\")\n",
    "plt.scatter(X, Y, c=temperature, cmap=\"coolwarm\", edgecolors=\"k\", s=100)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"IntDen Interpolation (IDW)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "nyfqS3evNiJz",
    "outputId": "36e36b15-efa4-4a4d-97ab-bf22ca2c2742"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Generate example data\n",
    "n_points = len(df2)\n",
    "X = df2['X']  # X coordinates\n",
    "Y = df2['Y']  # Y coordinates\n",
    "temperature = df2['IntDen'].values  # Temperature values\n",
    "\n",
    "# Create a grid for prediction\n",
    "x_grid, y_grid = np.meshgrid(np.linspace(0, 20, 150), np.linspace(0, 20, 150))\n",
    "\n",
    "# Define the IDW interpolation function\n",
    "def idw_interpolation(x, y, X, Y, values, power=2):\n",
    "    distances = distance.cdist(np.column_stack((x.flatten(), y.flatten())), np.column_stack((X, Y)))\n",
    "    weights = 1.0 / (distances**power)\n",
    "    weighted_values = values * weights\n",
    "    interpolated_values = np.sum(weighted_values, axis=1) / np.sum(weights, axis=1)\n",
    "    return interpolated_values\n",
    "\n",
    "# Perform the IDW interpolation\n",
    "z_grid = idw_interpolation(x_grid, y_grid, X, Y, temperature)\n",
    "\n",
    "# Reshape the grid and predicted values\n",
    "z_grid = z_grid.reshape(x_grid.shape)\n",
    "\n",
    "# Create a contour plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(x_grid, y_grid, z_grid, levels=100, cmap=\"viridis\")\n",
    "plt.colorbar(label=\"IntDen\")\n",
    "plt.scatter(X, Y, c=temperature, cmap=\"coolwarm\", edgecolors=\"k\", s=100)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"IntDen Interpolation (IDW)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wi-fNSYFQ9mE",
    "outputId": "8b1d282b-22b4-436a-f586-e372c910425b"
   },
   "outputs": [],
   "source": [
    "import spaghetti\n",
    "import esda\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "import libpysal\n",
    "\n",
    "# Sample dataset with X, Y, and IntDen columns\n",
    "# Example data (replace with your actual data):\n",
    "# df1 = pd.DataFrame({\n",
    "#     'X': [1, 2, 3, 4, 5],\n",
    "#     'Y': [2, 3, 4, 5, 6],\n",
    "#     'IntDen': [10, 15, 20, 25, 30]\n",
    "# })\n",
    "\n",
    "# Create a GeoDataFrame from your DataFrame with Point geometries\n",
    "geometry = [Point(xy) for xy in zip(df1['X'], df1['Y'])]\n",
    "gdf = gpd.GeoDataFrame(df1, geometry=geometry)\n",
    "\n",
    "# Create a spatial weights matrix (k-nearest neighbors with k=3, for example)\n",
    "w = libpysal.weights.KNN.from_dataframe(gdf, k=3)\n",
    "\n",
    "# Calculate Geary's C for spatial autocorrelation\n",
    "gearys_c = esda.geary.Geary(gdf['IntDen'], w)\n",
    "\n",
    "# Access Geary's C statistic and p-value\n",
    "gearys_c_statistic = gearys_c.C\n",
    "p_value = gearys_c.p_sim\n",
    "\n",
    "print(\"Geary's C:\", gearys_c_statistic)\n",
    "print(\"p-value:\", p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 647
    },
    "id": "Vl9LN1ZLRWsP",
    "outputId": "86f0bb0c-685c-4866-b932-d4adea81c5d9"
   },
   "outputs": [],
   "source": [
    "import spaghetti\n",
    "import esda\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "import libpysal\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Sample dataset with X, Y, and IntDen columns\n",
    "# Example data (replace with your actual data):\n",
    "# df1 = pd.DataFrame({\n",
    "#     'X': [1, 2, 3, 4, 5],\n",
    "#     'Y': [2, 3, 4, 5, 6],\n",
    "#     'IntDen': [10, 15, 20, 25, 30]\n",
    "# })\n",
    "\n",
    "# Create a GeoDataFrame from your DataFrame with Point geometries\n",
    "geometry = [Point(xy) for xy in zip(df1['X'], df1['Y'])]\n",
    "gdf = gpd.GeoDataFrame(df1, geometry=geometry)\n",
    "\n",
    "# Create a spatial weights matrix (k-nearest neighbors with k=3, for example)\n",
    "w = libpysal.weights.KNN.from_dataframe(gdf, k=3)\n",
    "\n",
    "# Suppress the warning about disconnected components\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Calculate Geary's C for spatial autocorrelation\n",
    "gearys_c = esda.geary.Geary(gdf['IntDen'], w)\n",
    "\n",
    "# Access Geary's C statistic and p-value\n",
    "gearys_c_statistic = gearys_c.C\n",
    "p_value = gearys_c.p_sim\n",
    "\n",
    "# Create a Moran scatterplot\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Calculate the spatial lag of IntDen\n",
    "lag_intden = libpysal.weights.lag_spatial(w, gdf['IntDen'])\n",
    "\n",
    "# Scatterplot\n",
    "plt.scatter(gdf['IntDen'], lag_intden, color='b', alpha=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"IntDen\")\n",
    "plt.ylabel(\"Spatial Lag of IntDen\")\n",
    "plt.title(\"Moran Scatterplot (Geary's C)\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YovGJ3LnjCb5"
   },
   "source": [
    "##### Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "-4SCR5RqAjI7",
    "outputId": "56252cf1-1fed-451b-9069-1bda0ab89195"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define parameters\n",
    "initial_population = selected_clusters[(selected_clusters['Labels'] == 0) & (selected_clusters['Time'] == 92)]['N_cell'].values[0]  # Initial population or value\n",
    "decay_rate = 1.000000001603458         # Rate at which the population decays\n",
    "simulation_time = 26    # Total simulation time\n",
    "\n",
    "# Initialize time and population arrays\n",
    "time_points = [92]  # Initialize with the starting time of 92\n",
    "population_values = [initial_population]\n",
    "\n",
    "# Gillespie algorithm\n",
    "while time_points[-1] < 92 + simulation_time and population_values[-1] > 0:\n",
    "    # Calculate the decay rate (event rate)\n",
    "    event_rate = decay_rate * population_values[-1]\n",
    "\n",
    "    # Generate a random time for the next event based on exponential distribution\n",
    "    time_to_next_event = np.random.exponential(1 / event_rate)\n",
    "\n",
    "    # Update time and population values\n",
    "    time_points.append(time_points[-1] + time_to_next_event)\n",
    "    population_values.append(population_values[-1] - 1)\n",
    "\n",
    "# Plot the simulation results\n",
    "plt.plot(time_points, population_values)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Population')\n",
    "plt.title('Gillespie Algorithm Simulation')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lw3detUWSRfi"
   },
   "source": [
    "The result is not super convincing. Because we know from our perspective how the value may change over time. It's still higher than 0 at least until 118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETE9JrsVSlN-"
   },
   "outputs": [],
   "source": [
    "N_t1 = selected_clusters[(selected_clusters['Labels'] == 0) & (selected_clusters['Time'] == 92)]['N_cell'].values[0]\n",
    "N_t2 = 1\n",
    "decay_rate = math.log(N_t1 / N_t2) / (118 - 92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MqipBoCCTO4t",
    "outputId": "495a4bef-d46a-4d62-89f9-09046841cfcd"
   },
   "outputs": [],
   "source": [
    "decay_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "M25X6-_dTSVA",
    "outputId": "b61f7ead-5b82-4e4f-e2ba-32930ef6ddf0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define parameters\n",
    "initial_population = selected_clusters[(selected_clusters['Labels'] == 0) & (selected_clusters['Time'] == 92)]['N_cell'].values[0]  # Initial population or value\n",
    "decay_rate = decay_rate         # Rate at which the population decays\n",
    "simulation_time = 26    # Total simulation time\n",
    "\n",
    "# Initialize time and population arrays\n",
    "time_points = [92]  # Initialize with the starting time of 92\n",
    "population_values = [initial_population]\n",
    "\n",
    "# Gillespie algorithm\n",
    "while time_points[-1] < 92 + simulation_time and population_values[-1] > 0:\n",
    "    # Calculate the decay rate (event rate)\n",
    "    event_rate = decay_rate * population_values[-1]\n",
    "\n",
    "    # Generate a random time for the next event based on exponential distribution\n",
    "    time_to_next_event = np.random.exponential(1 / event_rate)\n",
    "\n",
    "    # Update time and population values\n",
    "    time_points.append(time_points[-1] + time_to_next_event)\n",
    "    population_values.append(population_values[-1] - 1)\n",
    "\n",
    "# Plot the simulation results\n",
    "plt.plot(time_points, population_values)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Population')\n",
    "plt.title('Gillespie Algorithm Simulation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZP_kPvY-dH0p",
    "outputId": "d526edd3-b440-4e68-dfd4-ed52987e861c"
   },
   "outputs": [],
   "source": [
    "float(decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 536
    },
    "id": "EL9ejLRFYRro",
    "outputId": "7451b4be-a731-4df6-c541-db1355edb37e"
   },
   "outputs": [],
   "source": [
    "from cayenne.simulation import Simulation\n",
    "model_str = \"\"\"\n",
    "        const compartment comp1;\n",
    "        comp1 = 1.0; # volume of compartment\n",
    "\n",
    "        r1: N_cell => B; k1;\n",
    "\n",
    "        k1 = 0.1000001603458;\n",
    "        chem_flag = false;\n",
    "\n",
    "        N_cell = 300;\n",
    "        B = 0;\n",
    "    \"\"\"\n",
    "sim = Simulation.load_model(model_str, \"ModelString\")\n",
    "# Run the simulation\n",
    "sim.simulate(max_t=26, max_iter=1000, n_rep=5, algorithm=\"tau_adaptive\")\n",
    "sim.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 536
    },
    "id": "Id07OsVKe_oG",
    "outputId": "bcbd62aa-71fe-45a9-9fdf-c45b8350e94b"
   },
   "outputs": [],
   "source": [
    "sim.plot(species_names = [\"N_cell\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXrjCsTTDYH3"
   },
   "source": [
    "#### Original Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTCQnDBGFAc9"
   },
   "outputs": [],
   "source": [
    "linked_clusters_or = linked_clusters.copy()\n",
    "df_all_or = df_all.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFA0TCraN6fh"
   },
   "source": [
    "We can display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h4hs4px3N_uA"
   },
   "outputs": [],
   "source": [
    "output_folder_png = '/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/Original_Algorithm/png_files'\n",
    "create_png_plots(df_all_or, output_folder_png)\n",
    "\n",
    "gdrive_folder = output_folder_png\n",
    "output_pdf_name = gdrive_folder + '/new_model.pdf'\n",
    "\n",
    "result_pdf_path = merge_pngs_to_pdf(gdrive_folder, output_pdf_name)\n",
    "\n",
    "print(f\"PDF saved at: {result_pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hb23AIo5DYH4"
   },
   "outputs": [],
   "source": [
    "df = df_all.copy()\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "df = df[df['Labels'].isin(selected_labels)]\n",
    "\n",
    "data = df\n",
    "output_folder_png = '/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/New_algorithm/clusters/N=0123'\n",
    "create_png_plots(data, output_folder_png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GlTA0v29DYH4"
   },
   "outputs": [],
   "source": [
    "gdrive_folder = '/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/New_algorithm/clusters/N=0123'\n",
    "output_pdf_name = '/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/New_algorithm/clusters/N=0123/new_model.pdf'\n",
    "\n",
    "result_pdf_path = merge_pngs_to_pdf(gdrive_folder, output_pdf_name)\n",
    "\n",
    "print(f\"PDF saved at: {result_pdf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHEMwgZQDYH4"
   },
   "source": [
    "##### Fixing some values due to code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVyK9S0pDYH4"
   },
   "source": [
    "What is possible to notice is that the value here is really close. And there are just minimal differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7b_M5X4DYH4"
   },
   "outputs": [],
   "source": [
    "df_all_or = adjust_clusters_disappeared(df_all_or)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75HWe9vGQOP4"
   },
   "outputs": [],
   "source": [
    "output_folder_png = '/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/Original_Algorithm/png_files_v2'\n",
    "create_png_plots(df_all_or, output_folder_png)\n",
    "\n",
    "gdrive_folder = output_folder_png\n",
    "output_pdf_name = gdrive_folder + '/new_model.pdf'\n",
    "\n",
    "result_pdf_path = merge_pngs_to_pdf(gdrive_folder, output_pdf_name)\n",
    "\n",
    "print(f\"PDF saved at: {result_pdf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B27LDN6RDYH4"
   },
   "source": [
    "perform final adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-axxHXX4DYH4"
   },
   "outputs": [],
   "source": [
    "df_all_or = order_labels(df_all_or)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ph4FWOenQeCv"
   },
   "outputs": [],
   "source": [
    "output_folder_png = '/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/Original_Algorithm/png_files_v2_ordered'\n",
    "create_png_plots(df_all_or, output_folder_png)\n",
    "\n",
    "gdrive_folder = output_folder_png\n",
    "output_pdf_name = gdrive_folder + '/new_model.pdf'\n",
    "\n",
    "result_pdf_path = merge_pngs_to_pdf(gdrive_folder, output_pdf_name)\n",
    "\n",
    "folder_id = gdrive_folder\n",
    "output_video_path = gdrive_folder + '/video.mp4'\n",
    "create_video_from_pngs(folder_id,output_video_path)\n",
    "\n",
    "print(f\"PDF saved at: {result_pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "noC2rWBZDYH4"
   },
   "outputs": [],
   "source": [
    "# Define replacement dictionary\n",
    "replacement_dict = {35: 3, 97:3, 24:3, 106:3, 30:1, 22:1, 84:1, 95:1, 103:2, 28:2, 129:0, 39:0}\n",
    "\n",
    "# Replace values in the 'label' column using the dictionary\n",
    "df_all_new['Labels'] = df_all_new['Labels'].replace(replacement_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1Ccg88VDYH4"
   },
   "outputs": [],
   "source": [
    "df_all_new = order_labels(df_all_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5OPa-FmQi5O"
   },
   "outputs": [],
   "source": [
    "output_folder_png = '/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/Original_Algorithm/png_files_v3'\n",
    "create_png_plots(df_all_or, output_folder_png)\n",
    "\n",
    "gdrive_folder = output_folder_png\n",
    "output_pdf_name = gdrive_folder + '/new_model.pdf'\n",
    "\n",
    "result_pdf_path = merge_pngs_to_pdf(gdrive_folder, output_pdf_name)\n",
    "\n",
    "print(f\"PDF saved at: {result_pdf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmmvIcILDYH4"
   },
   "source": [
    "Study the development of all the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLP7N7LGDYH4"
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "  selected_label = i\n",
    "  selected_cluster = df_all_new_updated[df_all_new_updated['Labels'] == selected_label]\n",
    "\n",
    "  output_folder_png = '/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/New_algorithm/clusters/N='+ str(i)\n",
    "  create_png_plots(selected_cluster, output_folder_png)\n",
    "\n",
    "  gdrive_folder = '/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/New_algorithm/clusters/N='+ str(i)\n",
    "  output_pdf_name = '/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/New_algorithm/clusters/N='+ str(i) + '/complete_pdf_N=' + str(i) + '.pdf'\n",
    "\n",
    "  result_pdf_path = merge_pngs_to_pdf(gdrive_folder, output_pdf_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iVrNWPBDYH5"
   },
   "source": [
    "As there is the same data frame, it's also possible to select just one particular cluster and to see how it would develop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjlphdl-DYH5"
   },
   "source": [
    "after we have saved the final value we can also create a new cluster from it. So we can analyse our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RGuvOJapDYH5"
   },
   "outputs": [],
   "source": [
    "df_all_new_updated = pd.read_csv('/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/New_algorithm/df_all_new_updated_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqK6nV3yDYH5"
   },
   "source": [
    "Obtain updated clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqO4HHe7DYH5"
   },
   "outputs": [],
   "source": [
    "cluster_partial_1 = df_all_new_updated.groupby(['Labels', 'Time']).mean().reset_index()\n",
    "\n",
    "# Group by 'Labels' and 'Time' and apply the custom aggregation function\n",
    "cluster_partial_2 = df_all_new_updated.copy()\n",
    "\n",
    "# Group by 'Old_Labels' and 'Time' and apply the custom aggregation function\n",
    "cluster_partial_2 = cluster_partial_2.groupby(['Labels', 'Time']).apply(data_cluster_updated).reset_index()\n",
    "\n",
    "# Merge based on the 'Labels' and 'Time' columns using an inner join\n",
    "merged_df = pd.merge(result_df, result_df1, how='inner', on=['Labels', 'Time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfX1494ODYH5"
   },
   "outputs": [],
   "source": [
    "merged_df.to_csv('/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/New_algorithm/cluster_new_updated_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3GuJ8xlDYH5"
   },
   "outputs": [],
   "source": [
    "cluster_new_updated = pd.read_csv('/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/New_algorithm/cluster_new_updated_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AUrSa4hDYH5"
   },
   "source": [
    "###### Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "id": "DxpZlEGoDYH5",
    "outputId": "0c8461c5-18be-4f8c-ff03-2b6c3218410a"
   },
   "outputs": [],
   "source": [
    "df = cluster_new_updated.copy()\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "df = df[df['Labels'].isin(selected_labels)]\n",
    "\n",
    "# Get unique labels from the DataFrame\n",
    "unique_labels = df['Labels'].unique()\n",
    "\n",
    "# Create a 3D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot each label's X_x and Y_y positions over time\n",
    "for label in unique_labels:\n",
    "    label_data = df[df['Labels'] == label]\n",
    "\n",
    "    ax.plot(label_data['Time'], label_data['X_x'], label_data['Y_y'], label=label)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('X_x')\n",
    "ax.set_zlabel('Y_y')\n",
    "plt.title('3D Plot of Position vs. Time for Each Label')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hq_zu8sUDYH5"
   },
   "source": [
    "###### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LkXode0xDYH5"
   },
   "outputs": [],
   "source": [
    "# Calculate Pearson correlation\n",
    "correlation_matrix_pearson = clustered_df.corr()\n",
    "\n",
    "# Calculate Spearman correlation\n",
    "correlation_matrix_spearman = clustered_df.corr(method='spearman')\n",
    "\n",
    "# Calculate Kendall correlation\n",
    "correlation_matrix_kendall = clustered_df.corr(method='kendall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    },
    "id": "tp1ej90WDYH5",
    "outputId": "a534ac7b-9340-4307-bb7a-6b67a36a0142"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "correlation_matrix = correlation_matrix_pearson\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the correlation matrix with a background gradient\n",
    "cax = ax.matshow(correlation_matrix, cmap='coolwarm')\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = fig.colorbar(cax)\n",
    "\n",
    "# Set the precision of the colorbar labels to 2 decimal places\n",
    "cbar.set_label('Correlation', rotation=270, labelpad=15)\n",
    "cbar.set_ticks([-1, -0.5, 0, 0.5, 1])\n",
    "cbar.set_ticklabels([-1, -0.5, 0, 0.5, 1])\n",
    "\n",
    "# Set the x and y axis labels and titles\n",
    "plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=90)\n",
    "plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)\n",
    "plt.title('Correlation Matrix')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    },
    "id": "hGpBoX9TDYH5",
    "outputId": "af55b3ad-c7dc-48a5-fc92-b369605121af"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "correlation_matrix = correlation_matrix_spearman\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the correlation matrix with a background gradient\n",
    "cax = ax.matshow(correlation_matrix, cmap='coolwarm')\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = fig.colorbar(cax)\n",
    "\n",
    "# Set the precision of the colorbar labels to 2 decimal places\n",
    "cbar.set_label('Correlation', rotation=270, labelpad=15)\n",
    "cbar.set_ticks([-1, -0.5, 0, 0.5, 1])\n",
    "cbar.set_ticklabels([-1, -0.5, 0, 0.5, 1])\n",
    "\n",
    "# Set the x and y axis labels and titles\n",
    "plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=90)\n",
    "plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)\n",
    "plt.title('Correlation Matrix')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    },
    "id": "Og_i6iPeDYH5",
    "outputId": "71680737-be93-4a1d-e7ec-a62c0c6dd259"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "correlation_matrix = correlation_matrix_kendall\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the correlation matrix with a background gradient\n",
    "cax = ax.matshow(correlation_matrix, cmap='coolwarm')\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = fig.colorbar(cax)\n",
    "\n",
    "# Set the precision of the colorbar labels to 2 decimal places\n",
    "cbar.set_label('Correlation', rotation=270, labelpad=15)\n",
    "cbar.set_ticks([-1, -0.5, 0, 0.5, 1])\n",
    "cbar.set_ticklabels([-1, -0.5, 0, 0.5, 1])\n",
    "\n",
    "# Set the x and y axis labels and titles\n",
    "plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=90)\n",
    "plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)\n",
    "plt.title('Correlation Matrix')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLMr49E6DYH5"
   },
   "source": [
    "From this graph is possible to understand some key components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qqD0BdIqDYH5"
   },
   "outputs": [],
   "source": [
    "# Select specific columns from the DataFrame\n",
    "selected_columns = cluster_new_updated[['Area', 'Geo_light_intensity', 'IntDen', 'Mean', 'N_cell', 'Mean']]\n",
    "\n",
    "# If you want to create a new DataFrame with these columns, you can do so:\n",
    "clustered_df = selected_columns.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    },
    "id": "GVtGXFExDYH5",
    "outputId": "7e9370e7-5d41-4c1f-d117-569296bb5e72"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataFrame is clustered_df\n",
    "correlation_matrix = clustered_df.corr(method='kendall')\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the correlation matrix with a background gradient\n",
    "cax = ax.matshow(correlation_matrix, cmap='coolwarm')\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = fig.colorbar(cax)\n",
    "\n",
    "# Set the precision of the colorbar labels to 2 decimal places\n",
    "cbar.set_label('Correlation', rotation=270, labelpad=15)\n",
    "cbar.set_ticks([-1, -0.5, 0, 0.5, 1])\n",
    "cbar.set_ticklabels([-1, -0.5, 0, 0.5, 1])\n",
    "\n",
    "# Set the x and y axis labels and titles\n",
    "plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=90)\n",
    "plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)\n",
    "plt.title('Correlation Matrix')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    },
    "id": "GrQHpdwsDYH5",
    "outputId": "3b46df7d-d14b-4224-829c-3c08daa38709"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataFrame is clustered_df\n",
    "correlation_matrix = clustered_df.corr(method='kendall')\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the correlation matrix with a background gradient\n",
    "cax = ax.matshow(correlation_matrix, cmap='coolwarm')\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = fig.colorbar(cax)\n",
    "\n",
    "# Set the precision of the colorbar labels to 2 decimal places\n",
    "cbar.set_label('Correlation', rotation=270, labelpad=15)\n",
    "cbar.set_ticks([-1, -0.5, 0, 0.5, 1])\n",
    "cbar.set_ticklabels([-1, -0.5, 0, 0.5, 1])\n",
    "\n",
    "# Set the x and y axis labels and titles\n",
    "plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=90)\n",
    "plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)\n",
    "plt.title('Correlation Matrix')\n",
    "\n",
    "# Add correlation values as text annotations\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(len(correlation_matrix.columns)):\n",
    "        text = f\"{correlation_matrix.iloc[i, j]:.2f}\"\n",
    "        ax.text(j, i, text, ha='center', va='center', color='black', fontsize=10)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574
    },
    "id": "wV7YSe1ZDYH5",
    "outputId": "dd207ded-1bdd-4aeb-8229-5747e99822ee"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataFrame is clustered_df\n",
    "correlation_matrix = clustered_df.corr(method='kendall')\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the correlation matrix with a background gradient\n",
    "cax = ax.matshow(correlation_matrix, cmap='coolwarm')\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = fig.colorbar(cax)\n",
    "\n",
    "# Set the precision of the colorbar labels to 2 decimal places\n",
    "cbar.set_label('Correlation', rotation=270, labelpad=15)\n",
    "cbar.set_ticks([-1, -0.5, 0, 0.5, 1])\n",
    "cbar.set_ticklabels([-1, -0.5, 0, 0.5, 1])\n",
    "\n",
    "# Set the x-axis labels at the bottom\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.set_xticks(range(len(correlation_matrix.columns)))\n",
    "ax.set_xticklabels(correlation_matrix.columns, rotation=90)\n",
    "\n",
    "# Set the y-axis labels on the left\n",
    "ax.set_yticks(range(len(correlation_matrix.columns)))\n",
    "ax.set_yticklabels(correlation_matrix.columns)\n",
    "\n",
    "# Add correlation values as text annotations\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(len(correlation_matrix.columns)):\n",
    "        text = f\"{correlation_matrix.iloc[i, j]:.2f}\"\n",
    "        ax.text(j, i, text, ha='center', va='center', color='black', fontsize=10)\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0r9HhbDrDYH5",
    "outputId": "8d761f74-6822-4eb3-9675-c241b11d464e"
   },
   "outputs": [],
   "source": [
    "# Example - Major - Minor\n",
    "\n",
    "Column1 = 'Major'\n",
    "Column2 = 'Minor'\n",
    "correlation_pearson = clustered_df[Column1].corr(clustered_df[Column2], method='spearman')\n",
    "\n",
    "correlation_spearman = clustered_df[Column1].corr(clustered_df[Column2], method='spearman')\n",
    "\n",
    "correlation_kendall = clustered_df[Column1].corr(clustered_df[Column2], method='spearman')\n",
    "\n",
    "# Print or inspect the Spearman correlation value\n",
    "print(correlation_pearson, correlation_spearman, correlation_kendall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-ldTESXDYH5"
   },
   "source": [
    "###### Plotting graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wr-nDlB2DYH6"
   },
   "source": [
    "As see in the cluster, there are some values that are missing. That is mostly due to the segmentation algorithm and or some particular values that have been excluded through the clustering. For this reason one possibility is to use some ML system to solve this problem. It may be hard to understand the number of cells, or their location. However, it can also be possible to use some ML algorithms to solve this problem. Creating synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sa-Z5m-QDYH6"
   },
   "source": [
    "There may be some works in another notebook, as it can require more than expected + several iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0Y4KUASDYH6"
   },
   "source": [
    "###### Geo_light_intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4-Ah_1mDYH6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named 'clustered_df' with columns 'Labels', 'Time', and 'Geo_light_intensity'\n",
    "\n",
    "# Get unique labels\n",
    "unique_labels = clustered_df['Labels'].unique()\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "Geo_light_intensity_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each unique label and apply the spline fitting\n",
    "for label in unique_labels:\n",
    "    # Select data for the current label\n",
    "    label_data = clustered_df[clustered_df['Labels'] == label]\n",
    "\n",
    "    # Check if the cluster has at least 2 elements\n",
    "    if len(label_data) < 2:\n",
    "        continue  # Skip clusters with less than 2 elements\n",
    "\n",
    "    # Extract Time and Value columns\n",
    "    t_data = label_data['Time']\n",
    "    w_data = label_data['Geo_light_intensity']\n",
    "\n",
    "    # Create a cubic spline object\n",
    "    spline = CubicSpline(t_data, w_data)\n",
    "\n",
    "    # Generate finer x values for plotting\n",
    "    t_fine = np.linspace(min(t_data), max(t_data), int(max(t_data)))\n",
    "    w_fine = spline(t_fine)\n",
    "\n",
    "    # Create a new DataFrame for the current label's results\n",
    "    label_results = pd.DataFrame({'Time': t_fine, 'Geo_light_intensity': w_fine})\n",
    "\n",
    "    # Add a 'Labels' column with the current label value\n",
    "    label_results['Labels'] = label\n",
    "\n",
    "    # Append the current label's results to the overall results DataFrame\n",
    "    Geo_light_intensity_df = pd.concat([Geo_light_intensity_df, label_results])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "5y7C184vDYH6",
    "outputId": "06f4c040-c6e5-49f6-ab9c-24e883a15301"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Sort the DataFrame by \"Time\" in descending order\n",
    "df_sorted = Geo_light_intensity_df.sort_values(by='Time', ascending=False)\n",
    "\n",
    "# Filter the DataFrame to include only labels 0, 1, 2, and 3\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "selected_clusters = df_sorted[df_sorted['Labels'].isin(selected_labels)]\n",
    "\n",
    "# Create a color cycle for plotting\n",
    "color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate through the selected clusters and plot them with different colors\n",
    "for cluster_label in selected_labels:\n",
    "    cluster_data = selected_clusters[selected_clusters['Labels'] == cluster_label]\n",
    "\n",
    "    color = next(color_cycle)\n",
    "    plt.plot(cluster_data['Time'], cluster_data['Geo_light_intensity'], label=f'Cluster {cluster_label}', color=color)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Geo_light_intensity')\n",
    "plt.title('Geo_light_intensity Over Time for Labels 0, 1, 2, and 3')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9X7RUq0DYH6"
   },
   "source": [
    "###### Geo_cell_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6PrSCAJDYH6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named 'clustered_df' with columns 'Labels', 'Time', and 'Geo_light_intensity'\n",
    "\n",
    "# Get unique labels\n",
    "unique_labels = clustered_df['Labels'].unique()\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "Geo_cell_size_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each unique label and apply the spline fitting\n",
    "for label in unique_labels:\n",
    "    # Select data for the current label\n",
    "    label_data = clustered_df[clustered_df['Labels'] == label]\n",
    "\n",
    "    # Check if the cluster has at least 2 elements\n",
    "    if len(label_data) < 2:\n",
    "        continue  # Skip clusters with less than 2 elements\n",
    "\n",
    "    # Extract Time and Value columns\n",
    "    t_data = label_data['Time']\n",
    "    w_data = label_data['Geo_cell_size']\n",
    "\n",
    "    # Create a cubic spline object\n",
    "    spline = CubicSpline(t_data, w_data)\n",
    "\n",
    "    # Generate finer x values for plotting\n",
    "    t_fine = np.linspace(min(t_data), max(t_data), int(max(t_data)))\n",
    "    w_fine = spline(t_fine)\n",
    "\n",
    "    # Create a new DataFrame for the current label's results\n",
    "    label_results = pd.DataFrame({'Time': t_fine, 'Geo_cell_size': w_fine})\n",
    "\n",
    "    # Add a 'Labels' column with the current label value\n",
    "    label_results['Labels'] = label\n",
    "\n",
    "    # Append the current label's results to the overall results DataFrame\n",
    "    Geo_cell_size_df = pd.concat([Geo_cell_size_df, label_results])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "5LglSyr-DYH6",
    "outputId": "5b8ce557-44a7-43c1-e480-f854b116b706"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Sort the DataFrame by \"Time\" in descending order\n",
    "df_sorted = Geo_cell_size_df.sort_values(by='Time', ascending=False)\n",
    "\n",
    "# Filter the DataFrame to include only labels 0, 1, 2, and 3\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "selected_clusters = df_sorted[df_sorted['Labels'].isin(selected_labels)]\n",
    "\n",
    "# Create a color cycle for plotting\n",
    "color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate through the selected clusters and plot them with different colors\n",
    "for cluster_label in selected_labels:\n",
    "    cluster_data = selected_clusters[selected_clusters['Labels'] == cluster_label]\n",
    "\n",
    "    color = next(color_cycle)\n",
    "    plt.plot(cluster_data['Time'], cluster_data['Geo_cell_size'], label=f'Cluster {cluster_label}', color=color)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Geo_cell_size')\n",
    "plt.title('Geo_cell_size Over Time for Labels 0, 1, 2, and 3')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouRbVz2yDYH6"
   },
   "source": [
    "###### Cluster_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_48lSBsDYH6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named 'clustered_df' with columns 'Labels', 'Time', and 'Geo_light_intensity'\n",
    "\n",
    "# Get unique labels\n",
    "unique_labels = clustered_df['Labels'].unique()\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "Cluster_dimension_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each unique label and apply the spline fitting\n",
    "for label in unique_labels:\n",
    "    # Select data for the current label\n",
    "    label_data = clustered_df[clustered_df['Labels'] == label]\n",
    "\n",
    "    # Check if the cluster has at least 2 elements\n",
    "    if len(label_data) < 2:\n",
    "        continue  # Skip clusters with less than 2 elements\n",
    "\n",
    "    # Extract Time and Value columns\n",
    "    t_data = label_data['Time']\n",
    "    w_data = label_data['Cluster_dimension']\n",
    "\n",
    "    # Create a cubic spline object\n",
    "    spline = CubicSpline(t_data, w_data)\n",
    "\n",
    "    # Generate finer x values for plotting\n",
    "    t_fine = np.linspace(min(t_data), max(t_data), int(max(t_data)))\n",
    "    w_fine = spline(t_fine)\n",
    "\n",
    "    # Create a new DataFrame for the current label's results\n",
    "    label_results = pd.DataFrame({'Time': t_fine, 'Cluster_dimension': w_fine})\n",
    "\n",
    "    # Add a 'Labels' column with the current label value\n",
    "    label_results['Labels'] = label\n",
    "\n",
    "    # Append the current label's results to the overall results DataFrame\n",
    "    Cluster_dimension_df = pd.concat([Cluster_dimension_df, label_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "kXac3VZEDYH6",
    "outputId": "a65ee137-f83c-405d-d2fb-fd320aaeb123"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Sort the DataFrame by \"Time\" in descending order\n",
    "df_sorted = Cluster_dimension_df.sort_values(by='Time', ascending=False)\n",
    "\n",
    "# Filter the DataFrame to include only labels 0, 1, 2, and 3\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "selected_clusters = df_sorted[df_sorted['Labels'].isin(selected_labels)]\n",
    "\n",
    "# Create a color cycle for plotting\n",
    "color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate through the selected clusters and plot them with different colors\n",
    "for cluster_label in selected_labels:\n",
    "    cluster_data = selected_clusters[selected_clusters['Labels'] == cluster_label]\n",
    "\n",
    "    color = next(color_cycle)\n",
    "    plt.plot(cluster_data['Time'], cluster_data['Cluster_dimension'], label=f'Cluster {cluster_label}', color=color)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cluster_dimension')\n",
    "plt.title('Cluster_dimension Over Time for Labels 0, 1, 2, and 3')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "8e5D_aXfDYH6",
    "outputId": "95c214df-40e3-41e8-9384-adcff1b332cd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Sort the DataFrame by \"Time\" in descending order\n",
    "df_sorted = Cluster_dimension_df.sort_values(by='Time', ascending=False)\n",
    "\n",
    "# Filter the DataFrame to include only labels 0, 1, 2, and 3\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "selected_clusters = df_sorted[df_sorted['Labels'].isin(selected_labels)]\n",
    "\n",
    "# Create a color cycle for plotting\n",
    "color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate through the selected clusters and plot moving averages with different colors\n",
    "for cluster_label in selected_labels:\n",
    "    cluster_data = selected_clusters[selected_clusters['Labels'] == cluster_label]\n",
    "\n",
    "    color = next(color_cycle)\n",
    "\n",
    "    # Calculate the moving average with a window size of, for example, 5\n",
    "    window_size = 5\n",
    "    moving_avg = cluster_data['Cluster_dimension'].rolling(window=window_size).mean()\n",
    "\n",
    "    plt.plot(cluster_data['Time'], moving_avg, label=f'Cluster {cluster_label} (Moving Avg)', color=color)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Moving Average of Cluster_dimension')\n",
    "plt.title('Moving Average of Cluster_dimension Over Time for Labels 0, 1, 2, and 3')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "K-ONnLR2DYH6",
    "outputId": "48a39951-a994-49dc-f2d5-b112a50ce683"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Sort the DataFrame by \"Time\" in descending order\n",
    "df_sorted = Cluster_dimension_df.sort_values(by='Time', ascending=False)\n",
    "\n",
    "# Filter the DataFrame to include only labels 0, 1, 2, and 3\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "selected_clusters = df_sorted[df_sorted['Labels'].isin(selected_labels)]\n",
    "\n",
    "# Create a color cycle for plotting\n",
    "color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate through the selected clusters and plot them with different colors\n",
    "for cluster_label in selected_labels:\n",
    "    cluster_data = selected_clusters[selected_clusters['Labels'] == cluster_label]\n",
    "\n",
    "    color = next(color_cycle)\n",
    "    plt.plot(cluster_data['Time'], cluster_data['Cluster_dimension'], label=f'Cluster {cluster_label}', color=color)\n",
    "\n",
    "    # Calculate and plot the moving average with a window of your choice, e.g., window=5\n",
    "    moving_average = cluster_data['Cluster_dimension'].rolling(window=5).mean()\n",
    "    plt.plot(cluster_data['Time'], moving_average, label=f'Moving Avg {cluster_label}', linestyle='--', color=color)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cluster_dimension')\n",
    "plt.title('Cluster_dimension Over Time for Labels 0, 1, 2, and 3')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVSniLVlDYH6"
   },
   "source": [
    "###### N_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xgER6Yz7DYH6"
   },
   "outputs": [],
   "source": [
    "clustered_df = cluster_new_updated.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyR11SdoDYH6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named 'clustered_df' with columns 'Labels', 'Time', and 'Geo_light_intensity'\n",
    "\n",
    "# Get unique labels\n",
    "unique_labels = clustered_df['Labels'].unique()\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "N_cell_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each unique label and apply the spline fitting\n",
    "for label in unique_labels:\n",
    "    # Select data for the current label\n",
    "    label_data = clustered_df[clustered_df['Labels'] == label]\n",
    "\n",
    "    # Check if the cluster has at least 2 elements\n",
    "    if len(label_data) < 2:\n",
    "        continue  # Skip clusters with less than 2 elements\n",
    "\n",
    "    # Extract Time and Value columns\n",
    "    t_data = label_data['Time']\n",
    "    w_data = label_data['N_cell']\n",
    "\n",
    "    # Create a cubic spline object\n",
    "    spline = CubicSpline(t_data, w_data)\n",
    "\n",
    "    # Generate finer x values for plotting\n",
    "    t_fine = np.linspace(min(t_data), max(t_data), int(max(t_data)))\n",
    "    w_fine = spline(t_fine)\n",
    "\n",
    "    # Create a new DataFrame for the current label's results\n",
    "    label_results = pd.DataFrame({'Time': t_fine, 'N_cell': w_fine})\n",
    "\n",
    "    # Add a 'Labels' column with the current label value\n",
    "    label_results['Labels'] = label\n",
    "\n",
    "    # Append the current label's results to the overall results DataFrame\n",
    "    N_cell_df = pd.concat([N_cell_df, label_results])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "KfCCeKIYDYH6",
    "outputId": "11666f2d-1a96-4249-cdda-561b92445ffc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Sort the DataFrame by \"Time\" in descending order\n",
    "df_sorted = N_cell_df.sort_values(by='Time', ascending=False)\n",
    "\n",
    "# Filter the DataFrame to include only labels 0, 1, 2, and 3\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "selected_clusters = df_sorted[df_sorted['Labels'].isin(selected_labels)]\n",
    "\n",
    "# Create a color cycle for plotting\n",
    "color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate through the selected clusters and plot them with different colors\n",
    "for cluster_label in selected_labels:\n",
    "    cluster_data = selected_clusters[selected_clusters['Labels'] == cluster_label]\n",
    "\n",
    "    color = next(color_cycle)\n",
    "    plt.plot(cluster_data['Time'], cluster_data['N_cell'], label=f'Cluster {cluster_label}', color=color)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('N_cell')\n",
    "plt.title('N_cell Over Time for Labels 0, 1, 2, and 3')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "w5BelsAZDYH6",
    "outputId": "ad87d0a5-4c3a-4c63-f7fa-5624a6d671a5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Sort the DataFrame by \"Time\" in descending order\n",
    "df_sorted = N_cell_df.sort_values(by='Time', ascending=False)\n",
    "\n",
    "# Filter the DataFrame to include only labels 0, 1, 2, and 3\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "selected_clusters = df_sorted[df_sorted['Labels'].isin(selected_labels)]\n",
    "\n",
    "# Create a color cycle for plotting\n",
    "color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate through the selected clusters and plot them with different colors\n",
    "for cluster_label in selected_labels:\n",
    "    cluster_data = selected_clusters[selected_clusters['Labels'] == cluster_label]\n",
    "\n",
    "    color = next(color_cycle)\n",
    "    plt.plot(cluster_data['Time'], cluster_data['N_cell'], label=f'Cluster {cluster_label}', color=color)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('N_cell')\n",
    "plt.title('N_cell Over Time for Labels 0, 1, 2, and 3')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Set the y-axis to log scale\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "h3YsNq8iDYH7",
    "outputId": "88f008d4-d980-45f7-d20b-c9ab3e3334d1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Sort the DataFrame by \"Time\" in descending order\n",
    "df_sorted = N_cell_df.sort_values(by='Time', ascending=False)\n",
    "\n",
    "# Filter the DataFrame to include only labels 0, 1, 2, and 3\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "selected_clusters = df_sorted[df_sorted['Labels'].isin(selected_labels)]\n",
    "\n",
    "# Create a color cycle for plotting\n",
    "color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate through the selected clusters and plot them with different colors\n",
    "for cluster_label in selected_labels:\n",
    "    cluster_data = selected_clusters[selected_clusters['Labels'] == cluster_label]\n",
    "\n",
    "    color = next(color_cycle)\n",
    "\n",
    "    # Plot the original data\n",
    "    plt.plot(cluster_data['Time'], cluster_data['N_cell'], label=f'Cluster {cluster_label}', color=color)\n",
    "\n",
    "    # Compute and plot the moving average with a window of your choice, e.g., window=5\n",
    "    moving_average = cluster_data['N_cell'].rolling(window=5).mean()\n",
    "    plt.plot(cluster_data['Time'], moving_average, label=f'Moving Avg {cluster_label}', linestyle='--', color=color)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('N_cell')\n",
    "plt.title('N_cell Over Time for Labels 0, 1, 2, and 3')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Set the y-axis to log scale\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HAk2z_-DYH7"
   },
   "source": [
    "We can understand the closest exponential function achievable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "zfVOBEI-DYH7",
    "outputId": "d93c9e2a-816b-458b-dcc9-2972ea723535"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sort the DataFrame by \"Time\" in descending order\n",
    "df_sorted = N_cell_df.sort_values(by='Time', ascending=False)\n",
    "\n",
    "# Filter the DataFrame to include only labels 0, 1, 2, and 3\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "selected_clusters = df_sorted[df_sorted['Labels'].isin(selected_labels)]\n",
    "\n",
    "# Create a color cycle for plotting\n",
    "color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate through the selected clusters and plot them with different colors\n",
    "for cluster_label in selected_labels:\n",
    "    cluster_data = selected_clusters[selected_clusters['Labels'] == cluster_label]\n",
    "\n",
    "    color = next(color_cycle)\n",
    "\n",
    "    # Calculate the moving average with a window size of, for example, 5\n",
    "    window_size = 5\n",
    "    moving_avg = cluster_data['N_cell'].rolling(window=window_size).mean()\n",
    "\n",
    "    plt.plot(cluster_data['Time'], moving_avg, label=f'Cluster {cluster_label} (Moving Avg)', color=color)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('N_cell')\n",
    "plt.title('Moving Average of N_cell Over Time for Labels 0, 1, 2, and 3')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzzqGdo2DYH7"
   },
   "source": [
    "###### Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wz0zrUJODYH7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named 'clustered_df' with columns 'Labels', 'Time', and 'Geo_light_intensity'\n",
    "\n",
    "# Get unique labels\n",
    "unique_labels = clustered_df['Labels'].unique()\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "Mean_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each unique label and apply the spline fitting\n",
    "for label in unique_labels:\n",
    "    # Select data for the current label\n",
    "    label_data = clustered_df[clustered_df['Labels'] == label]\n",
    "\n",
    "    # Check if the cluster has at least 2 elements\n",
    "    if len(label_data) < 2:\n",
    "        continue  # Skip clusters with less than 2 elements\n",
    "\n",
    "    # Extract Time and Value columns\n",
    "    t_data = label_data['Time']\n",
    "    w_data = label_data['Mean']\n",
    "\n",
    "    # Create a cubic spline object\n",
    "    spline = CubicSpline(t_data, w_data)\n",
    "\n",
    "    # Generate finer x values for plotting\n",
    "    t_fine = np.linspace(min(t_data), max(t_data), int(max(t_data)))\n",
    "    w_fine = spline(t_fine)\n",
    "\n",
    "    # Create a new DataFrame for the current label's results\n",
    "    label_results = pd.DataFrame({'Time': t_fine, 'Mean': w_fine})\n",
    "\n",
    "    # Add a 'Labels' column with the current label value\n",
    "    label_results['Labels'] = label\n",
    "\n",
    "    # Append the current label's results to the overall results DataFrame\n",
    "    Mean_df = pd.concat([Mean_df, label_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "UCJWs0kvDYH7",
    "outputId": "71da5b79-5ded-4807-d60e-51d4f089ebf5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Sort the DataFrame by \"Time\" in descending order\n",
    "df_sorted = Mean_df.sort_values(by='Time', ascending=False)\n",
    "\n",
    "# Filter the DataFrame to include only labels 0, 1, 2, and 3\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "selected_clusters = df_sorted[df_sorted['Labels'].isin(selected_labels)]\n",
    "\n",
    "# Create a color cycle for plotting\n",
    "color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate through the selected clusters and plot them with different colors\n",
    "for cluster_label in selected_labels:\n",
    "    cluster_data = selected_clusters[selected_clusters['Labels'] == cluster_label]\n",
    "\n",
    "    color = next(color_cycle)\n",
    "    plt.plot(cluster_data['Time'], cluster_data['Mean'], label=f'Cluster {cluster_label}', color=color)\n",
    "\n",
    "    # Calculate and plot the moving average with a window of your choice, e.g., window=5\n",
    "    moving_average = cluster_data['Mean'].rolling(window=5).mean()\n",
    "    plt.plot(cluster_data['Time'], moving_average, label=f'Moving Avg {cluster_label}', linestyle='--', color=color)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Mean')\n",
    "plt.title('Mean Over Time for Labels 0, 1, 2, and 3')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axqYTU6gDYH7"
   },
   "source": [
    "###### IntDen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGlMPH6QDYH7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named 'clustered_df' with columns 'Labels', 'Time', and 'Geo_light_intensity'\n",
    "\n",
    "# Get unique labels\n",
    "unique_labels = clustered_df['Labels'].unique()\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "IntDen_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each unique label and apply the spline fitting\n",
    "for label in unique_labels:\n",
    "    # Select data for the current label\n",
    "    label_data = clustered_df[clustered_df['Labels'] == label]\n",
    "\n",
    "    # Check if the cluster has at least 2 elements\n",
    "    if len(label_data) < 2:\n",
    "        continue  # Skip clusters with less than 2 elements\n",
    "\n",
    "    # Extract Time and Value columns\n",
    "    t_data = label_data['Time']\n",
    "    w_data = label_data['IntDen']\n",
    "\n",
    "    # Create a cubic spline object\n",
    "    spline = CubicSpline(t_data, w_data)\n",
    "\n",
    "    # Generate finer x values for plotting\n",
    "    t_fine = np.linspace(min(t_data), max(t_data), int(max(t_data)))\n",
    "    w_fine = spline(t_fine)\n",
    "\n",
    "    # Create a new DataFrame for the current label's results\n",
    "    label_results = pd.DataFrame({'Time': t_fine, 'IntDen': w_fine})\n",
    "\n",
    "    # Add a 'Labels' column with the current label value\n",
    "    label_results['Labels'] = label\n",
    "\n",
    "    # Append the current label's results to the overall results DataFrame\n",
    "    IntDen_df = pd.concat([IntDen_df, label_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "kVpa68aoDYH7",
    "outputId": "48491caa-ac32-4786-ee16-4c05941f6d6d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Sort the DataFrame by \"Time\" in descending order\n",
    "df_sorted = IntDen_df.sort_values(by='Time', ascending=False)\n",
    "\n",
    "# Filter the DataFrame to include only labels 0, 1, 2, and 3\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "selected_clusters = df_sorted[df_sorted['Labels'].isin(selected_labels)]\n",
    "\n",
    "# Create a color cycle for plotting\n",
    "color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate through the selected clusters and plot them with different colors\n",
    "for cluster_label in selected_labels:\n",
    "    cluster_data = selected_clusters[selected_clusters['Labels'] == cluster_label]\n",
    "\n",
    "    color = next(color_cycle)\n",
    "    plt.plot(cluster_data['Time'], cluster_data['IntDen'], label=f'Cluster {cluster_label}', color=color)\n",
    "\n",
    "    # Calculate and plot the moving average with a window of your choice, e.g., window=5\n",
    "    moving_average = cluster_data['IntDen'].rolling(window=5).mean()\n",
    "    plt.plot(cluster_data['Time'], moving_average, label=f'Moving Avg {cluster_label}', linestyle='--', color=color)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('IntDen')\n",
    "plt.title('IntDen Over Time for Labels 0, 1, 2, and 3')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3iJK6w1EDYH7"
   },
   "source": [
    "###### StdDev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "En5lgMASDYH7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named 'clustered_df' with columns 'Labels', 'Time', and 'Geo_light_intensity'\n",
    "\n",
    "# Get unique labels\n",
    "unique_labels = clustered_df['Labels'].unique()\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "StdDev_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each unique label and apply the spline fitting\n",
    "for label in unique_labels:\n",
    "    # Select data for the current label\n",
    "    label_data = clustered_df[clustered_df['Labels'] == label]\n",
    "\n",
    "    # Check if the cluster has at least 2 elements\n",
    "    if len(label_data) < 2:\n",
    "        continue  # Skip clusters with less than 2 elements\n",
    "\n",
    "    # Extract Time and Value columns\n",
    "    t_data = label_data['Time']\n",
    "    w_data = label_data['StdDev']\n",
    "\n",
    "    # Create a cubic spline object\n",
    "    spline = CubicSpline(t_data, w_data)\n",
    "\n",
    "    # Generate finer x values for plotting\n",
    "    t_fine = np.linspace(min(t_data), max(t_data), int(max(t_data)))\n",
    "    w_fine = spline(t_fine)\n",
    "\n",
    "    # Create a new DataFrame for the current label's results\n",
    "    label_results = pd.DataFrame({'Time': t_fine, 'StdDev': w_fine})\n",
    "\n",
    "    # Add a 'Labels' column with the current label value\n",
    "    label_results['Labels'] = label\n",
    "\n",
    "    # Append the current label's results to the overall results DataFrame\n",
    "    StdDev_df = pd.concat([StdDev_df, label_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "RLtDEd53DYH7",
    "outputId": "e79df58e-672c-45c0-877e-27eb10bb4d1a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Sort the DataFrame by \"Time\" in descending order\n",
    "df_sorted = StdDev_df.sort_values(by='Time', ascending=False)\n",
    "\n",
    "# Filter the DataFrame to include only labels 0, 1, 2, and 3\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "selected_clusters = df_sorted[df_sorted['Labels'].isin(selected_labels)]\n",
    "\n",
    "# Create a color cycle for plotting\n",
    "color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate through the selected clusters and plot them with different colors\n",
    "for cluster_label in selected_labels:\n",
    "    cluster_data = selected_clusters[selected_clusters['Labels'] == cluster_label]\n",
    "\n",
    "    color = next(color_cycle)\n",
    "    plt.plot(cluster_data['Time'], cluster_data['StdDev'], label=f'Cluster {cluster_label}', color=color)\n",
    "\n",
    "    # Calculate and plot the moving average with a window of your choice, e.g., window=5\n",
    "    moving_average = cluster_data['StdDev'].rolling(window=5).mean()\n",
    "    plt.plot(cluster_data['Time'], moving_average, label=f'Moving Avg {cluster_label}', linestyle='--', color=color)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('StdDev')\n",
    "plt.title('StdDev Over Time for Labels 0, 1, 2, and 3')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2b3y79sDYH7"
   },
   "source": [
    "###### Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhAHaVa7DYH7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named 'clustered_df' with columns 'Labels', 'Time', and 'Geo_light_intensity'\n",
    "\n",
    "# Get unique labels\n",
    "unique_labels = clustered_df['Labels'].unique()\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "Area_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each unique label and apply the spline fitting\n",
    "for label in unique_labels:\n",
    "    # Select data for the current label\n",
    "    label_data = clustered_df[clustered_df['Labels'] == label]\n",
    "\n",
    "    # Check if the cluster has at least 2 elements\n",
    "    if len(label_data) < 2:\n",
    "        continue  # Skip clusters with less than 2 elements\n",
    "\n",
    "    # Extract Time and Value columns\n",
    "    t_data = label_data['Time']\n",
    "    w_data = label_data['Area']\n",
    "\n",
    "    # Create a cubic spline object\n",
    "    spline = CubicSpline(t_data, w_data)\n",
    "\n",
    "    # Generate finer x values for plotting\n",
    "    t_fine = np.linspace(min(t_data), max(t_data), int(max(t_data)))\n",
    "    w_fine = spline(t_fine)\n",
    "\n",
    "    # Create a new DataFrame for the current label's results\n",
    "    label_results = pd.DataFrame({'Time': t_fine, 'Area': w_fine})\n",
    "\n",
    "    # Add a 'Labels' column with the current label value\n",
    "    label_results['Labels'] = label\n",
    "\n",
    "    # Append the current label's results to the overall results DataFrame\n",
    "    Area_df = pd.concat([Area_df, label_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "iD7Gm4XeDYH7",
    "outputId": "b7013076-f3e8-4791-9d51-f35792300aca"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Sort the DataFrame by \"Time\" in descending order\n",
    "df_sorted = Area_df.sort_values(by='Time', ascending=False)\n",
    "\n",
    "# Filter the DataFrame to include only labels 0, 1, 2, and 3\n",
    "selected_labels = [0, 1, 2, 3]\n",
    "selected_clusters = df_sorted[df_sorted['Labels'].isin(selected_labels)]\n",
    "\n",
    "# Create a color cycle for plotting\n",
    "color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate through the selected clusters and plot them with different colors\n",
    "for cluster_label in selected_labels:\n",
    "    cluster_data = selected_clusters[selected_clusters['Labels'] == cluster_label]\n",
    "\n",
    "    color = next(color_cycle)\n",
    "    plt.plot(cluster_data['Time'], cluster_data['Area'], label=f'Cluster {cluster_label}', color=color)\n",
    "\n",
    "    # Calculate and plot the moving average with a window of your choice, e.g., window=5\n",
    "    moving_average = cluster_data['Area'].rolling(window=5).mean()\n",
    "    plt.plot(cluster_data['Time'], moving_average, label=f'Moving Avg {cluster_label}', linestyle='--', color=color)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Area')\n",
    "plt.title('Area Over Time for Labels 0, 1, 2, and 3')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eVvLkxiDYH7"
   },
   "source": [
    "##### Analysis of a particular cluster in space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3HpsbqwDYH7"
   },
   "source": [
    "###### Functions used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5gs_PMADDYH7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_distance_vs_cell_location(data, window_size=2):\n",
    "    df = data.copy()\n",
    "\n",
    "    # Calculate the barycenter (centroid)\n",
    "    Barycenter_x = df['X'].mean()\n",
    "    Barycenter_y = df['Y'].mean()\n",
    "\n",
    "    # Calculate the distance for each data point\n",
    "    df['Distance'] = df.apply(lambda row: math.sqrt((row['X'] - Barycenter_x)**2 + (row['Y'] - Barycenter_y)**2), axis=1)\n",
    "\n",
    "    # Sort the data by distance\n",
    "    df.sort_values(by='Distance', inplace=True)\n",
    "\n",
    "    # Calculate the cumulative count of cells with distances lower than or equal to each distance\n",
    "    df['Cumulative_Count'] = np.arange(1, len(df) + 1)\n",
    "\n",
    "    # Calculate the derivative (change in cumulative count) using numpy's gradient function\n",
    "    df['Derivative'] = np.gradient(df['Cumulative_Count'], df['Distance'])\n",
    "\n",
    "    # Create subplots with two graphs (main cumulative distribution and derivative)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Main cumulative distribution plot\n",
    "    ax1.plot(df['Distance'], df['Cumulative_Count'], marker='o', linestyle='-', color='b')\n",
    "    ax1.set_xlabel('Distance from Barycenter')\n",
    "    ax1.set_ylabel('Number of Cells (Cumulative)')\n",
    "    ax1.set_title('Cumulative Distribution of Cell Distances from Barycenter')\n",
    "\n",
    "    # Derivative plot\n",
    "    ax2.plot(df['Distance'], df['Derivative'], marker='o', linestyle='-', color='r')\n",
    "    ax2.set_xlabel('Distance from Barycenter')\n",
    "    ax2.set_ylabel('Derivative (Change in Cumulative Count)')\n",
    "    ax2.set_title('Derivative of Cumulative Distribution')\n",
    "\n",
    "    # Optionally, you can save the plot to a file or display it\n",
    "    plt.savefig('cumulative_and_derivative_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a DataFrame 'data' with columns 'X' and 'Y', representing cell coordinates,\n",
    "# and 'Distance', 'Cumulative_Count', and 'Derivative' columns calculated using your function.\n",
    "# plot_distance_vs_cell_location(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QjFqism3DYH8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def plot_distance_vs_value(data):\n",
    "    df = data.copy()\n",
    "    # Calculate the barycenter (centroid)\n",
    "    Barycenter_x = df['X'].mean()\n",
    "    Barycenter_y = df['Y'].mean()\n",
    "\n",
    "    # Calculate the distance for each data point\n",
    "    df['Distance'] = df.apply(lambda row: math.sqrt((row['X'] - Barycenter_x)**2 + (row['Y'] - Barycenter_y)**2), axis=1)\n",
    "\n",
    "    # Create a scatter plot of 'Distance' vs 'IntDen'\n",
    "    plt.scatter(df['Distance'], df['IntDen'], alpha=0.7)\n",
    "    plt.xlabel('Distance to Barycenter')\n",
    "    plt.ylabel('IntDen')\n",
    "    plt.title('IntDen vs. Distance to Barycenter')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# plot_distance_vs_value(your_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfCyyXf-DYH8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def plot_distance_vs_value_with_histogram(data, bins=10):\n",
    "    df = data.copy()\n",
    "    # Calculate the barycenter (centroid)\n",
    "    Barycenter_x = df['X'].mean()\n",
    "    Barycenter_y = df['Y'].mean()\n",
    "\n",
    "    # Calculate the distance for each data point\n",
    "    df['Distance'] = df.apply(lambda row: math.sqrt((row['X'] - Barycenter_x)**2 + (row['Y'] - Barycenter_y)**2), axis=1)\n",
    "\n",
    "    # Create a histogram of the 'Distance' column\n",
    "    plt.hist(df['Distance'], bins=bins, edgecolor='k', alpha=0.7)\n",
    "    plt.xlabel('Distance to Barycenter')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distance to Barycenter Histogram')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTsz7UFVDYH8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def plot_moving_average(data, window_size=3500):\n",
    "    df = data.copy()\n",
    "    # Calculate the barycenter (centroid)\n",
    "    Barycenter_x = df['X'].mean()\n",
    "    Barycenter_y = df['Y'].mean()\n",
    "\n",
    "    # Calculate the distance for each data point\n",
    "    df['Distance'] = df.apply(lambda row: math.sqrt((row['X'] - Barycenter_x)**2 + (row['Y'] - Barycenter_y)**2), axis=1)\n",
    "\n",
    "    # Create a new DataFrame for plotting\n",
    "    plot_df = df[['Distance', 'IntDen']]\n",
    "\n",
    "    # Sort the DataFrame by distance for a cleaner plot\n",
    "    plot_df = plot_df.sort_values(by='Distance')\n",
    "\n",
    "    # Calculate the moving average of the 'IntDen' column with a window size of 30\n",
    "    plot_df['Moving_Average'] = plot_df['IntDen'].rolling(window=window_size).mean()\n",
    "\n",
    "    # Plot the moving average\n",
    "    plt.plot(plot_df['Distance'], plot_df['Moving_Average'], linestyle='--', label='Moving Average')\n",
    "    plt.xlabel('Distance to Barycenter')\n",
    "    plt.ylabel('Moving Average of IntDen')\n",
    "    plt.title(f'Moving Average of IntDen vs. Distance to Barycenter (Window Size {window_size})')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J89kfdR8DYH8"
   },
   "outputs": [],
   "source": [
    "def plot_values_based_on_the_area_in_space(data, window_size=3500):\n",
    "    df = data.copy()\n",
    "    # Calculate the barycenter (centroid)\n",
    "    Barycenter_x = df['X'].mean()\n",
    "    Barycenter_y = df['Y'].mean()\n",
    "\n",
    "    # Calculate the distance for each data point\n",
    "    df['Distance'] = df.apply(lambda row: math.sqrt((row['X'] - Barycenter_x)**2 + (row['Y'] - Barycenter_y)**2), axis=1)\n",
    "\n",
    "    # Create a new DataFrame for plotting\n",
    "    plot_df = df[['Distance', 'IntDen']]\n",
    "\n",
    "    # Sort the DataFrame by distance for a cleaner plot\n",
    "    plot_df = plot_df.sort_values(by='Distance')\n",
    "\n",
    "    # I want to divide my label my points based on the distance they are. I can consider a value called d_value. Making an example, if there is a point whose distance is x < d_value, the label is one. If the distance is between 2*d_value < x < d_value, the label is two.\n",
    "    # After that, I want to create a new dataframe where I have three columns. One is the sum of IntDen of the raws with the same labels, the other is the area of the considered area, considered as pi*(value of the label**2 - (value of the label - 2)**2), the last one\n",
    "    # is the ratio between those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIdGO-IuDYH8"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_values_based_on_the_area_in_space(data, d_value=7.5):\n",
    "    # Calculate the barycenter (centroid)\n",
    "    Barycenter_x = data['X'].mean()\n",
    "    Barycenter_y = data['Y'].mean()\n",
    "\n",
    "    # Calculate the distance for each data point\n",
    "    data['Distance'] = ((data['X'] - Barycenter_x)**2 + (data['Y'] - Barycenter_y)**2).apply(math.sqrt)\n",
    "\n",
    "    # Create a new DataFrame for plotting\n",
    "    plot_df = data[['Distance', 'IntDen']]\n",
    "\n",
    "    # Sort the DataFrame by distance for a cleaner plot\n",
    "    plot_df = plot_df.sort_values('Distance')\n",
    "\n",
    "    # Assign labels based on distance\n",
    "    plot_df['Label'] = plot_df['Distance'].apply(lambda distance: math.ceil(distance / d_value))\n",
    "    plot_df['New_Distance'] = plot_df['Label']*d_value - d_value / 2\n",
    "\n",
    "    # Calculate area and sum of IntDen\n",
    "    plot_df['Area'] = math.pi * (plot_df['Label']**2 - (plot_df['Label'] - 1)**2)\n",
    "    plot_df['Sum_IntDen'] = plot_df.groupby('Label')['IntDen'].transform('sum')\n",
    "    plot_df['Ratio'] = plot_df['Sum_IntDen'] / plot_df['Area']\n",
    "\n",
    "    # Plotting the function\n",
    "    plt.plot(plot_df['New_Distance'], plot_df['Ratio'])\n",
    "    plt.xlabel('New Distance')\n",
    "    plt.ylabel('Ratio')\n",
    "    plt.title('Ratio vs New Distance')\n",
    "    plt.show()\n",
    "\n",
    "    return plot_df\n",
    "\n",
    "# Example usage:\n",
    "# plot_values_based_on_the_area_in_space(selected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R17BSLYTDYH8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "from google.colab import drive\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    # Extract and return the numeric part of the filename\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\\d+)', s)]\n",
    "\n",
    "def create_video_from_pngs(folder_path, output_video_path, frame_size=(1920, 1080), frame_rate=10):\n",
    "    \"\"\"\n",
    "    Create a video from PNG files in a Google Drive folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder containing PNG files.\n",
    "        output_video_path (str): The path where the output video will be saved.\n",
    "        frame_size (tuple): The frame size of the output video (width, height).\n",
    "        frame_rate (int): The frame rate of the output video.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Sort the PNG files by their filenames using natural sorting\n",
    "    png_files = sorted([os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if filename.endswith('.png')], key=natural_sort_key)\n",
    "\n",
    "    # Initialize the video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # You can use other codecs like 'XVID' or 'MJPG'\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, frame_rate, frame_size)\n",
    "\n",
    "    # Create the video from PNG files\n",
    "    for png_file in png_files:\n",
    "        frame = cv2.imread(png_file)\n",
    "        frame = cv2.resize(frame, frame_size)\n",
    "        out.write(frame)\n",
    "\n",
    "    # Release the video writer\n",
    "    out.release()\n",
    "\n",
    "    # Unmount Google Drive to avoid errors when saving the video\n",
    "    drive.flush_and_unmount()\n",
    "\n",
    "    print(f\"Video '{output_video_path}' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmRNmSBjDYH8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_distance_vs_moving_average_with_labels(data, labels=[0, 1, 2, 3], window_size=2, interruption_point=None):\n",
    "    df = data.copy()\n",
    "\n",
    "    # Filter the DataFrame to select rows with specific labels\n",
    "    df = df[df['Labels'].isin(labels)]\n",
    "\n",
    "    # Initialize an empty dictionary to store barycenters for each label\n",
    "    barycenters = {}\n",
    "\n",
    "    # Calculate the barycenter (centroid) for each selected label\n",
    "    for label in labels:\n",
    "        label_df = df[df['Labels'] == label]\n",
    "        barycenter_x = label_df['X'].mean()\n",
    "        barycenter_y = label_df['Y'].mean()\n",
    "        barycenters[label] = (barycenter_x, barycenter_y)\n",
    "\n",
    "    # Calculate the distance for each data point based on its label-dependent barycenter\n",
    "    df['Distance'] = df.apply(lambda row: math.sqrt((row['X'] - barycenters[row['Labels']][0])**2 + (row['Y'] - barycenters[row['Labels']][1])**2), axis=1)\n",
    "\n",
    "    # Determine the maximum distance for the x-axis\n",
    "    max_distance = df['Distance'].max()\n",
    "\n",
    "    if interruption_point is not None:\n",
    "        # Filter the DataFrame to include only data points with distances less than the interruption_point\n",
    "        df = df[df['Distance'] < interruption_point]\n",
    "\n",
    "    # Create a new DataFrame for plotting\n",
    "    plot_df = df[['Distance', 'IntDen']]\n",
    "\n",
    "    # Sort the DataFrame by distance for a cleaner plot\n",
    "    plot_df = plot_df.sort_values(by='Distance')\n",
    "\n",
    "    # Calculate the moving average of the 'IntDen' column\n",
    "    plot_df['Moving_Average'] = plot_df['IntDen'].rolling(window=window_size).mean()\n",
    "\n",
    "    # Plot only the moving average\n",
    "    plt.plot(plot_df['Distance'], plot_df['Moving_Average'], linestyle='--', label='Moving Average')\n",
    "    plt.xlabel('Distance to Barycenter')\n",
    "    plt.ylabel('Moving Average of IntDen')\n",
    "    plt.title('Moving Average of IntDen vs. Distance to Barycenter')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a DataFrame 'df' with columns 'X', 'Y', 'IntDen', and 'Labels'\n",
    "# plot_distance_vs_moving_average_with_labels(df, labels=[0, 1, 2, 3], window_size=2, interruption_point=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lC30NdTwDYH8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def calculate_correlation(data, labels=[0, 1, 2, 3], interruption_point=None):\n",
    "    df = data.copy()\n",
    "\n",
    "    # Filter the DataFrame to select rows with specific labels\n",
    "    df = df[df['Labels'].isin(labels)]\n",
    "\n",
    "    # Initialize an empty dictionary to store barycenters for each label\n",
    "    barycenters = {}\n",
    "\n",
    "    # Calculate the barycenter (centroid) for each selected label\n",
    "    for label in labels:\n",
    "        label_df = df[df['Labels'] == label]\n",
    "        barycenter_x = label_df['X'].mean()\n",
    "        barycenter_y = label_df['Y'].mean()\n",
    "        barycenters[label] = (barycenter_x, barycenter_y)\n",
    "\n",
    "    # Calculate the distance for each data point based on its label-dependent barycenter\n",
    "    df['Distance'] = df.apply(lambda row: math.sqrt((row['X'] - barycenters[row['Labels']][0])**2 + (row['Y'] - barycenters[row['Labels']][1])**2), axis=1)\n",
    "\n",
    "    if interruption_point is not None:\n",
    "        # Filter the DataFrame to include only data points with distances less than the interruption_point\n",
    "        df = df[df['Distance'] < interruption_point]\n",
    "\n",
    "    # Calculate the correlation between 'Distance' and 'IntDen' columns\n",
    "    correlation = df['Distance'].corr(df['IntDen'])\n",
    "\n",
    "    return correlation\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a DataFrame 'df' with columns 'X', 'Y', 'IntDen', and 'Labels'\n",
    "# correlation = calculate_correlation(df, labels=[0, 1, 2, 3], interruption_point=10)\n",
    "# print(\"Correlation between Distance and IntDen:\", correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8N15NWgCDYH8"
   },
   "outputs": [],
   "source": [
    "def calculate_correlation_single(data, labels=[0, 1, 2, 3], interruption_point=None):\n",
    "    df = data.copy()\n",
    "\n",
    "    # Initialize an empty dictionary to store barycenters for each label\n",
    "    barycenters = {}\n",
    "\n",
    "    correlations = {}\n",
    "\n",
    "    for label in labels:\n",
    "        # Filter the DataFrame to select rows with the current label\n",
    "        label_df = df[df['Labels'] == label]\n",
    "\n",
    "        if label_df.empty:\n",
    "            continue  # Skip labels with no data\n",
    "\n",
    "        # Calculate the barycenter (centroid) for the current label\n",
    "        barycenter_x = label_df['X'].mean()\n",
    "        barycenter_y = label_df['Y'].mean()\n",
    "        barycenters[label] = (barycenter_x, barycenter_y)\n",
    "\n",
    "        # Calculate the distance for each data point based on its label-dependent barycenter\n",
    "        label_df['Distance'] = label_df.apply(lambda row: math.sqrt((row['X'] - barycenters[label][0])**2 + (row['Y'] - barycenters[label][1])**2), axis=1)\n",
    "\n",
    "        if interruption_point is not None:\n",
    "            # Filter the DataFrame to include only data points with distances less than the interruption_point\n",
    "            label_df = label_df[label_df['Distance'] < interruption_point]\n",
    "\n",
    "        # Calculate the correlation between 'Distance' and 'IntDen' columns for the current label\n",
    "        correlation = label_df['Distance'].corr(label_df['IntDen'])\n",
    "        correlations[label] = correlation\n",
    "\n",
    "\n",
    "    return correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2yjcurPDYH8"
   },
   "outputs": [],
   "source": [
    "def calculate_correlation_single_Area(data, labels=[0, 1, 2, 3], interruption_point=None):\n",
    "    df = data.copy()\n",
    "\n",
    "    # Initialize an empty dictionary to store barycenters for each label\n",
    "    barycenters = {}\n",
    "\n",
    "    correlations = {}\n",
    "\n",
    "    for label in labels:\n",
    "        # Filter the DataFrame to select rows with the current label\n",
    "        label_df = df[df['Labels'] == label]\n",
    "\n",
    "        if label_df.empty:\n",
    "            continue  # Skip labels with no data\n",
    "\n",
    "        # Calculate the barycenter (centroid) for the current label\n",
    "        barycenter_x = label_df['X'].mean()\n",
    "        barycenter_y = label_df['Y'].mean()\n",
    "        barycenters[label] = (barycenter_x, barycenter_y)\n",
    "\n",
    "        # Calculate the distance for each data point based on its label-dependent barycenter\n",
    "        label_df['Distance'] = label_df.apply(lambda row: math.sqrt((row['X'] - barycenters[label][0])**2 + (row['Y'] - barycenters[label][1])**2), axis=1)\n",
    "\n",
    "        if interruption_point is not None:\n",
    "            # Filter the DataFrame to include only data points with distances less than the interruption_point\n",
    "            label_df = label_df[label_df['Distance'] < interruption_point]\n",
    "\n",
    "        # Calculate the correlation between 'Distance' and 'IntDen' columns for the current label\n",
    "        correlation = label_df['Distance'].corr(label_df['Area'])\n",
    "        correlations[label] = correlation\n",
    "\n",
    "\n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9WNF7xPXDYH8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vbYo3DODYH8"
   },
   "source": [
    "###### Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "oG4J3ThvDYH8",
    "outputId": "0643d10f-f193-4069-9801-92328d2b3df0"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Replace 'data' with your actual data\n",
    "data = df_all_new_updated['IntDen']\n",
    "\n",
    "# Create probability plots for different distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Normal distribution\n",
    "plt.subplot(231)\n",
    "stats.probplot(data, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Normal Distribution\")\n",
    "\n",
    "# Exponential distribution\n",
    "plt.subplot(232)\n",
    "stats.probplot(data, dist=\"expon\", plot=plt)\n",
    "plt.title(\"Exponential Distribution\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "Kc5G-0H5DYH8",
    "outputId": "d46a1920-6ab2-4a93-f835-01bcbf559cc0"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'points_cluster_0_time_5' is your DataFrame with the 'IntDen' column\n",
    "\n",
    "# Plot the Gaussian distribution using seaborn\n",
    "sns.histplot(points_cluster_0_time_5['IntDen'], kde=True, bins=20, color='blue')\n",
    "plt.xlabel('IntDen')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Gaussian Distribution of IntDen')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "oeUwrVmaDYH8",
    "outputId": "696bc26d-70c2-4e11-de0e-3d7a63745170"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_value_with_moving_average(points_cluster_0_time_5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "yizhiMDZDYH8",
    "outputId": "dfc0fc5d-1220-4fe3-b99e-4ba8e95177d6"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_value(selected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "Df-iDKP0DYH9",
    "outputId": "fd1df514-abe5-4b05-bf83-56bd050fb984"
   },
   "outputs": [],
   "source": [
    "plot_moving_average(selected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2OOUGbkDYH9"
   },
   "outputs": [],
   "source": [
    "folder_id = '/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/New_algorithm/clusters/N=0123'\n",
    "output_video_path = '/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/New_algorithm/clusters/N=0123/video.mp4'\n",
    "create_video_from_pngs(folder_id,output_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GqBYBlv4DYH9"
   },
   "outputs": [],
   "source": [
    "visual = plot_values_based_on_the_area_in_space(selected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "kx5XO3DCDYH9",
    "outputId": "89bcbd35-68be-4d58-d0b9-02a16e3f34c0"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Replace 'data' with your actual data\n",
    "data = visual['IntDen']\n",
    "\n",
    "# Create probability plots for different distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Normal distribution\n",
    "plt.subplot(231)\n",
    "stats.probplot(data, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Normal Distribution\")\n",
    "\n",
    "# Exponential distribution\n",
    "plt.subplot(232)\n",
    "stats.probplot(data, dist=\"expon\", plot=plt)\n",
    "plt.title(\"Exponential Distribution\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "g5441MjADYH9",
    "outputId": "36d73946-1a02-4b04-9e66-b3fdc4ad649f"
   },
   "outputs": [],
   "source": [
    "f0 = plot_distance_histogram_with_custom_ticks(df_distance_C0_T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "id": "iqJpuwUtDYH9",
    "outputId": "13700db8-f96c-4cab-92e2-196ebb064d63"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_cell_location(df_distance_C0_T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OS8tV8zIDYH9",
    "outputId": "8108be6f-9d25-49bb-eae6-ed71a34d8c69"
   },
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "u3rH_qp6DYH9",
    "outputId": "2491abd5-0f3a-4068-ce50-6c441a44d3f0"
   },
   "outputs": [],
   "source": [
    "f1 = plot_distance_histogram_with_custom_ticks(df_distance_C1_T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "id": "jpxJ1O_CDYH9",
    "outputId": "2df5c2b6-7e2c-43aa-8f0f-ac4c0c9247bd"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_cell_location(df_distance_C1_T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "id": "jjHLyNbiDYH9",
    "outputId": "98d96fe2-52bb-410d-8abb-5132a66cfe80"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_cell_location(df_distance_C2_T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u8TqjA5TDYH9",
    "outputId": "2ebb4662-8dc5-4315-eda3-1f076b0503c5"
   },
   "outputs": [],
   "source": [
    "f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "kk0gN9XODYH9",
    "outputId": "1c66e896-7a8e-43d2-b467-82c8ac727aad"
   },
   "outputs": [],
   "source": [
    "f2 = plot_distance_histogram_with_custom_ticks(df_distance_C2_T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "07PPtSg2DYH9",
    "outputId": "20a81507-141c-4385-d2eb-f3442e96e220"
   },
   "outputs": [],
   "source": [
    "df_see = plot_distance_vs_cell_location(df_distance_C3_T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "8YwsFpjYDYH9",
    "outputId": "1c9d1763-f3de-4633-ac84-40beeca03202"
   },
   "outputs": [],
   "source": [
    "plot_distance_histogram_with_custom_ticks(df_distance_C3_T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "p20xNyhCDYH9",
    "outputId": "7c1441c2-e3ea-4ef9-ebaf-ed99d5e5c9b7"
   },
   "outputs": [],
   "source": [
    "first_d = plot_distance_histogram_with_custom_ticks(df_distance_C3_T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FSHVnO9xDYH9",
    "outputId": "d5d305f2-31dd-4ddc-fbb0-f6cdb6b76b49"
   },
   "outputs": [],
   "source": [
    "first_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYr1uRELDYH9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_distance_histogram_with_custom_ticks(data):\n",
    "    df = data.copy()\n",
    "\n",
    "    # Calculate the barycenter (centroid)\n",
    "    Barycenter_x = df['X'].mean()\n",
    "    Barycenter_y = df['Y'].mean()\n",
    "\n",
    "    # Calculate the distance for each data point\n",
    "    df['Distance'] = df.apply(lambda row: math.sqrt((row['X'] - Barycenter_x)**2 + (row['Y'] - Barycenter_y)**2), axis=1)\n",
    "    custom_ticks = int(max(df['Distance']) / 7.5)\n",
    "    # Create a histogram of distances with custom ticks\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    n, bins, patches = plt.hist(df['Distance'], bins=custom_ticks, color='b', alpha=0.7)\n",
    "    plt.xlabel('Distance from Barycenter')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Cell Distances from Barycenter')\n",
    "\n",
    "    # Find the first bin with a count of zero or close to zero\n",
    "    first_zero_bin = None\n",
    "    for i, count in enumerate(n):\n",
    "        if count <= 1e-6:  # You can adjust this threshold as needed\n",
    "            first_zero_bin = bins[i]\n",
    "            break\n",
    "\n",
    "    plt.show()\n",
    "    return first_zero_bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "NYtl9xhSDYH9",
    "outputId": "58268420-f200-47df-fd96-ad86984f1250"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_moving_average(points_cluster_0_time_5, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ob6aEbPNDYH9"
   },
   "source": [
    "study other points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hcgpQ6eDYH9"
   },
   "outputs": [],
   "source": [
    "points_cluster_0_time_25 = df_all_new_updated[(df_all_new_updated['Time'] == 25) & (df_all_new_updated['Labels'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "ox9wjiJNDYH-",
    "outputId": "5751f022-7889-4da4-c33e-c84f20acfba0"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_moving_average(points_cluster_0_time_25, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oudl3HbADYH-"
   },
   "outputs": [],
   "source": [
    "points_cluster_0_time_50 = df_all_new_updated[(df_all_new_updated['Time'] == 50) & (df_all_new_updated['Labels'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "wBHCnU9vDYH-",
    "outputId": "7e3b12e3-72ee-4b7f-b5c5-0b1e9552b6ca"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_moving_average(points_cluster_0_time_50, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quL4BEmMDYH-"
   },
   "outputs": [],
   "source": [
    "points_cluster_0_time_75 = df_all_new_updated[(df_all_new_updated['Time'] == 75) & (df_all_new_updated['Labels'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "qeAhFtbxDYH-",
    "outputId": "32b96a6e-aa4b-4b2d-e6ad-56895f41455b"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_moving_average(points_cluster_0_time_75, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GrKzk5W0DYH-"
   },
   "outputs": [],
   "source": [
    "points_cluster_0_time_100 = df_all_new_updated[(df_all_new_updated['Time'] == 100) & (df_all_new_updated['Labels'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "8Lw1sncXDYH-",
    "outputId": "35b6ed2c-4e1d-443c-d198-2e849afb1b29"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_moving_average(points_cluster_0_time_100, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DwPQtV6DYH-"
   },
   "source": [
    "I can take all the values and calculate a distance based on all of them. I can see what happens with several points. The distance can be more covered in this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "tTkTXTC0DYH-",
    "outputId": "9061693d-3e94-4182-d97c-8301f397ea5e"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_moving_average_with_labels(df_all_new_updated, window_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "NhkPV8VUDYH-",
    "outputId": "db188c2c-8f60-4ebe-8be7-efbb409ba200"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_moving_average_with_labels(df_all_new_updated, window_size = 20, interruption_point = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beNrkyCcDYIB"
   },
   "source": [
    "Just a few of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "ba-UAPbJDYIB",
    "outputId": "5cf77fae-2975-43b9-81ca-f1b44fe13eee"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_moving_average_with_labels(df_all_new_updated, labels = [0], window_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "HNakcCMcDYIB",
    "outputId": "db75ba1c-5f0c-4643-ff1d-30d9a2517379"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_moving_average_with_labels(df_all_new_updated, labels = [1], window_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "ZDvrq4-LDYIB",
    "outputId": "86d63bf6-ce9f-48f6-98d6-2196608a0b3d"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_moving_average_with_labels(df_all_new_updated, labels = [2], window_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "bKO_AMX0DYIB",
    "outputId": "b3fd577c-ffcf-4b32-b0a8-109a79a60650"
   },
   "outputs": [],
   "source": [
    "plot_distance_vs_moving_average_with_labels(df_all_new_updated, labels = [3], window_size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfgbM6o3DYIB"
   },
   "source": [
    "After this analysis on the intensity, it may be useful to understand how the values are correlated. We can study the correlation considering single clusters or all the values taken together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQqTV6P8DYIB"
   },
   "source": [
    "###### Correlation with IntDen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tl9ADCVnDYIB"
   },
   "outputs": [],
   "source": [
    "correlation_df = pd.DataFrame.from_dict(correlations, orient='index', columns=['Correlation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABvzV2G-DYIB"
   },
   "outputs": [],
   "source": [
    "correlation_df.loc['Total'] = calculate_correlation(df_all_new_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "8Cq9W4lPDYIB",
    "outputId": "9251cbee-3d16-477c-a5e4-8a03733748de"
   },
   "outputs": [],
   "source": [
    "# Create a heat map\n",
    "sns.heatmap(correlation_df, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Correlations')\n",
    "plt.title('Correlation Heat Map')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rY44-Jv7DYIB"
   },
   "source": [
    "Another question that may be useful to answer is whether there is a difference between the area and the distance. That can define if it's possible to identify any correlation between the area and the distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y3W9ZGW4DYIB",
    "outputId": "a9ead9f6-1746-474b-8f0f-0d5fb46fd9ff"
   },
   "outputs": [],
   "source": [
    "area_correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otVK5Ym6DYIB"
   },
   "source": [
    "##### Additional Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "4qI3pXbiDYIB",
    "outputId": "2b2fef38-866c-4443-e306-dd60de680d12"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df_all_new_updated is your DataFrame\n",
    "data = df_all_new_updated['IntDen']\n",
    "\n",
    "# Create a histogram\n",
    "plt.hist(data, bins=10, color='blue', edgecolor='black')\n",
    "\n",
    "# Add labels and a title\n",
    "plt.xlabel('IntDen')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of IntDen')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkDTtCIvDYIB"
   },
   "outputs": [],
   "source": [
    "# Select specific columns from the DataFrame\n",
    "selected_columns = cluster_new_updated[['Area', 'Geo_light_intensity', 'IntDen', 'Mean', 'N_cell', 'Mean', 'Labels']]\n",
    "selected_columns2 = cluster_new_updated[['Area', 'Geo_light_intensity', 'IntDen', 'Mean', 'N_cell', 'Mean']]\n",
    "\n",
    "# If you want to create a new DataFrame with these columns, you can do so:\n",
    "clustered_df = selected_columns.copy()\n",
    "measurement_data = selected_columns2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "geAwRHbeDYIB"
   },
   "outputs": [],
   "source": [
    "clustered_df_L0 = clustered_df[clustered_df['Labels'] == 0]\n",
    "clustered_df_L1 = clustered_df[clustered_df['Labels'] == 1]\n",
    "clustered_df_L2 = clustered_df[clustered_df['Labels'] == 2]\n",
    "clustered_df_L3 = clustered_df[clustered_df['Labels'] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g6nTSLoADYIB"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "luminescence_pca = pca.fit_transform(clustered_df_L0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "k8DbNYSLDYIC",
    "outputId": "008a1c89-c2e2-4908-959c-e7deeada585f"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming you have already performed PCA on 'clustered_df_L0'\n",
    "pca = PCA()\n",
    "luminescence_pca = pca.fit_transform(clustered_df)\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(8, 6))  # Adjust the figure size as needed\n",
    "plt.scatter(luminescence_pca[:, 0], luminescence_pca[:, 1], alpha=0.5)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA Scatter Plot')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDhI99SrDYIC"
   },
   "outputs": [],
   "source": [
    "pip install scanpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "id": "_gf3PnVpDYIC",
    "outputId": "8f7fbbb2-1cca-4297-da1e-b590ed26e5ed"
   },
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "# Perform PCA with optional data normalization\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named measurement_data\n",
    "# Convert your DataFrame to an AnnData object\n",
    "adata = sc.AnnData(X=measurement_data.values)\n",
    "#sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "\n",
    "# Perform PCA with optional data normalization\n",
    "sc.tl.pca(adata)\n",
    "\n",
    "# Visualize PCA results\n",
    "sc.pl.pca(adata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "id": "7rtsI20_DYIC",
    "outputId": "5a73db6c-9589-49a5-8792-8708d6c4a500"
   },
   "outputs": [],
   "source": [
    "adata = sc.AnnData(X=measurement_data.values)\n",
    "\n",
    "# Perform PCA with optional data normalization\n",
    "sc.tl.pca(adata)\n",
    "\n",
    "# Visualize PCA results\n",
    "sc.pl.pca(adata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UUg0Je4xDYIC",
    "outputId": "22a02913-cd58-459a-e781-034191df1c9c"
   },
   "outputs": [],
   "source": [
    "!pip install leidenalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "rtsrPsa0DYIC",
    "outputId": "476b0fed-5465-48ea-b493-70c4c64aa602"
   },
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "# Assuming you have an AnnData object named 'adata'\n",
    "\n",
    "# Calculate neighbors\n",
    "sc.pp.neighbors(adata, n_pcs=6)\n",
    "\n",
    "# Perform clustering using the Leiden algorithm\n",
    "sc.tl.leiden(adata, resolution=1.0)  # You can adjust the resolution parameter\n",
    "\n",
    "# UMAP embedding\n",
    "sc.tl.umap(adata)\n",
    "\n",
    "# Visualize the UMAP plot with cluster coloring\n",
    "sc.pl.umap(adata, color='leiden')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FaKJrP1DDYIC"
   },
   "outputs": [],
   "source": [
    "pip install pysal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VE2IHnXtDYIC"
   },
   "source": [
    "###### Spatial Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E92-FjrcDYIC"
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import libpysal\n",
    "import esda\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select specific columns from the DataFrame\n",
    "selected_columns_points = df_all_new_updated[['X', 'Y', 'IntDen']]\n",
    "\n",
    "# Create a GeoDataFrame with Point geometries\n",
    "geometry = [Point(x, y) for x, y in zip(selected_columns_points['X'], selected_columns_points['Y'])]\n",
    "gdf = gpd.GeoDataFrame(selected_columns_points, geometry=geometry)\n",
    "\n",
    "# Create a spatial weights matrix (W) using Queen contiguity\n",
    "w = libpysal.weights.Queen.from_dataframe(gdf)\n",
    "\n",
    "# Extract the 'IntDen' column for Moran's I analysis\n",
    "y = selected_columns_points['IntDen']\n",
    "\n",
    "# Perform Moran's I analysis\n",
    "moran = esda.Moran(y, w)\n",
    "\n",
    "# Access Moran's I results\n",
    "moran_I = moran.I\n",
    "moran_EI = moran.EI\n",
    "moran_p_value = moran.p_sim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 856
    },
    "id": "S-ri345rDYIC",
    "outputId": "8c601f41-d21e-4cd3-e376-89a82a817c3a"
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from splot.esda import plot_moran\n",
    "\n",
    "# Plot Moran scatterplot\n",
    "plot_moran(moran, zstandard=True, figsize=(10, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QiP9qur7DYIC",
    "outputId": "a35974da-1545-4389-f49b-f4ffd7de7a7a"
   },
   "outputs": [],
   "source": [
    "!pip install pykrige"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "h3npB1qRDYIC",
    "outputId": "62e3da60-2d94-4979-b6d6-07bd723abc58"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Generate example data\n",
    "n_points = len(df1)\n",
    "X = df1['X']  # X coordinates\n",
    "Y = df1['Y']  # Y coordinates\n",
    "temperature = df1['IntDen'].values  # Temperature values\n",
    "\n",
    "# Create a grid for prediction\n",
    "x_grid, y_grid = np.meshgrid(np.linspace(0, 20, 150), np.linspace(0, 20, 150))\n",
    "\n",
    "# Define the IDW interpolation function\n",
    "def idw_interpolation(x, y, X, Y, values, power=2):\n",
    "    distances = distance.cdist(np.column_stack((x.flatten(), y.flatten())), np.column_stack((X, Y)))\n",
    "    weights = 1.0 / (distances**power)\n",
    "    weighted_values = values * weights\n",
    "    interpolated_values = np.sum(weighted_values, axis=1) / np.sum(weights, axis=1)\n",
    "    return interpolated_values\n",
    "\n",
    "# Perform the IDW interpolation\n",
    "z_grid = idw_interpolation(x_grid, y_grid, X, Y, temperature)\n",
    "\n",
    "# Reshape the grid and predicted values\n",
    "z_grid = z_grid.reshape(x_grid.shape)\n",
    "\n",
    "# Create a contour plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(x_grid, y_grid, z_grid, levels=100, cmap=\"viridis\")\n",
    "plt.colorbar(label=\"IntDen\")\n",
    "plt.scatter(X, Y, c=temperature, cmap=\"coolwarm\", edgecolors=\"k\", s=100)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"IntDen Interpolation (IDW)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "F5pjKPbYDYIC",
    "outputId": "36e36b15-efa4-4a4d-97ab-bf22ca2c2742"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Generate example data\n",
    "n_points = len(df2)\n",
    "X = df2['X']  # X coordinates\n",
    "Y = df2['Y']  # Y coordinates\n",
    "temperature = df2['IntDen'].values  # Temperature values\n",
    "\n",
    "# Create a grid for prediction\n",
    "x_grid, y_grid = np.meshgrid(np.linspace(0, 20, 150), np.linspace(0, 20, 150))\n",
    "\n",
    "# Define the IDW interpolation function\n",
    "def idw_interpolation(x, y, X, Y, values, power=2):\n",
    "    distances = distance.cdist(np.column_stack((x.flatten(), y.flatten())), np.column_stack((X, Y)))\n",
    "    weights = 1.0 / (distances**power)\n",
    "    weighted_values = values * weights\n",
    "    interpolated_values = np.sum(weighted_values, axis=1) / np.sum(weights, axis=1)\n",
    "    return interpolated_values\n",
    "\n",
    "# Perform the IDW interpolation\n",
    "z_grid = idw_interpolation(x_grid, y_grid, X, Y, temperature)\n",
    "\n",
    "# Reshape the grid and predicted values\n",
    "z_grid = z_grid.reshape(x_grid.shape)\n",
    "\n",
    "# Create a contour plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(x_grid, y_grid, z_grid, levels=100, cmap=\"viridis\")\n",
    "plt.colorbar(label=\"IntDen\")\n",
    "plt.scatter(X, Y, c=temperature, cmap=\"coolwarm\", edgecolors=\"k\", s=100)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"IntDen Interpolation (IDW)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5OBB3Al1DYIC",
    "outputId": "8b1d282b-22b4-436a-f586-e372c910425b"
   },
   "outputs": [],
   "source": [
    "import spaghetti\n",
    "import esda\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "import libpysal\n",
    "\n",
    "# Sample dataset with X, Y, and IntDen columns\n",
    "# Example data (replace with your actual data):\n",
    "# df1 = pd.DataFrame({\n",
    "#     'X': [1, 2, 3, 4, 5],\n",
    "#     'Y': [2, 3, 4, 5, 6],\n",
    "#     'IntDen': [10, 15, 20, 25, 30]\n",
    "# })\n",
    "\n",
    "# Create a GeoDataFrame from your DataFrame with Point geometries\n",
    "geometry = [Point(xy) for xy in zip(df1['X'], df1['Y'])]\n",
    "gdf = gpd.GeoDataFrame(df1, geometry=geometry)\n",
    "\n",
    "# Create a spatial weights matrix (k-nearest neighbors with k=3, for example)\n",
    "w = libpysal.weights.KNN.from_dataframe(gdf, k=3)\n",
    "\n",
    "# Calculate Geary's C for spatial autocorrelation\n",
    "gearys_c = esda.geary.Geary(gdf['IntDen'], w)\n",
    "\n",
    "# Access Geary's C statistic and p-value\n",
    "gearys_c_statistic = gearys_c.C\n",
    "p_value = gearys_c.p_sim\n",
    "\n",
    "print(\"Geary's C:\", gearys_c_statistic)\n",
    "print(\"p-value:\", p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 647
    },
    "id": "oKfDalcuDYIC",
    "outputId": "86f0bb0c-685c-4866-b932-d4adea81c5d9"
   },
   "outputs": [],
   "source": [
    "import spaghetti\n",
    "import esda\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "import libpysal\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Sample dataset with X, Y, and IntDen columns\n",
    "# Example data (replace with your actual data):\n",
    "# df1 = pd.DataFrame({\n",
    "#     'X': [1, 2, 3, 4, 5],\n",
    "#     'Y': [2, 3, 4, 5, 6],\n",
    "#     'IntDen': [10, 15, 20, 25, 30]\n",
    "# })\n",
    "\n",
    "# Create a GeoDataFrame from your DataFrame with Point geometries\n",
    "geometry = [Point(xy) for xy in zip(df1['X'], df1['Y'])]\n",
    "gdf = gpd.GeoDataFrame(df1, geometry=geometry)\n",
    "\n",
    "# Create a spatial weights matrix (k-nearest neighbors with k=3, for example)\n",
    "w = libpysal.weights.KNN.from_dataframe(gdf, k=3)\n",
    "\n",
    "# Suppress the warning about disconnected components\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Calculate Geary's C for spatial autocorrelation\n",
    "gearys_c = esda.geary.Geary(gdf['IntDen'], w)\n",
    "\n",
    "# Access Geary's C statistic and p-value\n",
    "gearys_c_statistic = gearys_c.C\n",
    "p_value = gearys_c.p_sim\n",
    "\n",
    "# Create a Moran scatterplot\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Calculate the spatial lag of IntDen\n",
    "lag_intden = libpysal.weights.lag_spatial(w, gdf['IntDen'])\n",
    "\n",
    "# Scatterplot\n",
    "plt.scatter(gdf['IntDen'], lag_intden, color='b', alpha=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"IntDen\")\n",
    "plt.ylabel(\"Spatial Lag of IntDen\")\n",
    "plt.title(\"Moran Scatterplot (Geary's C)\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlCeNb8kDYIC"
   },
   "source": [
    "##### Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "HoY3X2JpDYIC",
    "outputId": "56252cf1-1fed-451b-9069-1bda0ab89195"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define parameters\n",
    "initial_population = selected_clusters[(selected_clusters['Labels'] == 0) & (selected_clusters['Time'] == 92)]['N_cell'].values[0]  # Initial population or value\n",
    "decay_rate = 1.000000001603458         # Rate at which the population decays\n",
    "simulation_time = 26    # Total simulation time\n",
    "\n",
    "# Initialize time and population arrays\n",
    "time_points = [92]  # Initialize with the starting time of 92\n",
    "population_values = [initial_population]\n",
    "\n",
    "# Gillespie algorithm\n",
    "while time_points[-1] < 92 + simulation_time and population_values[-1] > 0:\n",
    "    # Calculate the decay rate (event rate)\n",
    "    event_rate = decay_rate * population_values[-1]\n",
    "\n",
    "    # Generate a random time for the next event based on exponential distribution\n",
    "    time_to_next_event = np.random.exponential(1 / event_rate)\n",
    "\n",
    "    # Update time and population values\n",
    "    time_points.append(time_points[-1] + time_to_next_event)\n",
    "    population_values.append(population_values[-1] - 1)\n",
    "\n",
    "# Plot the simulation results\n",
    "plt.plot(time_points, population_values)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Population')\n",
    "plt.title('Gillespie Algorithm Simulation')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9nMH9fADYIC"
   },
   "source": [
    "The result is not super convincing. Because we know from our perspective how the value may change over time. It's still higher than 0 at least until 118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MqiXZ5JfDYID"
   },
   "outputs": [],
   "source": [
    "N_t1 = selected_clusters[(selected_clusters['Labels'] == 0) & (selected_clusters['Time'] == 92)]['N_cell'].values[0]\n",
    "N_t2 = 1\n",
    "decay_rate = math.log(N_t1 / N_t2) / (118 - 92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AzMBkndCDYID",
    "outputId": "495a4bef-d46a-4d62-89f9-09046841cfcd"
   },
   "outputs": [],
   "source": [
    "decay_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "xjzYBrecDYID",
    "outputId": "b61f7ead-5b82-4e4f-e2ba-32930ef6ddf0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define parameters\n",
    "initial_population = selected_clusters[(selected_clusters['Labels'] == 0) & (selected_clusters['Time'] == 92)]['N_cell'].values[0]  # Initial population or value\n",
    "decay_rate = decay_rate         # Rate at which the population decays\n",
    "simulation_time = 26    # Total simulation time\n",
    "\n",
    "# Initialize time and population arrays\n",
    "time_points = [92]  # Initialize with the starting time of 92\n",
    "population_values = [initial_population]\n",
    "\n",
    "# Gillespie algorithm\n",
    "while time_points[-1] < 92 + simulation_time and population_values[-1] > 0:\n",
    "    # Calculate the decay rate (event rate)\n",
    "    event_rate = decay_rate * population_values[-1]\n",
    "\n",
    "    # Generate a random time for the next event based on exponential distribution\n",
    "    time_to_next_event = np.random.exponential(1 / event_rate)\n",
    "\n",
    "    # Update time and population values\n",
    "    time_points.append(time_points[-1] + time_to_next_event)\n",
    "    population_values.append(population_values[-1] - 1)\n",
    "\n",
    "# Plot the simulation results\n",
    "plt.plot(time_points, population_values)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Population')\n",
    "plt.title('Gillespie Algorithm Simulation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xPfY29y0DYID",
    "outputId": "d526edd3-b440-4e68-dfd4-ed52987e861c"
   },
   "outputs": [],
   "source": [
    "float(decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 536
    },
    "id": "dNs_gErtDYID",
    "outputId": "7451b4be-a731-4df6-c541-db1355edb37e"
   },
   "outputs": [],
   "source": [
    "from cayenne.simulation import Simulation\n",
    "model_str = \"\"\"\n",
    "        const compartment comp1;\n",
    "        comp1 = 1.0; # volume of compartment\n",
    "\n",
    "        r1: N_cell => B; k1;\n",
    "\n",
    "        k1 = 0.1000001603458;\n",
    "        chem_flag = false;\n",
    "\n",
    "        N_cell = 300;\n",
    "        B = 0;\n",
    "    \"\"\"\n",
    "sim = Simulation.load_model(model_str, \"ModelString\")\n",
    "# Run the simulation\n",
    "sim.simulate(max_t=26, max_iter=1000, n_rep=5, algorithm=\"tau_adaptive\")\n",
    "sim.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 536
    },
    "id": "_XRo9bO5DYID",
    "outputId": "bcbd62aa-71fe-45a9-9fdf-c45b8350e94b"
   },
   "outputs": [],
   "source": [
    "sim.plot(species_names = [\"N_cell\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GD2a8-wlw5a"
   },
   "source": [
    "#### Adjust clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3CggyNadykLu"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nIqxgs45ycXJ"
   },
   "outputs": [],
   "source": [
    "create_inline_plots(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5TTgg4qwS6T"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "import pandas as pd\n",
    "\n",
    "def create_inline_plots(data):\n",
    "    # Iterate over each timepoint in your DataFrame\n",
    "    for timepoint in data['Time'].unique():\n",
    "        # Create a sub-dataframe for the current timepoint\n",
    "        sub_data = data[data['Time'] == timepoint]\n",
    "\n",
    "        # Call your plotting function with the sub-dataframe and get the figure object\n",
    "        fig = plotPointsFromCSV_same_colour(sub_data, title=f\"Timepoint {timepoint}\")\n",
    "\n",
    "        # Display the current figure inline in Google Colab\n",
    "        display(fig)\n",
    "\n",
    "        # Close the current figure to free up memory\n",
    "        plt.close(fig)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a DataFrame 'your_data' with columns 'Time' and other data columns\n",
    "# create_inline_plots(your_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pd4tabtYLRrz"
   },
   "outputs": [],
   "source": [
    "a = df_all_new_updated[(df_all_new_updated['Labels'] == 1) & (df_all_new_updated['Time'] == 2)]\n",
    "b = df_all_new_updated[(df_all_new_updated['Labels'] == 0) & (df_all_new_updated['Time'] == 2)]\n",
    "c = df_all_new_updated[(df_all_new_updated['Labels'] == 2) & (df_all_new_updated['Time'] == 2)]\n",
    "d = df_all_new_updated[(df_all_new_updated['Labels'] == 3) & (df_all_new_updated['Time'] == 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 986
    },
    "id": "8Jsr0Y_jL2tR",
    "outputId": "7860aa24-343b-4771-975e-06f67c1a8ecd"
   },
   "outputs": [],
   "source": [
    "plot_values_based_on_the_area_in_space(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 986
    },
    "id": "Qwfy9n4CL1ef",
    "outputId": "9e7e7fed-4de6-4f08-cb54-864f93cb0674"
   },
   "outputs": [],
   "source": [
    "plot_values_based_on_the_area_in_space(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 986
    },
    "id": "HA21X7cWLym8",
    "outputId": "95c0b33a-a34c-47ab-c32e-b80cdbeaf326"
   },
   "outputs": [],
   "source": [
    "plot_values_based_on_the_area_in_space(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 879
    },
    "id": "4XtTkrQZLN2x",
    "outputId": "0da99c54-fad6-4ab2-c29c-430f3db21668"
   },
   "outputs": [],
   "source": [
    "plot_values_based_on_the_area_in_space(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "gyrN37cnG9PA",
    "outputId": "779b01fe-70f1-4b8f-db15-70f5d57e1870"
   },
   "outputs": [],
   "source": [
    "removed = remove_values_based_on_the_area_in_space(df_adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BRDu3M_ru9CI"
   },
   "outputs": [],
   "source": [
    "removed = remove_values_after_zerobin(df_adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RIVHT6RhuNY3",
    "outputId": "13bc73a2-acc7-4032-8698-aae294dd664c"
   },
   "outputs": [],
   "source": [
    "len(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AtEvCwh1QOYJ",
    "outputId": "36807ffb-b84b-454f-c446-68a8796c0f97"
   },
   "outputs": [],
   "source": [
    "print(len(df_all_imp), len(cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "9oN7TwysPEdS",
    "outputId": "52653bb1-f36a-41d4-9b79-3e0be3147075"
   },
   "outputs": [],
   "source": [
    "cleaned = remove_values_after_zerobin(df_all_imp, T_max=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_WZP-G2bAuP1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def remove_values_after_zerobin(data, label_column='Labels', d_value=7.5, min_bin_value=100, T_max=None):\n",
    "    # Create an empty DataFrame to store the filtered data\n",
    "    filtered_data = pd.DataFrame()\n",
    "\n",
    "    # Create a subset of timepoints to consider, up to T_max if specified\n",
    "    if T_max is not None:\n",
    "        data_until_T_max = data[data['Time'] <= T_max]\n",
    "        timepoints_to_process = data_until_T_max['Time'].unique()\n",
    "    else:\n",
    "        timepoints_to_process = data['Time'].unique()\n",
    "\n",
    "    # Iterate over unique timepoints in the 'Time' column\n",
    "    for timepoint in timepoints_to_process:\n",
    "        # Get the data for the current timepoint\n",
    "        timepoint_data = data[data['Time'] == timepoint]\n",
    "\n",
    "        # Iterate over unique labels in the 'label_column' within the current timepoint\n",
    "        for label in timepoint_data[label_column].unique():\n",
    "            # Get the data for the current label within the current timepoint\n",
    "            label_data = timepoint_data[timepoint_data[label_column] == label]\n",
    "\n",
    "            # Calculate the barycenter (centroid) for the current label's data\n",
    "            Barycenter_x = label_data['X'].mean()\n",
    "            Barycenter_y = label_data['Y'].mean()\n",
    "\n",
    "            # Calculate the distance for each data point in the current label's data\n",
    "            label_data['Distance'] = label_data.apply(lambda row: math.sqrt((row['X'] - Barycenter_x)**2 + (row['Y'] - Barycenter_y)**2), axis=1)\n",
    "\n",
    "            # Create a histogram of distances with custom ticks for the current label's data\n",
    "            custom_ticks = int(max(label_data['Distance']) / d_value)\n",
    "            n, bins, patches = plt.hist(label_data['Distance'], bins=custom_ticks, color='b', alpha=0.7)\n",
    "\n",
    "            # Find the first bin with a count of zero or close to zero and with a value of at least min_bin_value\n",
    "            first_zero_bin = None\n",
    "            for i, count in enumerate(n):\n",
    "                if count <= 1e-6 and bins[i] >= min_bin_value:  # Adjust threshold and min_bin_value as needed\n",
    "                    first_zero_bin = bins[i]\n",
    "                    break\n",
    "\n",
    "            # If there's no suitable first_zero_bin, append the original label_data to the filtered_data\n",
    "            if first_zero_bin is None:\n",
    "                filtered_data = pd.concat([filtered_data, label_data], ignore_index=True)\n",
    "            else:\n",
    "                # Filter the current label's data to keep only rows with 'Distance' less than or equal to first_zero_bin\n",
    "                label_data_filtered = label_data[label_data['Distance'] <= first_zero_bin]\n",
    "                filtered_data = pd.concat([filtered_data, label_data_filtered], ignore_index=True)\n",
    "\n",
    "    if T_max is not None:\n",
    "        data_after_T_max = data[data['Time'] > T_max]\n",
    "        filtered_data = pd.concat([filtered_data, data_after_T_max], ignore_index=True)\n",
    "    return filtered_data\n",
    "# Disable the SettingWithCopyWarning for chained assignments\n",
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fw4kv2PdG2Pf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def remove_values_based_on_the_area_in_space(data, d_value=7.5, threshold=0.1, T_max=None):\n",
    "    # Create an empty DataFrame to store the filtered data\n",
    "    filtered_data = pd.DataFrame()\n",
    "\n",
    "    # Create a subset of timepoints to consider, up to T_max if specified\n",
    "    if T_max is not None:\n",
    "        data_until_T_max = data[data['Time'] <= T_max]\n",
    "        timepoints_to_process = data_until_T_max['Time'].unique()\n",
    "    else:\n",
    "        timepoints_to_process = data['Time'].unique()\n",
    "\n",
    "    # Iterate over unique timepoints in the 'Time' column\n",
    "    for timepoint in timepoints_to_process:\n",
    "        # Get the data for the current timepoint\n",
    "        timepoint_data = data_until_T_max[data_until_T_max['Time'] == timepoint]\n",
    "\n",
    "        # Calculate the barycenter (centroid)\n",
    "        Barycenter_x = timepoint_data['X'].mean()\n",
    "        Barycenter_y = timepoint_data['Y'].mean()\n",
    "\n",
    "        # Calculate the distance for each data point\n",
    "        timepoint_data['Distance'] = ((timepoint_data['X'] - Barycenter_x)**2 + (timepoint_data['Y'] - Barycenter_y)**2).apply(math.sqrt)\n",
    "\n",
    "        # Sort the DataFrame by distance for a cleaner plot\n",
    "        timepoint_data = timepoint_data.sort_values('Distance')\n",
    "\n",
    "        # Assign labels based on distance\n",
    "        timepoint_data['Label'] = timepoint_data['Distance'].apply(lambda distance: math.ceil(distance / d_value))\n",
    "        timepoint_data['New_Distance'] = timepoint_data['Label']*d_value - d_value / 2\n",
    "\n",
    "        # Calculate area and sum of IntDen\n",
    "        timepoint_data['Area_space'] = math.pi * (timepoint_data['Label']**2 - (timepoint_data['Label'] - 1)**2)\n",
    "        timepoint_data['Sum_IntDen'] = timepoint_data.groupby('Label')['IntDen'].transform('sum')\n",
    "        timepoint_data['Ratio'] = timepoint_data['Sum_IntDen'] / timepoint_data['Area_space']\n",
    "\n",
    "        # Filter based on the threshold\n",
    "        timepoint_data = timepoint_data[timepoint_data['Ratio'] > threshold]\n",
    "\n",
    "        # Append the filtered data for this timepoint to the overall filtered_data\n",
    "        filtered_data = pd.concat([filtered_data, timepoint_data], ignore_index=True)\n",
    "\n",
    "    if T_max is not None:\n",
    "        data_after_T_max = data[data['Time'] > T_max]\n",
    "        filtered_data = pd.concat([filtered_data, data_after_T_max], ignore_index=True)\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "# Disable the SettingWithCopyWarning for chained assignments\n",
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWC0VCpgE02s"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def remove_values_based_on_the_area_in_space(data, d_value=7.5, threshold=0.9):\n",
    "    # Calculate the barycenter (centroid)\n",
    "    Barycenter_x = data['X'].mean()\n",
    "    Barycenter_y = data['Y'].mean()\n",
    "\n",
    "    # Calculate the distance for each data point\n",
    "    data['Distance'] = ((data['X'] - Barycenter_x)**2 + (data['Y'] - Barycenter_y)**2).apply(math.sqrt)\n",
    "\n",
    "    # Sort the DataFrame by distance for a cleaner plot\n",
    "    data = data.sort_values('Distance')\n",
    "\n",
    "    # Assign labels based on distance\n",
    "    data['Label'] = data['Distance'].apply(lambda distance: math.ceil(distance / d_value))\n",
    "    data['New_Distance'] = data['Label']*d_value - d_value / 2\n",
    "\n",
    "    # Calculate area and sum of IntDen\n",
    "    data['Area_space'] = math.pi * (data['Label']**2 - (data['Label'] - 1)**2)\n",
    "    data['Sum_IntDen'] = data.groupby('Label')['IntDen'].transform('sum')\n",
    "    data['Ratio'] = data['Sum_IntDen'] / data['Area_space']\n",
    "\n",
    "    plt.plot(data['New_Distance'], data['Ratio'])\n",
    "    plt.xlabel('New Distance')\n",
    "    plt.ylabel('Ratio')\n",
    "    plt.title('Ratio vs New Distance')\n",
    "    plt.show()\n",
    "\n",
    "    data = data[data['Ratio'] > threshold]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GzOjGGYHJA8h"
   },
   "outputs": [],
   "source": [
    "df_adjusted = df_adjusted[(df_adjusted['Time'] == 1) & (df_adjusted['Labels'] == 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jq08TI5nIuvg"
   },
   "outputs": [],
   "source": [
    "plot_values_based_on_the_area_in_space(df_adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZURBlKmIl0B6"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_values_based_on_the_area_in_space(data, d_value=7.5):\n",
    "    # Calculate the barycenter (centroid)\n",
    "    Barycenter_x = data['X'].mean()\n",
    "    Barycenter_y = data['Y'].mean()\n",
    "\n",
    "    # Calculate the distance for each data point\n",
    "    data['Distance'] = ((data['X'] - Barycenter_x)**2 + (data['Y'] - Barycenter_y)**2).apply(math.sqrt)\n",
    "\n",
    "    # Create a new DataFrame for plotting\n",
    "    plot_df = data[['Distance', 'IntDen']]\n",
    "\n",
    "    # Sort the DataFrame by distance for a cleaner plot\n",
    "    plot_df = plot_df.sort_values('Distance')\n",
    "\n",
    "    # Assign labels based on distance\n",
    "    plot_df['Label'] = plot_df['Distance'].apply(lambda distance: math.ceil(distance / d_value))\n",
    "    plot_df['New_Distance'] = plot_df['Label']*d_value - d_value / 2\n",
    "\n",
    "    # Calculate area and sum of IntDen\n",
    "    plot_df['Area'] = math.pi * (plot_df['Label']**2 - (plot_df['Label'] - 1)**2)\n",
    "    plot_df['Sum_IntDen'] = plot_df.groupby('Label')['IntDen'].transform('sum')\n",
    "    plot_df['Ratio'] = plot_df['Sum_IntDen'] / plot_df['Area']\n",
    "\n",
    "    # Plotting the function\n",
    "    plt.plot(plot_df['New_Distance'], plot_df['Ratio'])\n",
    "    plt.xlabel('New Distance')\n",
    "    plt.ylabel('Ratio')\n",
    "    plt.title('Ratio vs New Distance')\n",
    "    plt.show()\n",
    "\n",
    "    return plot_df\n",
    "\n",
    "# Example usage:\n",
    "# plot_values_based_on_the_area_in_space(selected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOHb-Id2TG9_"
   },
   "outputs": [],
   "source": [
    "def plot_see_watch(data, folder_path):\n",
    "  output_folder_png = folder_path\n",
    "  create_png_plots(data, output_folder_png)\n",
    "  gdrive_folder = output_folder_png\n",
    "  output_pdf_name = gdrive_folder + '/new_model.pdf'\n",
    "  result_pdf_path = merge_pngs_to_pdf(gdrive_folder, output_pdf_name)\n",
    "  folder_id = gdrive_folder\n",
    "  output_video_path = gdrive_folder + '/video.mp4'\n",
    "  create_video_from_pngs(folder_id,output_video_path)\n",
    "  return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYb9gDT4B1rU"
   },
   "source": [
    "###### Used functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NIneMl6lznU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_distance_histogram_with_custom_ticks(data, d_value=7.5):\n",
    "    df = data.copy()\n",
    "\n",
    "    # Calculate the barycenter (centroid)\n",
    "    Barycenter_x = df['X'].mean()\n",
    "    Barycenter_y = df['Y'].mean()\n",
    "\n",
    "    # Calculate the distance for each data point\n",
    "    df['Distance'] = df.apply(lambda row: math.sqrt((row['X'] - Barycenter_x)**2 + (row['Y'] - Barycenter_y)**2), axis=1)\n",
    "    custom_ticks = int(max(df['Distance']) / d_value)\n",
    "    # Create a histogram of distances with custom ticks\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    n, bins, patches = plt.hist(df['Distance'], bins=custom_ticks, color='b', alpha=0.7)\n",
    "    plt.xlabel('Distance from Barycenter')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Cell Distances from Barycenter')\n",
    "\n",
    "    # Find the first bin with a count of zero or close to zero\n",
    "    first_zero_bin = None\n",
    "    for i, count in enumerate(n):\n",
    "        if count <= 1e-6:  # You can adjust this threshold as needed\n",
    "            first_zero_bin = bins[i]\n",
    "            break\n",
    "\n",
    "    plt.show()\n",
    "    return first_zero_bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_AwnSukHENIu"
   },
   "outputs": [],
   "source": [
    "def remove_values_after_zerobin(data, label_column='Labels', d_value=7.5, min_bin_value=100):\n",
    "    # Create an empty DataFrame to store the filtered data\n",
    "    filtered_data = pd.DataFrame()\n",
    "\n",
    "    # Iterate over unique labels in the 'label_column'\n",
    "    for label in data[label_column].unique():\n",
    "        # Get the data for the current label\n",
    "        label_data = data[data[label_column] == label]\n",
    "\n",
    "        # Calculate the barycenter (centroid) for the current label's data\n",
    "        Barycenter_x = label_data['X'].mean()\n",
    "        Barycenter_y = label_data['Y'].mean()\n",
    "\n",
    "        # Calculate the distance for each data point in the current label's data\n",
    "        label_data['Distance'] = label_data.apply(lambda row: math.sqrt((row['X'] - Barycenter_x)**2 + (row['Y'] - Barycenter_y)**2), axis=1)\n",
    "\n",
    "        # Create a histogram of distances with custom ticks for the current label's data\n",
    "        custom_ticks = int(max(label_data['Distance']) / d_value)\n",
    "        n, bins, patches = plt.hist(label_data['Distance'], bins=custom_ticks, color='b', alpha=0.7)\n",
    "\n",
    "        # Find the first bin with a count of zero or close to zero and with a value of at least min_bin_value\n",
    "        first_zero_bin = None\n",
    "        for i, count in enumerate(n):\n",
    "            if count <= 1e-6 and bins[i] >= min_bin_value:  # Adjust threshold and min_bin_value as needed\n",
    "                first_zero_bin = bins[i]\n",
    "                break\n",
    "\n",
    "        # If there's no suitable first_zero_bin, append the original label_data to the filtered_data\n",
    "        if first_zero_bin is None:\n",
    "            filtered_data = pd.concat([filtered_data, label_data], ignore_index=True)\n",
    "        else:\n",
    "            # Filter the current label's data to keep only rows with 'Distance' less than or equal to first_zero_bin\n",
    "            label_data_filtered = label_data[label_data['Distance'] <= first_zero_bin]\n",
    "            filtered_data = pd.concat([filtered_data, label_data_filtered], ignore_index=True)\n",
    "\n",
    "    return filtered_data\n",
    "# Disable the SettingWithCopyWarning for chained assignments\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def remove_values_after_zerobin(data, label_column='Labels', d_value=7.5, min_bin_value=100):\n",
    "    # Create an empty DataFrame to store the filtered data\n",
    "    filtered_data = pd.DataFrame()\n",
    "\n",
    "    # Iterate over unique labels in the 'label_column'\n",
    "    for label in data[label_column].unique():\n",
    "        # Get the data for the current label\n",
    "        label_data = data[data[label_column] == label]\n",
    "\n",
    "        # Calculate the barycenter (centroid) for the current label's data\n",
    "        Barycenter_x = label_data['X'].mean()\n",
    "        Barycenter_y = label_data['Y'].mean()\n",
    "\n",
    "        # Calculate the distance for each data point in the current label's data\n",
    "        label_data['Distance'] = label_data.apply(lambda row: math.sqrt((row['X'] - Barycenter_x)**2 + (row['Y'] - Barycenter_y)**2), axis=1)\n",
    "\n",
    "        # Create a histogram of distances with custom ticks for the current label's data\n",
    "        custom_ticks = int(max(label_data['Distance']) / d_value)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        n, bins, patches = plt.hist(label_data['Distance'], bins=custom_ticks, color='b', alpha=0.7)\n",
    "\n",
    "        # Find the first bin with a count of zero or close to zero and with a value of at least min_bin_value\n",
    "        first_zero_bin = None\n",
    "        for i, count in enumerate(n):\n",
    "            if count <= 1e-6 and bins[i] >= min_bin_value:  # Adjust threshold and min_bin_value as needed\n",
    "                first_zero_bin = bins[i]\n",
    "                break\n",
    "\n",
    "        # If there's no suitable first_zero_bin, append the original label_data to the filtered_data\n",
    "        if first_zero_bin is None:\n",
    "            filtered_data = pd.concat([filtered_data, label_data], ignore_index=True)\n",
    "        else:\n",
    "            # Filter the current label's data to keep only rows with 'Distance' less than or equal to first_zero_bin\n",
    "            label_data_filtered = label_data[label_data['Distance'] <= first_zero_bin]\n",
    "            filtered_data = pd.concat([filtered_data, label_data_filtered], ignore_index=True)\n",
    "\n",
    "    return filtered_data\n",
    "# Disable the SettingWithCopyWarning for chained assignments\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a DataFrame 'your_data' with columns 'X', 'Y', and 'Label'\n",
    "# filtered_data = remove_values_after_zerobin(your_data)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def remove_values_after_zerobin(data, d_value=7.5, min_bin_value=100):\n",
    "    df = data.copy()\n",
    "\n",
    "    # Calculate the barycenter (centroid)\n",
    "    Barycenter_x = df['X'].mean()\n",
    "    Barycenter_y = df['Y'].mean()\n",
    "\n",
    "    # Calculate the distance for each data point\n",
    "    df['Distance'] = df.apply(lambda row: math.sqrt((row['X'] - Barycenter_x)**2 + (row['Y'] - Barycenter_y)**2), axis=1)\n",
    "    custom_ticks = int(max(df['Distance']) / d_value)\n",
    "    # Create a histogram of distances with custom ticks\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    n, bins, patches = plt.hist(df['Distance'], bins=custom_ticks, color='b', alpha=0.7)\n",
    "\n",
    "    # Find the first bin with a count of zero or close to zero and with a value of at least min_bin_value\n",
    "    first_zero_bin = None\n",
    "    for i, count in enumerate(n):\n",
    "        if count <= 1e-6 and bins[i] >= min_bin_value:  # Adjust threshold and min_bin_value as needed\n",
    "            first_zero_bin = bins[i]\n",
    "            break\n",
    "\n",
    "    # If there's no suitable first_zero_bin, return the original DataFrame\n",
    "    if first_zero_bin is None:\n",
    "        return df\n",
    "\n",
    "    # Filter the DataFrame to keep only rows with 'Distance' less than or equal to first_zero_bin\n",
    "    df_filtered = df[df['Distance'] <= first_zero_bin]\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a DataFrame 'your_data' with columns 'X' and 'Y'\n",
    "# filtered_data = remove_values_further_zerobin(your_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXNnrQLAyP4L"
   },
   "source": [
    "#### Data Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLTCs9vjGI2z"
   },
   "source": [
    "##### Functions used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hI_LD8x1OhV5"
   },
   "outputs": [],
   "source": [
    "cluster_tmp_edit, df_all_imp =  impute_cells_and_cluster(cluster_tmp_edit, df_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3VUjKNNsJnK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NKSwZUJrH-C_"
   },
   "outputs": [],
   "source": [
    "df_adjusted = df_adjusted[df_adjusted['Time'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnLWO9g2HNr4"
   },
   "outputs": [],
   "source": [
    "df_adjusted = df_all_new_updated.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNfFLSieIeys"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpDKhtufBa3W"
   },
   "outputs": [],
   "source": [
    " cluster_new_updated = pd.read_csv('/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/New_algorithm/cluster_new_updated_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08RYanhaBz8C"
   },
   "outputs": [],
   "source": [
    "df_all_new_updated = pd.read_csv('/content/drive/MyDrive/Academic Work/Cellgroup/CSV_files/New_algorithm/df_all_new_updated_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KkPvhezeyR6h"
   },
   "outputs": [],
   "source": [
    "cluster_tmp = cluster_new_updated.copy()\n",
    "df_tmp = df_all_new_updated.copy()\n",
    "cluster_tmp_edit = cluster_new_updated.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEhLLIMcD5aH"
   },
   "source": [
    "it's mostly to have a try and to understand what may happen in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bmnYMvrSGjVh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def fill_missing_values_in_time_series(cluster_tmp_edit):\n",
    "    # Find all unique labels in the 'Labels' column\n",
    "    unique_labels = cluster_tmp_edit['Labels'].unique()\n",
    "\n",
    "    # Create an empty DataFrame to store missing data\n",
    "    missing_data = pd.DataFrame(columns=['Labels', 'Time'])\n",
    "\n",
    "    # Iterate through unique labels\n",
    "    for label in unique_labels:\n",
    "        # Find the minimum 'Time' value in the 'Time' column for the current label\n",
    "        start_time = cluster_tmp_edit[cluster_tmp_edit['Labels'] == label]['Time'].min()\n",
    "\n",
    "        # Study the desired cluster for the current label\n",
    "        desired_cluster = cluster_tmp_edit[cluster_tmp_edit['Labels'] == label]\n",
    "\n",
    "        # Create a list of numbers starting from the 'start_time'\n",
    "        max_value = desired_cluster['Time'].max()\n",
    "        all_numbers = list(range(int(start_time), int(max_value) + 1))\n",
    "\n",
    "        # Find the unique values in the 'Time' column for the current cluster\n",
    "        unique_values = desired_cluster['Time'].unique()\n",
    "\n",
    "        # Find the numbers that are not present in the column for the current cluster\n",
    "        missing_numbers = list(set(all_numbers) - set(unique_values))\n",
    "\n",
    "        # Create a DataFrame with missing values for the current cluster\n",
    "        missing_cluster_data = pd.DataFrame({'Labels': [label] * len(missing_numbers), 'Time': missing_numbers})\n",
    "\n",
    "        # Append the missing_cluster_data DataFrame to the overall missing_data\n",
    "        missing_data = pd.concat([missing_data, missing_cluster_data], ignore_index=True)\n",
    "\n",
    "    # Convert 'Labels' and 'Time' columns to int64\n",
    "    missing_data = missing_data.astype({'Labels': 'int64', 'Time': 'int64'})\n",
    "\n",
    "    # Append the missing_data DataFrame to your existing DataFrame\n",
    "    cluster_tmp_edit = pd.concat([cluster_tmp_edit, missing_data], ignore_index=True)\n",
    "\n",
    "    # Sort the DataFrame by 'Time' column if needed\n",
    "    cluster_tmp_edit.sort_values(by=['Time'], inplace=True)\n",
    "\n",
    "    # Reset the index\n",
    "    cluster_tmp_edit.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return cluster_tmp_edit\n",
    "\n",
    "# Example usage:\n",
    "# cluster_tmp_edit = fill_missing_values_in_time_series(cluster_tmp_edit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmZ_Hed4G9L3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "def interpolate_missing_values_with_spline(cluster_tmp_edit):\n",
    "    # Find the unique labels in the 'Labels' column\n",
    "    unique_labels = cluster_tmp_edit['Labels'].unique()\n",
    "\n",
    "    # Create a dictionary to store cubic spline functions for each column (excluding 'Time')\n",
    "    spline_functions = {}\n",
    "\n",
    "    # Iterate through unique labels and create spline functions\n",
    "    for label in unique_labels:\n",
    "        # Make a copy of the data for this label to avoid SettingWithCopyWarning\n",
    "        label_data = cluster_tmp_edit[cluster_tmp_edit['Labels'] == label].copy()\n",
    "\n",
    "        # Sort the data by 'Time' column if it's not already sorted\n",
    "        label_data.sort_values(by=['Time'], inplace=True)\n",
    "\n",
    "        # Iterate through columns (excluding 'Time') to create spline functions\n",
    "        for column in label_data.columns:\n",
    "            if column != 'Time':\n",
    "                # Check if there are missing values in the current column\n",
    "                missing_indices = label_data[column].isna()\n",
    "\n",
    "                if missing_indices.any():\n",
    "                    # Extract non-missing time and value data\n",
    "                    t_data = label_data.loc[~missing_indices, 'Time']\n",
    "                    w_data = label_data.loc[~missing_indices, column]\n",
    "\n",
    "                    # Create a cubic spline object\n",
    "                    spline = CubicSpline(t_data, w_data)\n",
    "\n",
    "                    # Store the spline function in the dictionary\n",
    "                    spline_functions[(label, column)] = spline\n",
    "\n",
    "    # Now you have spline functions for each (label, column) pair with missing values\n",
    "\n",
    "    # Iterate through unique labels and columns to interpolate missing values\n",
    "    for label in unique_labels:\n",
    "        for column in label_data.columns:\n",
    "            if column != 'Time':\n",
    "                # Check if there are missing values in the current column\n",
    "                missing_indices = cluster_tmp_edit['Labels'] == label\n",
    "                missing_values = cluster_tmp_edit.loc[missing_indices, column].isna()\n",
    "\n",
    "                if missing_values.any():\n",
    "                    # Extract time values where interpolation is needed\n",
    "                    t_missing = cluster_tmp_edit.loc[missing_indices, 'Time']\n",
    "\n",
    "                    # Use the corresponding spline function to interpolate missing values\n",
    "                    spline = spline_functions.get((label, column))\n",
    "                    if spline:\n",
    "                        interpolated_values = spline(t_missing)\n",
    "\n",
    "                        # Update the DataFrame with interpolated values\n",
    "                        cluster_tmp_edit.loc[missing_indices, column] = interpolated_values\n",
    "\n",
    "    # Now, cluster_tmp_edit contains interpolated values for missing data\n",
    "    return cluster_tmp_edit\n",
    "\n",
    "# Example usage:\n",
    "# cluster_tmp_edit = interpolate_missing_values_with_spline(cluster_tmp_edit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVwAKS3dHgaE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def adjust_data_for_missing_times(df_tmp, cluster_tmp_edit):\n",
    "    # Function to find common columns between two DataFrames\n",
    "    def find_common_columns(df1, df2):\n",
    "        common_columns = set(df1.columns).intersection(df2.columns)\n",
    "        return list(common_columns)\n",
    "\n",
    "    # Find unique labels in 'Labels' column of 'cluster_tmp_edit'\n",
    "    unique_labels = cluster_tmp_edit['Labels'].unique()\n",
    "\n",
    "    # Find common columns between df_all_imp and cluster_tmp_edit\n",
    "    common_columns = find_common_columns(df_tmp, cluster_tmp_edit)\n",
    "\n",
    "    # List to store DataFrame fragments\n",
    "    rows_to_concat = []\n",
    "\n",
    "    # Iterate through unique labels\n",
    "    for label in unique_labels:\n",
    "        # Find unique 'Time' values in 'cluster_tmp_edit' and 'df_tmp' for the current label\n",
    "        unique_times_clusters = cluster_tmp_edit[(cluster_tmp_edit['Labels'] == label)]['Time'].unique()\n",
    "        unique_times_df_tmp = df_tmp[(df_tmp['Labels'] == label)]['Time'].unique()\n",
    "        missing_times = set(unique_times_clusters) - set(unique_times_df_tmp)\n",
    "        # Iterate through missing 'Time' values for the current label\n",
    "        for considered_time in missing_times:\n",
    "            last_alive = 1\n",
    "            while len(df_tmp[(df_tmp['Time'] == (considered_time - last_alive)) & (df_tmp['Labels'] == label)]) == 0:\n",
    "                last_alive += 1\n",
    "\n",
    "            # Create a DataFrame to store the modified data\n",
    "            df_all_imp = pd.DataFrame()\n",
    "\n",
    "            # Find 'N_cell' value for the considered 'Time' and label from 'clusters'\n",
    "            N = int(cluster_tmp_edit[(cluster_tmp_edit['Time'] == considered_time) & (cluster_tmp_edit['Labels'] == label)]['N_cell'])\n",
    "\n",
    "            # Find 'N_cell' value for the previous 'Time' and label from 'clusters'\n",
    "            N_prev = int(cluster_tmp_edit[(cluster_tmp_edit['Time'] == (considered_time - last_alive)) & (cluster_tmp_edit['Labels'] == label)]['N_cell'])\n",
    "\n",
    "            if N >= N_prev:\n",
    "                # Copy all rows of df_tmp with the same label and 'Time' == (considered_time - 1) to df_all_imp\n",
    "                additional_rows = pd.concat([df_all_imp, df_tmp[(df_tmp['Time'] == (considered_time - last_alive)) & (df_tmp['Labels'] == label)]], ignore_index=True)\n",
    "                additional_rows['Time'] = considered_time\n",
    "                df_all_imp = pd.concat([df_all_imp, additional_rows], ignore_index=True)\n",
    "\n",
    "                if (N - N_prev) > 0:\n",
    "                    # Adjust the common columns in additional_rows\n",
    "                    selected_rows = df_tmp[(df_tmp['Time'] == (considered_time - last_alive)) & (df_tmp['Labels'] == label)].sample(n=(N - N_prev), replace=True)\n",
    "                    additional_rows['Time'] = considered_time\n",
    "                    df_all_imp = pd.concat([df_all_imp, additional_rows], ignore_index=True)\n",
    "\n",
    "            elif N < N_prev:\n",
    "                # Copy all rows of df_tmp with the same label and 'Time' == (considered_time - 1) to df_all_imp\n",
    "                diminished_rows = df_tmp[(df_tmp['Time'] == (considered_time - last_alive)) & (df_tmp['Labels'] == label)].sample(n=(N), replace=True)\n",
    "                diminished_rows['Time'] = considered_time\n",
    "                diminished_rows.reset_index(drop=True, inplace=True)\n",
    "                df_all_imp = pd.concat([df_all_imp, diminished_rows], ignore_index=True)\n",
    "\n",
    "            # Adjust the columns in df_all_imp for the specific label\n",
    "            for common_column in common_columns:\n",
    "                cluster_difference = (\n",
    "                    cluster_tmp_edit[(cluster_tmp_edit['Time'] == considered_time) & (cluster_tmp_edit['Labels'] == label)][common_column].values[0] -\n",
    "                    cluster_tmp_edit[(cluster_tmp_edit['Time'] == (considered_time - last_alive)) & (cluster_tmp_edit['Labels'] == label)][common_column].values[0]\n",
    "                )\n",
    "\n",
    "                # Adjust the cluster_difference in df_all_imp for the common columns\n",
    "                df_all_imp[df_all_imp['Time'] == considered_time][common_column] += cluster_difference\n",
    "            # Append the modified DataFrame to the list\n",
    "            rows_to_concat.append(df_all_imp)\n",
    "\n",
    "    # Concatenate all DataFrame fragments into a single DataFrame\n",
    "    df_all_imp = pd.concat(rows_to_concat, ignore_index=True)\n",
    "\n",
    "    # Now, df_all_imp contains the modified data with 'Time' values as considered_time and common columns adjusted for the same label and label-specific 'Time' values\n",
    "    return df_all_imp\n",
    "\n",
    "# Usage example:\n",
    "# df_all_imp_result = adjust_data_for_missing_times(df_tmp, cluster_tmp_edit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5VQ79QvIHl5-"
   },
   "outputs": [],
   "source": [
    "def impute_cells_and_cluster(cluster_tmp_edit, df_tmp, selected = []):\n",
    "  cluster_tmp_edit = fill_missing_values_in_time_series(cluster_tmp_edit)\n",
    "  cluster_tmp_edit = interpolate_missing_values_with_spline(cluster_tmp_edit)\n",
    "  if len(selected) > 0:\n",
    "    cluster_tmp_edit = cluster_tmp_edit[cluster_tmp_edit['Labels'].isin(selected)]\n",
    "  cluster_tmp_edit['N_cell'] = cluster_tmp_edit['N_cell'].apply(lambda x: max(0, int(x)))\n",
    "  df_all_imp = adjust_data_for_missing_times(df_tmp, cluster_tmp_edit)\n",
    "  df_all_imp = pd.concat([df_all_imp, df_tmp], ignore_index=True)\n",
    "  return cluster_tmp_edit, df_all_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kDGkoQdkgle"
   },
   "source": [
    "###### Here is something not to use. Maybe useful if something bad happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GncKTLi4FluW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Function to find common columns between two DataFrames\n",
    "def find_common_columns(df1, df2):\n",
    "    common_columns = set(df1.columns).intersection(df2.columns)\n",
    "    return list(common_columns)\n",
    "\n",
    "# Find unique labels in 'Labels' column of 'cluster_tmp_edit'\n",
    "unique_labels = cluster_tmp_edit['Labels'].unique()\n",
    "\n",
    "# Find common columns between df_all_imp and cluster_tmp_edit\n",
    "common_columns = find_common_columns(df_tmp, cluster_tmp_edit)\n",
    "\n",
    "# List to store DataFrame fragments\n",
    "rows_to_concat = []\n",
    "\n",
    "# Iterate through unique labels\n",
    "for label in unique_labels:\n",
    "    # Find unique 'Time' values in 'cluster_tmp_edit' and 'df_tmp' for the current label\n",
    "    unique_times_clusters = cluster_tmp_edit[(cluster_tmp_edit['Labels'] == label)]['Time'].unique()\n",
    "    unique_times_df_tmp = df_tmp[(df_tmp['Labels'] == label)]['Time'].unique()\n",
    "    missing_times = set(unique_times_clusters) - set(unique_times_df_tmp)\n",
    "    # Iterate through missing 'Time' values for the current label\n",
    "    for considered_time in missing_times:\n",
    "        last_alive = 1\n",
    "        while len(df_tmp[(df_tmp['Time'] == (considered_time - last_alive)) & (df_tmp['Labels'] == label)]) == 0:\n",
    "            last_alive += 1\n",
    "\n",
    "        # Create a DataFrame to store the modified data\n",
    "        df_all_imp = pd.DataFrame()\n",
    "\n",
    "        # Find 'N_cell' value for the considered 'Time' and label from 'clusters'\n",
    "        N = int(cluster_tmp_edit[(cluster_tmp_edit['Time'] == considered_time) & (cluster_tmp_edit['Labels'] == label)]['N_cell'])\n",
    "\n",
    "        # Find 'N_cell' value for the previous 'Time' and label from 'clusters'\n",
    "        N_prev = int(cluster_tmp_edit[(cluster_tmp_edit['Time'] == (considered_time - last_alive)) & (cluster_tmp_edit['Labels'] == label)]['N_cell'])\n",
    "\n",
    "        if N >= N_prev:\n",
    "            # Copy all rows of df_tmp with the same label and 'Time' == (considered_time - 1) to df_all_imp\n",
    "            additional_rows = pd.concat([df_all_imp, df_tmp[(df_tmp['Time'] == (considered_time - last_alive)) & (df_tmp['Labels'] == label)]], ignore_index=True)\n",
    "            additional_rows['Time'] = considered_time\n",
    "            df_all_imp = pd.concat([df_all_imp, additional_rows], ignore_index=True)\n",
    "\n",
    "            if (N - N_prev) > 0:\n",
    "                # Adjust the common columns in additional_rows\n",
    "                selected_rows = df_tmp[(df_tmp['Time'] == (considered_time - last_alive)) & (df_tmp['Labels'] == label)].sample(n=(N - N_prev), replace=True)\n",
    "                additional_rows['Time'] = considered_time\n",
    "                df_all_imp = pd.concat([df_all_imp, additional_rows], ignore_index=True)\n",
    "\n",
    "        elif N < N_prev:\n",
    "            # Copy all rows of df_tmp with the same label and 'Time' == (considered_time - 1) to df_all_imp\n",
    "            diminished_rows = df_tmp[(df_tmp['Time'] == (considered_time - last_alive)) & (df_tmp['Labels'] == label)].sample(n=(N), replace=True)\n",
    "            diminished_rows['Time'] = considered_time\n",
    "            diminished_rows.reset_index(drop=True, inplace=True)\n",
    "            df_all_imp = pd.concat([df_all_imp, diminished_rows], ignore_index=True)\n",
    "\n",
    "        # Adjust the columns in df_all_imp for the specific label\n",
    "        for common_column in common_columns:\n",
    "            cluster_difference = (\n",
    "                cluster_tmp_edit[(cluster_tmp_edit['Time'] == considered_time) & (cluster_tmp_edit['Labels'] == label)][common_column].values[0] -\n",
    "                cluster_tmp_edit[(cluster_tmp_edit['Time'] == (considered_time - last_alive)) & (cluster_tmp_edit['Labels'] == label)][common_column].values[0]\n",
    "            )\n",
    "\n",
    "            # Adjust the cluster_difference in df_all_imp for the common columns\n",
    "            # deprecated method -> df_all_imp.loc[df_all_imp['Time'] == considered_time, common_column] += cluster_difference\n",
    "            df_all_imp[df_all_imp['Time'] == considered_time][common_column] += cluster_difference\n",
    "        # Append the modified DataFrame to the list\n",
    "        rows_to_concat.append(df_all_imp)\n",
    "\n",
    "# Concatenate all DataFrame fragments into a single DataFrame\n",
    "df_all_imp = pd.concat(rows_to_concat, ignore_index=True)\n",
    "\n",
    "# Now, df_all_imp contains the modified data with 'Time' values as considered_time and common columns adjusted for the same label and label-specific 'Time' values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhL7BFi0B89-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Function to find common columns between two DataFrames\n",
    "def find_common_columns(df1, df2):\n",
    "    common_columns = set(df1.columns).intersection(df2.columns)\n",
    "    return list(common_columns)\n",
    "\n",
    "# Find unique labels in 'Labels' column of 'cluster_tmp_edit'\n",
    "unique_labels = cluster_tmp_edit['Labels'].unique()\n",
    "\n",
    "# Find common columns between df_all_imp and cluster_tmp_edit\n",
    "common_columns = find_common_columns(df_tmp, cluster_tmp_edit)\n",
    "\n",
    "# List to store DataFrame fragments\n",
    "rows_to_concat = []\n",
    "\n",
    "# Iterate through unique labels\n",
    "for label in unique_labels:\n",
    "    # Find unique 'Time' values in 'cluster_tmp_edit' and 'df_tmp' for the current label\n",
    "    unique_times_clusters = cluster_tmp_edit[(cluster_tmp_edit['Labels'] == label)]['Time'].unique()\n",
    "    unique_times_df_tmp = df_tmp[(df_tmp['Labels'] == label)]['Time'].unique()\n",
    "\n",
    "    # Iterate through missing 'Time' values for the current label\n",
    "    for considered_time in unique_times_clusters:\n",
    "        last_alive = 1\n",
    "        while len(df_tmp[(df_tmp['Time'] == (considered_time - last_alive)) & (df_tmp['Labels'] == label)]) == 0:\n",
    "            last_alive += 1\n",
    "\n",
    "        # Create a DataFrame to store the modified data\n",
    "        df_all_imp = pd.DataFrame()\n",
    "\n",
    "        # Find 'N_cell' value for the considered 'Time' and label from 'clusters'\n",
    "        N = int(cluster_tmp_edit[(cluster_tmp_edit['Time'] == considered_time) & (cluster_tmp_edit['Labels'] == label)]['N_cell'])\n",
    "\n",
    "        # Find 'N_cell' value for the previous 'Time' and label from 'clusters'\n",
    "        N_prev = int(cluster_tmp_edit[(cluster_tmp_edit['Time'] == (considered_time - last_alive)) & (cluster_tmp_edit['Labels'] == label)]['N_cell'])\n",
    "\n",
    "        if N >= N_prev:\n",
    "            # Copy all rows of df_tmp with the same label and 'Time' == (considered_time - 1) to df_all_imp\n",
    "            additional_rows = pd.concat([df_all_imp, df_tmp[(df_tmp['Time'] == (considered_time - last_alive)) & (df_tmp['Labels'] == label)]], ignore_index=True)\n",
    "            additional_rows['Time'] = considered_time\n",
    "            df_all_imp = pd.concat([df_all_imp, additional_rows], ignore_index=True)\n",
    "\n",
    "            if (N - N_prev) > 0:\n",
    "                # Adjust the common columns in additional_rows\n",
    "                selected_rows = df_tmp[(df_tmp['Time'] == (considered_time - last_alive)) & (df_tmp['Labels'] == label)].head(n=(N - N_prev))\n",
    "                additional_rows['Time'] = considered_time\n",
    "                df_all_imp = pd.concat([df_all_imp, additional_rows], ignore_index=True)\n",
    "\n",
    "        elif N < N_prev:\n",
    "            # Copy all rows of df_tmp with the same label and 'Time' == (considered_time - 1) to df_all_imp\n",
    "            diminished_rows = df_tmp[(df_tmp['Time'] == (considered_time - last_alive)) & (df_tmp['Labels'] == label)].head(n=(N))\n",
    "            diminished_rows['Time'] = considered_time\n",
    "            diminished_rows.reset_index(drop=True, inplace=True)\n",
    "            df_all_imp = pd.concat([df_all_imp, diminished_rows], ignore_index=True)\n",
    "\n",
    "        # Adjust the columns in df_all_imp for the specific label\n",
    "        for common_column in common_columns:\n",
    "            cluster_difference = (\n",
    "                cluster_tmp_edit[(cluster_tmp_edit['Time'] == considered_time) & (cluster_tmp_edit['Labels'] == label)][common_column].values[0] -\n",
    "                cluster_tmp_edit[(cluster_tmp_edit['Time'] == (considered_time - last_alive)) & (cluster_tmp_edit['Labels'] == label)][common_column].values[0]\n",
    "            )\n",
    "\n",
    "            # Adjust the cluster_difference in df_all_imp for the common columns\n",
    "            # deprecated method -> df_all_imp.loc[df_all_imp['Time'] == considered_time, common_column] += cluster_difference\n",
    "            df_all_imp[df_all_imp['Time'] == considered_time][common_column] += cluster_difference\n",
    "        # Append the modified DataFrame to the list\n",
    "        rows_to_concat.append(df_all_imp)\n",
    "\n",
    "# Concatenate all DataFrame fragments into a single DataFrame\n",
    "df_all_imp = pd.concat(rows_to_concat, ignore_index=True)\n",
    "\n",
    "# Now, df_all_imp contains the modified data with 'Time' values as considered_time and common columns adjusted for the same label and label-specific 'Time' values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Vk2hRBjt8NN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Function to find common columns between two DataFrames\n",
    "def find_common_columns(df1, df2):\n",
    "    common_columns = set(df1.columns).intersection(df2.columns)\n",
    "    return list(common_columns)\n",
    "\n",
    "# Create a DataFrame to store the modified data\n",
    "df_all_imp = pd.DataFrame()\n",
    "\n",
    "# Find unique labels in 'Labels' column of 'cluster_tmp_edit'\n",
    "unique_labels = cluster_tmp_edit['Labels'].unique()\n",
    "# Find common columns between df_all_imp and cluster_tmp_edit\n",
    "common_columns = find_common_columns(df_tmp, cluster_tmp_edit)\n",
    "# Iterate through unique labels\n",
    "for label in unique_labels:\n",
    "    # Find unique 'Time' values in 'cluster_tmp_edit' and 'df_tmp' for the current label\n",
    "    unique_times_clusters = cluster_tmp_edit[(cluster_tmp_edit['Labels'] == label)]['Time'].unique()\n",
    "    unique_times_df_tmp = df_tmp[(df_tmp['Labels'] == label)]['Time'].unique()\n",
    "\n",
    "    # Identify 'Time' values in 'clusters' not present in 'df_tmp' for the current label\n",
    "    missing_times = set(unique_times_clusters) - set(unique_times_df_tmp)\n",
    "\n",
    "    # Iterate through missing 'Time' values for the current label\n",
    "    for considered_time in missing_times:\n",
    "        last_alive = 1\n",
    "        while len(df_tmp[(df_tmp['Time'] == (considered_time - last_alive)) & (df_tmp['Labels'] == label)]) = 0:\n",
    "            last_alive += 1\n",
    "        # Find 'N_cell' value for the considered 'Time' and label from 'clusters'\n",
    "        N = int(cluster_tmp_edit[(cluster_tmp_edit['Time'] == considered_time) & (cluster_tmp_edit['Labels'] == label)]['N_cell'])\n",
    "\n",
    "        # Find 'N_cell' value for the previous 'Time' and label from 'clusters'\n",
    "        N_prev = int(cluster_tmp_edit[(cluster_tmp_edit['Time'] == (considered_time - last_alive)) & (cluster_tmp_edit['Labels'] == label)]['N_cell'])\n",
    "\n",
    "        if N >= N_prev:\n",
    "\n",
    "            # Copy all rows of df_tmp with the same label and 'Time' == (considered_time - 1) to df_all_imp\n",
    "            additional_rows = pd.concat([df_all_imp, df_tmp[(df_tmp['Time'] == (considered_time - last_alive)) & (df_tmp['Labels'] == label)]], ignore_index=True)\n",
    "            additional_rows['Time'] = considered_time\n",
    "            # Reset the index to have consecutive numbers starting from 1\n",
    "\n",
    "            df_all_imp = pd.concat([df_all_imp, additional_rows], ignore_index=True)\n",
    "\n",
    "\n",
    "            if (N - N_prev) > 0:\n",
    "                # Adjust the common columns in additional_rows\n",
    "                selected_rows = df_tmp[(df_tmp['Time'] == (considered_time - last_alive)) & (df_tmp['Labels'] == label)].head(n=(N - N_prev))\n",
    "                #additional_rows = df_tmp[(df_tmp['Time'] == (considered_time - 1)) & (df_tmp['Labels'] == label)].sample(n=(N - N_prev), replace=True)\n",
    "                # Update 'Time' in additional_rows to become considered_time\n",
    "                additional_rows['Time'] = considered_time\n",
    "                df_all_imp = pd.concat([df_all_imp, additional_rows], ignore_index=True)\n",
    "\n",
    "        elif N < N_prev:\n",
    "            # Copy all rows of df_tmp with the same label and 'Time' == (considered_time - 1) to df_all_imp\n",
    "            diminished_rows = df_tmp[(df_tmp['Time'] == (considered_time - last_alive)) & (df_tmp['Labels'] == label)].head(n=(N))\n",
    "            # Update 'Time' in additional_rows to become considered_time\n",
    "            diminished_rows['Time'] = considered_time\n",
    "\n",
    "            diminished_rows.reset_index(drop=True, inplace=True)\n",
    "            # Remove (N_prev - N) rows randomly from df_all_imp\n",
    "            #indices_to_remove = random.sample(range(len(diminished_rows)), N_prev - N)\n",
    "            #diminished_rows = diminished_rows.drop(indices_to_remove)\n",
    "            df_all_imp = pd.concat([df_all_imp, diminished_rows], ignore_index=True)\n",
    "            #diminished_rows = pd.DataFrame()\n",
    "                # Adjust the columns in additional_rows for the specific label\n",
    "\n",
    "        for common_column in common_columns:\n",
    "          cluster_difference = (\n",
    "            cluster_tmp_edit[(cluster_tmp_edit['Time'] == considered_time) & (cluster_tmp_edit['Labels'] == label)][common_column].values[0] -\n",
    "            cluster_tmp_edit[(cluster_tmp_edit['Time'] == (considered_time - last_alive)) & (cluster_tmp_edit['Labels'] == label)][common_column].values[0])\n",
    "\n",
    "            # Adjust the cluster_difference in df_all_imp for the common columns\n",
    "        df_all_imp.loc[df_all_imp['Time'] == considered_time, common_column] += cluster_difference\n",
    "\n",
    "# Now, df_all_imp contains the modified data with 'Time' values as considered_time and common columns adjusted for the same label and label-specific 'Time' values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bw-NI8m52UcU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "def find_common_columns(df1, df2):\n",
    "    common_columns = set(df1.columns).intersection(df2.columns)\n",
    "    return list(common_columns)\n",
    "\n",
    "#############df_all_imp = df_tmp.copy()\n",
    "unique_labels = cluster_tmp_edit['Labels'].unique()\n",
    "common_columns = find_common_columns(df_tmp, cluster_tmp_edit)\n",
    "for label in unique_labels:\n",
    "    unique_times_clusters = cluster_tmp_edit[(cluster_tmp_edit['Labels'] == label)]['Time'].unique()\n",
    "    unique_times_df_tmp = df_tmp[(df_tmp['Labels'] == label)]['Time'].unique()\n",
    "\n",
    "    missing_times = set(unique_times_clusters) - set(unique_times_df_tmp)\n",
    "\n",
    "    for considered_time in missing_times:\n",
    "        N = int(cluster_tmp_edit[(cluster_tmp_edit['Time'] == considered_time) & (cluster_tmp_edit['Labels'] == label)]['N_cell'])\n",
    "\n",
    "        N_prev = int(cluster_tmp_edit[(cluster_tmp_edit['Time'] == (considered_time - 1)) & (cluster_tmp_edit['Labels'] == label)]['N_cell'])\n",
    "\n",
    "        if N >= N_prev:\n",
    "            additional_rows = pd.concat([df_all_imp, df_tmp[(df_tmp['Time'] == (considered_time - 1)) & (df_tmp['Labels'] == label)]], ignore_index=True)\n",
    "            additional_rows['Time'] = considered_time\n",
    "\n",
    "            df_all_imp = pd.concat([df_all_imp, additional_rows], ignore_index=True)\n",
    "\n",
    "\n",
    "            if (N - N_prev) > 0:\n",
    "                selected_rows = df_tmp[(df_tmp['Time'] == (considered_time - 1)) & (df_tmp['Labels'] == label)].head(n=(N - N_prev))\n",
    "                additional_rows['Time'] = considered_time\n",
    "                df_all_imp = pd.concat([df_all_imp, additional_rows], ignore_index=True)\n",
    "\n",
    "        elif N < N_prev:\n",
    "            diminished_rows = df_tmp[(df_tmp['Time'] == (considered_time - 1)) & (df_tmp['Labels'] == label)].head(n=(N))\n",
    "            diminished_rows['Time'] = considered_time\n",
    "            diminished_rows.reset_index(drop=True, inplace=True)\n",
    "            df_all_imp = pd.concat([df_all_imp, diminished_rows], ignore_index=True)\n",
    "\n",
    "        for common_column in common_columns:\n",
    "          cluster_difference = (\n",
    "            cluster_tmp_edit[(cluster_tmp_edit['Time'] == considered_time) & (cluster_tmp_edit['Labels'] == label)][common_column].values[0] -\n",
    "            cluster_tmp_edit[(cluster_tmp_edit['Time'] == (considered_time - 1)) & (cluster_tmp_edit['Labels'] == label)][common_column].values[0])\n",
    "\n",
    "        df_all_imp[df_all_imp['Time'] == considered_time][common_column] += cluster_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvVbn3qyEZWC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Function to find common columns between two DataFrames\n",
    "def find_common_columns(df1, df2):\n",
    "    common_columns = set(df1.columns).intersection(df2.columns)\n",
    "    return list(common_columns)\n",
    "\n",
    "\n",
    "# Find unique labels in 'Labels' column of 'cluster_tmp_edit'\n",
    "unique_labels = cluster_tmp_edit['Labels'].unique()\n",
    "# Find common columns between df_all_imp and cluster_tmp_edit\n",
    "common_columns = find_common_columns(df_tmp, cluster_tmp_edit)\n",
    "# Iterate through unique labels\n",
    "rows_to_concat = []\n",
    "for label in unique_labels:\n",
    "    # Create a DataFrame to store the modified data\n",
    "    # Find unique 'Time' values in 'cluster_tmp_edit' and 'df_tmp' for the current label\n",
    "    unique_times_clusters = cluster_tmp_edit[(cluster_tmp_edit['Labels'] == label)]['Time'].unique()\n",
    "    unique_times_df_tmp = df_tmp[(df_tmp['Labels'] == label)]['Time'].unique()\n",
    "\n",
    "    # Identify 'Time' values in 'clusters' not present in 'df_tmp' for the current label\n",
    "    missing_times = set(unique_times_clusters) - set(unique_times_df_tmp)\n",
    "\n",
    "    # Iterate through missing 'Time' values for the current label\n",
    "    for considered_time in missing_times:\n",
    "        last_alive = 1\n",
    "        while len(df_tmp[(df_tmp['Time'] == (considered_time - last_alive)) & (df_tmp['Labels'] == label)]) == 0:\n",
    "            last_alive += 1\n",
    "        df_all_imp = pd.DataFrame()\n",
    "        # Find 'N_cell' value for the considered 'Time' and label from 'clusters'\n",
    "        N = int(cluster_tmp_edit[(cluster_tmp_edit['Time'] == considered_time) & (cluster_tmp_edit['Labels'] == label)]['N_cell'])\n",
    "\n",
    "        # Find 'N_cell' value for the previous 'Time' and label from 'clusters'\n",
    "        N_prev = int(cluster_tmp_edit[(cluster_tmp_edit['Time'] == (considered_time - last_alive)) & (cluster_tmp_edit['Labels'] == label)]['N_cell'])\n",
    "\n",
    "        if N >= N_prev:\n",
    "            # Copy all rows of df_tmp with the same label and 'Time' == (considered_time - 1) to df_all_imp\n",
    "            additional_rows = pd.concat([df_all_imp, df_tmp[(df_tmp['Time'] == (considered_time - last_alive)) & (df_tmp['Labels'] == label)]], ignore_index=True)\n",
    "            additional_rows['Time'] = considered_time\n",
    "            # Reset the index to have consecutive numbers starting from 1\n",
    "\n",
    "            df_all_imp = pd.concat([df_all_imp, additional_rows], ignore_index=True)\n",
    "            #additional_rows = pd.DataFrame()\n",
    "\n",
    "            if (N - N_prev) > 0:\n",
    "                # Adjust the common columns in additional_rows\n",
    "                selected_rows = df_tmp[(df_tmp['Time'] == (considered_time - last_alive)) & (df_tmp['Labels'] == label)].head(n=(N - N_prev))\n",
    "                #additional_rows = df_tmp[(df_tmp['Time'] == (considered_time - 1)) & (df_tmp['Labels'] == label)].sample(n=(N - N_prev), replace=True)\n",
    "                # Update 'Time' in additional_rows to become considered_time\n",
    "                additional_rows['Time'] = considered_time\n",
    "                df_all_imp = pd.concat([df_all_imp, additional_rows], ignore_index=True)\n",
    "\n",
    "        elif N < N_prev:\n",
    "            # Copy all rows of df_tmp with the same label and 'Time' == (considered_time - 1) to df_all_imp\n",
    "            diminished_rows = df_tmp[(df_tmp['Time'] == (considered_time - last_alive)) & (df_tmp['Labels'] == label)].head(n=(N))\n",
    "            # Update 'Time' in additional_rows to become considered_time\n",
    "            diminished_rows['Time'] = considered_time\n",
    "\n",
    "            diminished_rows.reset_index(drop=True, inplace=True)\n",
    "            # Remove (N_prev - N) rows randomly from df_all_imp\n",
    "            #indices_to_remove = random.sample(range(len(diminished_rows)), N_prev - N)\n",
    "            #diminished_rows = diminished_rows.drop(indices_to_remove)\n",
    "            df_all_imp = pd.concat([df_all_imp, diminished_rows], ignore_index=True)\n",
    "            #diminished_rows = pd.DataFrame()\n",
    "                # Adjust the columns in additional_rows for the specific label\n",
    "\n",
    "        for common_column in common_columns:\n",
    "          cluster_difference = (\n",
    "            cluster_tmp_edit[(cluster_tmp_edit['Time'] == considered_time) & (cluster_tmp_edit['Labels'] == label)][common_column].values[0] -\n",
    "            cluster_tmp_edit[(cluster_tmp_edit['Time'] == (considered_time - last_alive)) & (cluster_tmp_edit['Labels'] == label)][common_column].values[0])\n",
    "\n",
    "            # Adjust the cluster_difference in df_all_imp for the common columns\n",
    "        #df_all_imp.loc[df_all_imp['Time'] == considered_time, common_column] += cluster_difference\n",
    "        df_all_imp[df_all_imp['Time'] == considered_time][common_column] += cluster_difference\n",
    "\n",
    "        rows_to_concat.append(df_all_imp)\n",
    "\n",
    "df_all_imp = pd.concat(rows_to_concat, ignore_index=True)\n",
    "# Now, df_all_imp contains the modified data with 'Time' values as considered_time and common columns adjusted for the same label and label-specific 'Time' values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghH8jRtuOhBe"
   },
   "outputs": [],
   "source": [
    "selected = [0,1,2,3]\n",
    "cluster_tmp_edit = cluster_tmp_edit[cluster_tmp_edit['Labels'].isin(selected)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQLea9oaKKrO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "# Assuming you have a DataFrame named cluster_tmp_edit with 'Labels', 'Time', and other columns\n",
    "\n",
    "# Find the unique labels in the 'Labels' column\n",
    "unique_labels = cluster_tmp_edit['Labels'].unique()\n",
    "\n",
    "# Create a dictionary to store cubic spline functions for each column (excluding 'Time')\n",
    "spline_functions = {}\n",
    "\n",
    "# Iterate through unique labels and create spline functions\n",
    "for label in unique_labels:\n",
    "    # Make a copy of the data for this label to avoid SettingWithCopyWarning\n",
    "    label_data = cluster_tmp_edit[cluster_tmp_edit['Labels'] == label].copy()\n",
    "\n",
    "    # Sort the data by 'Time' column if it's not already sorted\n",
    "    label_data.sort_values(by=['Time'], inplace=True)\n",
    "\n",
    "    # Iterate through columns (excluding 'Time') to create spline functions\n",
    "    for column in label_data.columns:\n",
    "        if column != 'Time':\n",
    "            # Check if there are missing values in the current column\n",
    "            missing_indices = label_data[column].isna()\n",
    "\n",
    "            if missing_indices.any():\n",
    "                # Extract non-missing time and value data\n",
    "                t_data = label_data.loc[~missing_indices, 'Time']\n",
    "                w_data = label_data.loc[~missing_indices, column]\n",
    "\n",
    "                # Create a cubic spline object\n",
    "                spline = CubicSpline(t_data, w_data)\n",
    "\n",
    "                # Store the spline function in the dictionary\n",
    "                spline_functions[(label, column)] = spline\n",
    "\n",
    "# Now you have spline functions for each (label, column) pair with missing values\n",
    "\n",
    "# Iterate through unique labels and columns to interpolate missing values\n",
    "for label in unique_labels:\n",
    "    for column in label_data.columns:\n",
    "        if column != 'Time':\n",
    "            # Check if there are missing values in the current column\n",
    "            missing_indices = cluster_tmp_edit['Labels'] == label\n",
    "            missing_values = cluster_tmp_edit.loc[missing_indices, column].isna()\n",
    "\n",
    "            if missing_values.any():\n",
    "                # Extract time values where interpolation is needed\n",
    "                t_missing = cluster_tmp_edit.loc[missing_indices, 'Time']\n",
    "\n",
    "                # Use the corresponding spline function to interpolate missing values\n",
    "                spline = spline_functions.get((label, column))\n",
    "                if spline:\n",
    "                    interpolated_values = spline(t_missing)\n",
    "\n",
    "                    # Update the DataFrame with interpolated values\n",
    "                    cluster_tmp_edit.loc[missing_indices, column] = interpolated_values\n",
    "\n",
    "# Now, cluster_tmp_edit contains interpolated values for missing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkqmrSdmMTmT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Find all unique labels in the 'Labels' column\n",
    "unique_labels = cluster_tmp_edit['Labels'].unique()\n",
    "\n",
    "# Create an empty DataFrame to store missing data\n",
    "missing_data = pd.DataFrame(columns=['Labels', 'Time'])\n",
    "\n",
    "# Iterate through unique labels\n",
    "for label in unique_labels:\n",
    "    # Find the minimum 'Time' value in the 'Time' column for the current label\n",
    "    start_time = cluster_tmp_edit[cluster_tmp_edit['Labels'] == label]['Time'].min()\n",
    "\n",
    "    # Study the desired cluster for the current label\n",
    "    desired_cluster = cluster_tmp_edit[cluster_tmp_edit['Labels'] == label]\n",
    "\n",
    "    # Create a list of numbers starting from the 'start_time'\n",
    "    max_value = desired_cluster['Time'].max()\n",
    "    all_numbers = list(range(int(start_time), int(max_value) + 1))\n",
    "\n",
    "    # Find the unique values in the 'Time' column for the current cluster\n",
    "    unique_values = desired_cluster['Time'].unique()\n",
    "\n",
    "    # Find the numbers that are not present in the column for the current cluster\n",
    "    missing_numbers = list(set(all_numbers) - set(unique_values))\n",
    "\n",
    "    # Create a DataFrame with missing values for the current cluster\n",
    "    missing_cluster_data = pd.DataFrame({'Labels': [label] * len(missing_numbers), 'Time': missing_numbers})\n",
    "\n",
    "    # Append the missing_cluster_data DataFrame to the overall missing_data\n",
    "    missing_data = pd.concat([missing_data, missing_cluster_data], ignore_index=True)\n",
    "\n",
    "# Convert 'Labels' and 'Time' columns to int64\n",
    "missing_data = missing_data.astype({'Labels': 'int64', 'Time': 'int64'})\n",
    "\n",
    "# Append the missing_data DataFrame to your existing DataFrame\n",
    "cluster_tmp_edit = pd.concat([cluster_tmp_edit, missing_data], ignore_index=True)\n",
    "\n",
    "# Sort the DataFrame by 'Time' column if needed\n",
    "cluster_tmp_edit.sort_values(by=['Time'], inplace=True)\n",
    "\n",
    "# Reset the index\n",
    "cluster_tmp_edit.reset_index(drop=True, inplace=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "-omqw5Cfje43",
    "2XYCSDf9KqNk",
    "ZbtVK8a2joLH",
    "rF_KTEqwjnV0",
    "4FbOcmAwBuLZ",
    "3Z0IpRhJPEYh",
    "DIa4cINvWD11",
    "uVqSpyXqQNcn",
    "gIND83pJQU4Z",
    "L70TyypIQXWS",
    "KVjZQyGOQiaj",
    "2rdNrqKuQSjf",
    "gwX7fboF8zsQ",
    "85l7oUs481cf",
    "Yi_BFp6f822q",
    "VVvzqsVWCqwY",
    "joRemBvzCwx_",
    "rPI2isHakCV1",
    "oQ4Ghv4XjD77",
    "CDs9W1THyha8",
    "Dz3RDoUazvdD",
    "Ckn_7gLs2Y-P",
    "lSULPDG566XV"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
